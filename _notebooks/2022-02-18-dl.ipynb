{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f951c46-f34a-489c-a54f-4d897534b6b3",
   "metadata": {},
   "source": [
    "# 드롭 아웃 & 합성곱 신경망(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92cf11-14ad-4c4f-a1f8-7ea7e16e74b2",
   "metadata": {},
   "source": [
    "- 오버피팅을 억제하는 방법으로 드롭아웃이라는 기법을 사용할 수 있다. \n",
    "  - 뉴런을 임의로 삭제하면서 학습하는 방법이다. \n",
    "  - 훈련 때 뉴런을 무작위로 골라 삭제한다. 삭제된 뉴런은 신호를 전달하지 않는다. 훈련 때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고, 시험 때는 모든 뉴런에 신호를 전달한다. \n",
    "  - 단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fdf6608-f06c-4d3b-a197-89af54e49527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "297f9f89-78c3-456c-a4a2-cca7985b4305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout : \n",
    "    def __init__(self, dropout_ratio=.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask=None\n",
    "    def forward(self, x, train_flg = True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else : return x * (1 - self.dropout_ratio)\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab9cc8-c19c-4f3b-bff4-79125c88635b",
   "metadata": {},
   "source": [
    "- 여기에서의 핵심은 훈련 시에는 순전파 때마다 self.mask에 삭제할 뉴런을 False로 표시한다는 것이다. \n",
    "- self.mask는 x와 형상이 같은 배열을 무작위로 생성하고, 그 값이 dropout_ratio보다 큰 원소만 True로 설정한다.\n",
    "- 역전파때의 동작은 ReLU와 같다. \n",
    "- 드롭아웃의 효과를 MNIST 데이터셋으로 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686be4ee-53bc-4bab-89bf-6e36e3f98674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.324100259095491\n",
      "=== epoch:1, train acc:0.06, test acc:0.0519 ===\n",
      "train loss:2.3367240344717546\n",
      "train loss:2.3198828799791507\n",
      "train loss:2.334138201403393\n",
      "=== epoch:2, train acc:0.06333333333333334, test acc:0.0534 ===\n",
      "train loss:2.368374415559161\n",
      "train loss:2.3381363551504286\n",
      "train loss:2.3330928399833835\n",
      "=== epoch:3, train acc:0.06333333333333334, test acc:0.0548 ===\n",
      "train loss:2.3385353626060423\n",
      "train loss:2.321698027938764\n",
      "train loss:2.342470764732769\n",
      "=== epoch:4, train acc:0.06666666666666667, test acc:0.0566 ===\n",
      "train loss:2.3181256091192863\n",
      "train loss:2.318016527906595\n",
      "train loss:2.3341585909597278\n",
      "=== epoch:5, train acc:0.07, test acc:0.0585 ===\n",
      "train loss:2.3223824145629153\n",
      "train loss:2.3127880613389453\n",
      "train loss:2.3268800220936066\n",
      "=== epoch:6, train acc:0.07, test acc:0.0612 ===\n",
      "train loss:2.3323298965987163\n",
      "train loss:2.3165949954128062\n",
      "train loss:2.3143363185934573\n",
      "=== epoch:7, train acc:0.08333333333333333, test acc:0.0633 ===\n",
      "train loss:2.315828117487278\n",
      "train loss:2.333196760768538\n",
      "train loss:2.306113270261319\n",
      "=== epoch:8, train acc:0.08333333333333333, test acc:0.0665 ===\n",
      "train loss:2.3230691151629\n",
      "train loss:2.302295036765928\n",
      "train loss:2.3050098518514877\n",
      "=== epoch:9, train acc:0.09, test acc:0.0681 ===\n",
      "train loss:2.302610372179809\n",
      "train loss:2.3192645732440322\n",
      "train loss:2.322518870537485\n",
      "=== epoch:10, train acc:0.10666666666666667, test acc:0.0707 ===\n",
      "train loss:2.307218021902985\n",
      "train loss:2.3061724911770507\n",
      "train loss:2.3010604532329983\n",
      "=== epoch:11, train acc:0.11, test acc:0.0737 ===\n",
      "train loss:2.30023057573994\n",
      "train loss:2.3051623665362864\n",
      "train loss:2.2938154369504575\n",
      "=== epoch:12, train acc:0.11, test acc:0.0813 ===\n",
      "train loss:2.3134678116607215\n",
      "train loss:2.2945534263230702\n",
      "train loss:2.3100487292414247\n",
      "=== epoch:13, train acc:0.12333333333333334, test acc:0.0859 ===\n",
      "train loss:2.3037582367612863\n",
      "train loss:2.308003699202601\n",
      "train loss:2.2988065268228475\n",
      "=== epoch:14, train acc:0.12666666666666668, test acc:0.0923 ===\n",
      "train loss:2.286301855338051\n",
      "train loss:2.301356815456316\n",
      "train loss:2.310309438466073\n",
      "=== epoch:15, train acc:0.12666666666666668, test acc:0.0969 ===\n",
      "train loss:2.2859136734966503\n",
      "train loss:2.320434074887247\n",
      "train loss:2.3022566690845334\n",
      "=== epoch:16, train acc:0.13333333333333333, test acc:0.105 ===\n",
      "train loss:2.303994382649848\n",
      "train loss:2.2981563636787774\n",
      "train loss:2.278755644740708\n",
      "=== epoch:17, train acc:0.13666666666666666, test acc:0.1095 ===\n",
      "train loss:2.292802302269951\n",
      "train loss:2.2967010449612086\n",
      "train loss:2.297434898716603\n",
      "=== epoch:18, train acc:0.14, test acc:0.1122 ===\n",
      "train loss:2.307140414078851\n",
      "train loss:2.3112541114077234\n",
      "train loss:2.2912007308676308\n",
      "=== epoch:19, train acc:0.14666666666666667, test acc:0.1173 ===\n",
      "train loss:2.305011525489231\n",
      "train loss:2.2933798585530156\n",
      "train loss:2.2842759863784563\n",
      "=== epoch:20, train acc:0.14666666666666667, test acc:0.1212 ===\n",
      "train loss:2.294419306615848\n",
      "train loss:2.29882546005831\n",
      "train loss:2.30492862773529\n",
      "=== epoch:21, train acc:0.15, test acc:0.1253 ===\n",
      "train loss:2.3081045476194086\n",
      "train loss:2.283894393780756\n",
      "train loss:2.291670885938608\n",
      "=== epoch:22, train acc:0.15666666666666668, test acc:0.1295 ===\n",
      "train loss:2.2890985760190974\n",
      "train loss:2.2840575010962896\n",
      "train loss:2.2822402773763595\n",
      "=== epoch:23, train acc:0.16, test acc:0.1336 ===\n",
      "train loss:2.271743863077227\n",
      "train loss:2.280909345191916\n",
      "train loss:2.2788580115653647\n",
      "=== epoch:24, train acc:0.15333333333333332, test acc:0.1396 ===\n",
      "train loss:2.2962624753864516\n",
      "train loss:2.2857594410543243\n",
      "train loss:2.2734956258291743\n",
      "=== epoch:25, train acc:0.16, test acc:0.1433 ===\n",
      "train loss:2.281196733898885\n",
      "train loss:2.2855165343491333\n",
      "train loss:2.272800202200122\n",
      "=== epoch:26, train acc:0.17, test acc:0.146 ===\n",
      "train loss:2.2710752373125755\n",
      "train loss:2.2708058215894718\n",
      "train loss:2.281390365009942\n",
      "=== epoch:27, train acc:0.17666666666666667, test acc:0.149 ===\n",
      "train loss:2.276096368479296\n",
      "train loss:2.2868398592049792\n",
      "train loss:2.2533021288181128\n",
      "=== epoch:28, train acc:0.18, test acc:0.1531 ===\n",
      "train loss:2.279790935445175\n",
      "train loss:2.290175069849667\n",
      "train loss:2.28755381923219\n",
      "=== epoch:29, train acc:0.18666666666666668, test acc:0.1584 ===\n",
      "train loss:2.268625340428652\n",
      "train loss:2.271549787381626\n",
      "train loss:2.277468101452462\n",
      "=== epoch:30, train acc:0.20666666666666667, test acc:0.1625 ===\n",
      "train loss:2.274422617681268\n",
      "train loss:2.27950797654735\n",
      "train loss:2.2595019502888984\n",
      "=== epoch:31, train acc:0.21, test acc:0.1682 ===\n",
      "train loss:2.2547736778519254\n",
      "train loss:2.2955682215285855\n",
      "train loss:2.281911820002909\n",
      "=== epoch:32, train acc:0.21, test acc:0.1706 ===\n",
      "train loss:2.2849632248118397\n",
      "train loss:2.270001897878741\n",
      "train loss:2.2721893742474184\n",
      "=== epoch:33, train acc:0.20666666666666667, test acc:0.1744 ===\n",
      "train loss:2.2702002840967537\n",
      "train loss:2.2636977488932697\n",
      "train loss:2.2457445209173006\n",
      "=== epoch:34, train acc:0.21333333333333335, test acc:0.1782 ===\n",
      "train loss:2.282174901289564\n",
      "train loss:2.2719907929706573\n",
      "train loss:2.278374323648339\n",
      "=== epoch:35, train acc:0.21333333333333335, test acc:0.1822 ===\n",
      "train loss:2.2780296393134916\n",
      "train loss:2.260936883419436\n",
      "train loss:2.2791379144450414\n",
      "=== epoch:36, train acc:0.21333333333333335, test acc:0.1847 ===\n",
      "train loss:2.2682905978520376\n",
      "train loss:2.244083967617716\n",
      "train loss:2.284818421389002\n",
      "=== epoch:37, train acc:0.22666666666666666, test acc:0.1884 ===\n",
      "train loss:2.2703278778363924\n",
      "train loss:2.27787084368482\n",
      "train loss:2.264595328350379\n",
      "=== epoch:38, train acc:0.23, test acc:0.192 ===\n",
      "train loss:2.2564862825603043\n",
      "train loss:2.248164370992879\n",
      "train loss:2.2527666356919998\n",
      "=== epoch:39, train acc:0.23333333333333334, test acc:0.1943 ===\n",
      "train loss:2.2527864764123833\n",
      "train loss:2.2609731773162323\n",
      "train loss:2.2690263147617125\n",
      "=== epoch:40, train acc:0.23333333333333334, test acc:0.1952 ===\n",
      "train loss:2.2728311806303987\n",
      "train loss:2.2538989367626523\n",
      "train loss:2.2480245940540566\n",
      "=== epoch:41, train acc:0.23333333333333334, test acc:0.1975 ===\n",
      "train loss:2.265938876735138\n",
      "train loss:2.2582592056648663\n",
      "train loss:2.2479282408121732\n",
      "=== epoch:42, train acc:0.23333333333333334, test acc:0.1964 ===\n",
      "train loss:2.266068436358752\n",
      "train loss:2.2576908657265338\n",
      "train loss:2.274389230169998\n",
      "=== epoch:43, train acc:0.23333333333333334, test acc:0.1976 ===\n",
      "train loss:2.2699133277976635\n",
      "train loss:2.2822305782291945\n",
      "train loss:2.2583668340850913\n",
      "=== epoch:44, train acc:0.24, test acc:0.2027 ===\n",
      "train loss:2.2614999803016334\n",
      "train loss:2.2715128262870534\n",
      "train loss:2.2588317789592836\n",
      "=== epoch:45, train acc:0.25333333333333335, test acc:0.2062 ===\n",
      "train loss:2.2638318019294044\n",
      "train loss:2.2537424854780173\n",
      "train loss:2.2731680110310455\n",
      "=== epoch:46, train acc:0.25333333333333335, test acc:0.2067 ===\n",
      "train loss:2.2671178394983924\n",
      "train loss:2.2449131639062205\n",
      "train loss:2.2437541809558295\n",
      "=== epoch:47, train acc:0.25666666666666665, test acc:0.2074 ===\n",
      "train loss:2.263695618550946\n",
      "train loss:2.244976087974502\n",
      "train loss:2.2411468468562883\n",
      "=== epoch:48, train acc:0.25333333333333335, test acc:0.2082 ===\n",
      "train loss:2.26033814844771\n",
      "train loss:2.242678644044038\n",
      "train loss:2.248558420132348\n",
      "=== epoch:49, train acc:0.25, test acc:0.2068 ===\n",
      "train loss:2.2207890938560095\n",
      "train loss:2.2312159532346563\n",
      "train loss:2.2579461674859767\n",
      "=== epoch:50, train acc:0.25333333333333335, test acc:0.2077 ===\n",
      "train loss:2.2477645504298174\n",
      "train loss:2.2476478990710826\n",
      "train loss:2.263757677075739\n",
      "=== epoch:51, train acc:0.25, test acc:0.2086 ===\n",
      "train loss:2.2348103691313668\n",
      "train loss:2.2545860567012075\n",
      "train loss:2.255671015156717\n",
      "=== epoch:52, train acc:0.25666666666666665, test acc:0.2096 ===\n",
      "train loss:2.2441162497387506\n",
      "train loss:2.255375576433403\n",
      "train loss:2.2346755166153973\n",
      "=== epoch:53, train acc:0.26, test acc:0.2108 ===\n",
      "train loss:2.2570070909264732\n",
      "train loss:2.2245861219591716\n",
      "train loss:2.2565773725303098\n",
      "=== epoch:54, train acc:0.26, test acc:0.2117 ===\n",
      "train loss:2.226096569672674\n",
      "train loss:2.2218325076971666\n",
      "train loss:2.227156394991192\n",
      "=== epoch:55, train acc:0.25666666666666665, test acc:0.2105 ===\n",
      "train loss:2.2244379733138535\n",
      "train loss:2.2325094973723605\n",
      "train loss:2.241027209234398\n",
      "=== epoch:56, train acc:0.25666666666666665, test acc:0.2097 ===\n",
      "train loss:2.260287003375664\n",
      "train loss:2.229487178465668\n",
      "train loss:2.220148957477374\n",
      "=== epoch:57, train acc:0.25666666666666665, test acc:0.2094 ===\n",
      "train loss:2.239870454972404\n",
      "train loss:2.235996812833804\n",
      "train loss:2.229316333709944\n",
      "=== epoch:58, train acc:0.25666666666666665, test acc:0.2091 ===\n",
      "train loss:2.233170338009764\n",
      "train loss:2.259041256588281\n",
      "train loss:2.2487296817763007\n",
      "=== epoch:59, train acc:0.26, test acc:0.2097 ===\n",
      "train loss:2.228170631739541\n",
      "train loss:2.2589295915082332\n",
      "train loss:2.244326933571112\n",
      "=== epoch:60, train acc:0.2633333333333333, test acc:0.2126 ===\n",
      "train loss:2.2364859471415284\n",
      "train loss:2.2225963595525378\n",
      "train loss:2.2464349094017746\n",
      "=== epoch:61, train acc:0.2633333333333333, test acc:0.2136 ===\n",
      "train loss:2.2360741988667225\n",
      "train loss:2.2176610983003044\n",
      "train loss:2.2504923570537336\n",
      "=== epoch:62, train acc:0.25666666666666665, test acc:0.2161 ===\n",
      "train loss:2.2377980569671685\n",
      "train loss:2.2149168805210655\n",
      "train loss:2.2266473653024543\n",
      "=== epoch:63, train acc:0.26, test acc:0.2161 ===\n",
      "train loss:2.257205434155905\n",
      "train loss:2.23834838368647\n",
      "train loss:2.2165933792455035\n",
      "=== epoch:64, train acc:0.26, test acc:0.2179 ===\n",
      "train loss:2.21724329659052\n",
      "train loss:2.212005481209344\n",
      "train loss:2.2086091616077375\n",
      "=== epoch:65, train acc:0.26, test acc:0.2154 ===\n",
      "train loss:2.19237124426394\n",
      "train loss:2.1841162031880064\n",
      "train loss:2.225928661918044\n",
      "=== epoch:66, train acc:0.25333333333333335, test acc:0.2138 ===\n",
      "train loss:2.2489977310895224\n",
      "train loss:2.1993606346466934\n",
      "train loss:2.2489438258992642\n",
      "=== epoch:67, train acc:0.25333333333333335, test acc:0.2148 ===\n",
      "train loss:2.2274704687524407\n",
      "train loss:2.2267277709549926\n",
      "train loss:2.2124939673748223\n",
      "=== epoch:68, train acc:0.25666666666666665, test acc:0.2154 ===\n",
      "train loss:2.21276779651636\n",
      "train loss:2.223876646024247\n",
      "train loss:2.2140444856538646\n",
      "=== epoch:69, train acc:0.25666666666666665, test acc:0.2163 ===\n",
      "train loss:2.1904753882346104\n",
      "train loss:2.2475553205352745\n",
      "train loss:2.1829183218013553\n",
      "=== epoch:70, train acc:0.25666666666666665, test acc:0.2167 ===\n",
      "train loss:2.221431769059627\n",
      "train loss:2.239988305199648\n",
      "train loss:2.1887003426742577\n",
      "=== epoch:71, train acc:0.25333333333333335, test acc:0.2168 ===\n",
      "train loss:2.2314761993500203\n",
      "train loss:2.235210919162627\n",
      "train loss:2.194756686548659\n",
      "=== epoch:72, train acc:0.25666666666666665, test acc:0.2173 ===\n",
      "train loss:2.2058456540427045\n",
      "train loss:2.2334618712692835\n",
      "train loss:2.1839681190162086\n",
      "=== epoch:73, train acc:0.27, test acc:0.2188 ===\n",
      "train loss:2.2102695288167293\n",
      "train loss:2.168932264118608\n",
      "train loss:2.2454311199247416\n",
      "=== epoch:74, train acc:0.2633333333333333, test acc:0.2189 ===\n",
      "train loss:2.2210089627153304\n",
      "train loss:2.2127253844490204\n",
      "train loss:2.187347815206704\n",
      "=== epoch:75, train acc:0.2633333333333333, test acc:0.2196 ===\n",
      "train loss:2.209102544892577\n",
      "train loss:2.1826570566323915\n",
      "train loss:2.2309548372006645\n",
      "=== epoch:76, train acc:0.27, test acc:0.2198 ===\n",
      "train loss:2.1992447116750014\n",
      "train loss:2.180957522375729\n",
      "train loss:2.218867388209114\n",
      "=== epoch:77, train acc:0.26666666666666666, test acc:0.2198 ===\n",
      "train loss:2.1675144642464685\n",
      "train loss:2.2068994329399634\n",
      "train loss:2.2018092638357847\n",
      "=== epoch:78, train acc:0.2633333333333333, test acc:0.2188 ===\n",
      "train loss:2.1847385704055253\n",
      "train loss:2.180533813260938\n",
      "train loss:2.1697845427004205\n",
      "=== epoch:79, train acc:0.27, test acc:0.2184 ===\n",
      "train loss:2.2072852284818087\n",
      "train loss:2.1970507122089566\n",
      "train loss:2.1797600512638184\n",
      "=== epoch:80, train acc:0.2633333333333333, test acc:0.2185 ===\n",
      "train loss:2.2161420395002245\n",
      "train loss:2.2094001411863213\n",
      "train loss:2.2460413384658\n",
      "=== epoch:81, train acc:0.2633333333333333, test acc:0.2201 ===\n",
      "train loss:2.1806652633553365\n",
      "train loss:2.198152397792485\n",
      "train loss:2.1607521011605084\n",
      "=== epoch:82, train acc:0.2633333333333333, test acc:0.2188 ===\n",
      "train loss:2.1807034162148136\n",
      "train loss:2.2130427621985707\n",
      "train loss:2.154109095438752\n",
      "=== epoch:83, train acc:0.26, test acc:0.2188 ===\n",
      "train loss:2.189663737590424\n",
      "train loss:2.155494630287265\n",
      "train loss:2.2159501574240745\n",
      "=== epoch:84, train acc:0.26, test acc:0.2198 ===\n",
      "train loss:2.162539240008073\n",
      "train loss:2.195543512092732\n",
      "train loss:2.1853185353311493\n",
      "=== epoch:85, train acc:0.2633333333333333, test acc:0.2208 ===\n",
      "train loss:2.1600057055782638\n",
      "train loss:2.1818113822289478\n",
      "train loss:2.1328014536694466\n",
      "=== epoch:86, train acc:0.2633333333333333, test acc:0.2193 ===\n",
      "train loss:2.1887660681677885\n",
      "train loss:2.1884111205495524\n",
      "train loss:2.193076194970264\n",
      "=== epoch:87, train acc:0.26, test acc:0.2204 ===\n",
      "train loss:2.152790054758269\n",
      "train loss:2.187410599046354\n",
      "train loss:2.1901313150457593\n",
      "=== epoch:88, train acc:0.2633333333333333, test acc:0.2205 ===\n",
      "train loss:2.167731209783988\n",
      "train loss:2.1935874284839825\n",
      "train loss:2.1537553190642615\n",
      "=== epoch:89, train acc:0.26, test acc:0.2211 ===\n",
      "train loss:2.150969900292231\n",
      "train loss:2.193534128106561\n",
      "train loss:2.161949285578587\n",
      "=== epoch:90, train acc:0.2633333333333333, test acc:0.2218 ===\n",
      "train loss:2.163776864663713\n",
      "train loss:2.187102902202226\n",
      "train loss:2.136568041162818\n",
      "=== epoch:91, train acc:0.26666666666666666, test acc:0.2227 ===\n",
      "train loss:2.149692629425911\n",
      "train loss:2.1352815933093523\n",
      "train loss:2.1981598394195743\n",
      "=== epoch:92, train acc:0.2733333333333333, test acc:0.2232 ===\n",
      "train loss:2.144301004755044\n",
      "train loss:2.2054131763238263\n",
      "train loss:2.1654375496545053\n",
      "=== epoch:93, train acc:0.2733333333333333, test acc:0.2233 ===\n",
      "train loss:2.1408959125166023\n",
      "train loss:2.2156218333940774\n",
      "train loss:2.1763032139023584\n",
      "=== epoch:94, train acc:0.28, test acc:0.224 ===\n",
      "train loss:2.147698204758112\n",
      "train loss:2.183318985046108\n",
      "train loss:2.118187234473891\n",
      "=== epoch:95, train acc:0.28, test acc:0.2251 ===\n",
      "train loss:2.1444901122248985\n",
      "train loss:2.1933666832314045\n",
      "train loss:2.124085736784169\n",
      "=== epoch:96, train acc:0.2833333333333333, test acc:0.2252 ===\n",
      "train loss:2.152872063824104\n",
      "train loss:2.1633217497260144\n",
      "train loss:2.173299458971835\n",
      "=== epoch:97, train acc:0.2833333333333333, test acc:0.2261 ===\n",
      "train loss:2.1060404749821835\n",
      "train loss:2.1602194770159246\n",
      "train loss:2.1683339156636467\n",
      "=== epoch:98, train acc:0.27666666666666667, test acc:0.2262 ===\n",
      "train loss:2.1394693562006832\n",
      "train loss:2.1734312638358966\n",
      "train loss:2.1380837001883064\n",
      "=== epoch:99, train acc:0.2833333333333333, test acc:0.227 ===\n",
      "train loss:2.155884162173334\n",
      "train loss:2.1427548765428406\n",
      "train loss:2.0945199304591653\n",
      "=== epoch:100, train acc:0.27666666666666667, test acc:0.2262 ===\n",
      "train loss:2.1313715872888093\n",
      "train loss:2.1608023802178153\n",
      "train loss:2.075899404413534\n",
      "=== epoch:101, train acc:0.27, test acc:0.2252 ===\n",
      "train loss:2.1304498390585507\n",
      "train loss:2.0392660263772164\n",
      "train loss:2.0775353393252787\n",
      "=== epoch:102, train acc:0.27, test acc:0.2238 ===\n",
      "train loss:2.063112995513289\n",
      "train loss:2.140493741714319\n",
      "train loss:2.1104146611017107\n",
      "=== epoch:103, train acc:0.2733333333333333, test acc:0.2236 ===\n",
      "train loss:2.1176756524604308\n",
      "train loss:2.177458480517272\n",
      "train loss:2.0863229827353367\n",
      "=== epoch:104, train acc:0.2833333333333333, test acc:0.2248 ===\n",
      "train loss:2.0859330117226564\n",
      "train loss:2.122782503716861\n",
      "train loss:2.0864938964362127\n",
      "=== epoch:105, train acc:0.27666666666666667, test acc:0.2247 ===\n",
      "train loss:2.081051745566688\n",
      "train loss:2.1361028681037437\n",
      "train loss:2.22147822839819\n",
      "=== epoch:106, train acc:0.27666666666666667, test acc:0.2256 ===\n",
      "train loss:2.0015204448745227\n",
      "train loss:2.1104614728950937\n",
      "train loss:2.113990270074682\n",
      "=== epoch:107, train acc:0.27666666666666667, test acc:0.2249 ===\n",
      "train loss:2.065367547469283\n",
      "train loss:2.1381304178186964\n",
      "train loss:2.0455887997103117\n",
      "=== epoch:108, train acc:0.28, test acc:0.2251 ===\n",
      "train loss:2.06493140690479\n",
      "train loss:2.0508167423796717\n",
      "train loss:2.1529074587919537\n",
      "=== epoch:109, train acc:0.28, test acc:0.2255 ===\n",
      "train loss:2.1667413655310095\n",
      "train loss:2.103452449869255\n",
      "train loss:2.0790366549237493\n",
      "=== epoch:110, train acc:0.2833333333333333, test acc:0.2263 ===\n",
      "train loss:2.1774945792404194\n",
      "train loss:2.0385256422624334\n",
      "train loss:2.0629139150764084\n",
      "=== epoch:111, train acc:0.2833333333333333, test acc:0.2266 ===\n",
      "train loss:2.074240722121803\n",
      "train loss:2.1010259158418596\n",
      "train loss:2.0074007529022735\n",
      "=== epoch:112, train acc:0.2866666666666667, test acc:0.2265 ===\n",
      "train loss:2.1528450740157044\n",
      "train loss:2.08511780682749\n",
      "train loss:2.0901948848345926\n",
      "=== epoch:113, train acc:0.2866666666666667, test acc:0.2278 ===\n",
      "train loss:2.166109902076436\n",
      "train loss:2.09336347639171\n",
      "train loss:2.1613331299208287\n",
      "=== epoch:114, train acc:0.29, test acc:0.2308 ===\n",
      "train loss:2.002040289162185\n",
      "train loss:2.097397864386531\n",
      "train loss:2.098147022018419\n",
      "=== epoch:115, train acc:0.2966666666666667, test acc:0.2318 ===\n",
      "train loss:2.0381972726258972\n",
      "train loss:2.15172476114923\n",
      "train loss:2.1027479429246907\n",
      "=== epoch:116, train acc:0.2966666666666667, test acc:0.2334 ===\n",
      "train loss:2.07739882818642\n",
      "train loss:2.135866705591904\n",
      "train loss:2.1189283073357306\n",
      "=== epoch:117, train acc:0.29, test acc:0.2338 ===\n",
      "train loss:2.1116926320733493\n",
      "train loss:2.099161725550368\n",
      "train loss:2.0873500758779087\n",
      "=== epoch:118, train acc:0.29333333333333333, test acc:0.2351 ===\n",
      "train loss:2.099495182896229\n",
      "train loss:2.0126648327487664\n",
      "train loss:2.0601573227547645\n",
      "=== epoch:119, train acc:0.29, test acc:0.2351 ===\n",
      "train loss:2.074034520229973\n",
      "train loss:2.1378415870297904\n",
      "train loss:2.1141976674230287\n",
      "=== epoch:120, train acc:0.2966666666666667, test acc:0.2383 ===\n",
      "train loss:2.1194542249195916\n",
      "train loss:2.0985911423683614\n",
      "train loss:2.122046816594318\n",
      "=== epoch:121, train acc:0.30666666666666664, test acc:0.2393 ===\n",
      "train loss:2.079052667678309\n",
      "train loss:2.046963236671442\n",
      "train loss:2.1335675274216577\n",
      "=== epoch:122, train acc:0.31, test acc:0.2421 ===\n",
      "train loss:2.127066274108861\n",
      "train loss:2.100447609300942\n",
      "train loss:2.098331735511033\n",
      "=== epoch:123, train acc:0.31, test acc:0.244 ===\n",
      "train loss:2.0469081741676955\n",
      "train loss:2.1039289102085332\n",
      "train loss:1.9575786135807827\n",
      "=== epoch:124, train acc:0.31, test acc:0.243 ===\n",
      "train loss:1.97676936296635\n",
      "train loss:2.056448119174276\n",
      "train loss:2.070794672709109\n",
      "=== epoch:125, train acc:0.31, test acc:0.2439 ===\n",
      "train loss:1.9718699908195887\n",
      "train loss:2.1200802686016287\n",
      "train loss:2.014578938068621\n",
      "=== epoch:126, train acc:0.31, test acc:0.2446 ===\n",
      "train loss:1.9547181661505006\n",
      "train loss:2.0686912310767838\n",
      "train loss:2.1844177786643537\n",
      "=== epoch:127, train acc:0.30666666666666664, test acc:0.2438 ===\n",
      "train loss:1.9812634982241502\n",
      "train loss:2.15078634646344\n",
      "train loss:2.0881807215005583\n",
      "=== epoch:128, train acc:0.31, test acc:0.2438 ===\n",
      "train loss:2.034935910585923\n",
      "train loss:2.0164586052624873\n",
      "train loss:2.112468141598271\n",
      "=== epoch:129, train acc:0.31, test acc:0.2449 ===\n",
      "train loss:2.0907644950072735\n",
      "train loss:1.9975399152081927\n",
      "train loss:2.0736024599035736\n",
      "=== epoch:130, train acc:0.31, test acc:0.2464 ===\n",
      "train loss:1.9387292142127248\n",
      "train loss:2.041868755617978\n",
      "train loss:2.046324630042435\n",
      "=== epoch:131, train acc:0.31, test acc:0.2473 ===\n",
      "train loss:1.9727510993177229\n",
      "train loss:2.043531455268269\n",
      "train loss:2.0240402732367677\n",
      "=== epoch:132, train acc:0.31, test acc:0.2458 ===\n",
      "train loss:2.04070453818416\n",
      "train loss:2.079140562625451\n",
      "train loss:2.021468476008009\n",
      "=== epoch:133, train acc:0.31666666666666665, test acc:0.2476 ===\n",
      "train loss:1.935517946457607\n",
      "train loss:2.051982446203123\n",
      "train loss:2.100136613442308\n",
      "=== epoch:134, train acc:0.31333333333333335, test acc:0.2495 ===\n",
      "train loss:1.9448120641057778\n",
      "train loss:2.0112830470003633\n",
      "train loss:2.057390431157715\n",
      "=== epoch:135, train acc:0.31333333333333335, test acc:0.2484 ===\n",
      "train loss:2.0341794582941435\n",
      "train loss:2.025210458234844\n",
      "train loss:2.0281735103776906\n",
      "=== epoch:136, train acc:0.3233333333333333, test acc:0.2488 ===\n",
      "train loss:1.969998278174036\n",
      "train loss:2.0030721293636775\n",
      "train loss:1.876649732732676\n",
      "=== epoch:137, train acc:0.31666666666666665, test acc:0.2479 ===\n",
      "train loss:2.023792049309528\n",
      "train loss:2.099909568899363\n",
      "train loss:2.0489493873325912\n",
      "=== epoch:138, train acc:0.31666666666666665, test acc:0.2477 ===\n",
      "train loss:2.045474970547569\n",
      "train loss:1.9696447491452977\n",
      "train loss:1.9603566304596434\n",
      "=== epoch:139, train acc:0.31666666666666665, test acc:0.2479 ===\n",
      "train loss:1.9298739300918757\n",
      "train loss:2.028965973441627\n",
      "train loss:1.9836225923680686\n",
      "=== epoch:140, train acc:0.31666666666666665, test acc:0.2485 ===\n",
      "train loss:2.0368180039384147\n",
      "train loss:2.0666601418106563\n",
      "train loss:2.052390861421367\n",
      "=== epoch:141, train acc:0.31666666666666665, test acc:0.2503 ===\n",
      "train loss:2.039428538652129\n",
      "train loss:2.1011665299140505\n",
      "train loss:1.9527534298844116\n",
      "=== epoch:142, train acc:0.33, test acc:0.2534 ===\n",
      "train loss:1.9917154522367349\n",
      "train loss:1.989418501654413\n",
      "train loss:2.0873160884058253\n",
      "=== epoch:143, train acc:0.33666666666666667, test acc:0.2564 ===\n",
      "train loss:2.063290643753276\n",
      "train loss:1.9756256132963648\n",
      "train loss:1.9965410166665116\n",
      "=== epoch:144, train acc:0.3433333333333333, test acc:0.2585 ===\n",
      "train loss:1.9643850841753834\n",
      "train loss:2.0175293728607238\n",
      "train loss:2.0103351770632343\n",
      "=== epoch:145, train acc:0.34, test acc:0.2592 ===\n",
      "train loss:2.1078657793706843\n",
      "train loss:1.8411574963386443\n",
      "train loss:1.922042281137775\n",
      "=== epoch:146, train acc:0.34, test acc:0.261 ===\n",
      "train loss:2.0825775896245937\n",
      "train loss:2.126813271397813\n",
      "train loss:1.9528955552909528\n",
      "=== epoch:147, train acc:0.34, test acc:0.2611 ===\n",
      "train loss:2.03098018321778\n",
      "train loss:1.9632901055441285\n",
      "train loss:1.862457814085648\n",
      "=== epoch:148, train acc:0.34, test acc:0.2609 ===\n",
      "train loss:1.9685806847460605\n",
      "train loss:1.923425037316148\n",
      "train loss:1.957268510494084\n",
      "=== epoch:149, train acc:0.34, test acc:0.2611 ===\n",
      "train loss:2.043878630017103\n",
      "train loss:1.9744954960327386\n",
      "train loss:1.9848518415320993\n",
      "=== epoch:150, train acc:0.34, test acc:0.2622 ===\n",
      "train loss:2.051841403997525\n",
      "train loss:2.011274044477478\n",
      "train loss:1.8818355118043169\n",
      "=== epoch:151, train acc:0.34, test acc:0.261 ===\n",
      "train loss:2.0560602621623874\n",
      "train loss:2.004623926602401\n",
      "train loss:1.8932646503316422\n",
      "=== epoch:152, train acc:0.33666666666666667, test acc:0.2601 ===\n",
      "train loss:1.9834598038826332\n",
      "train loss:2.0182768513882636\n",
      "train loss:1.9986896365191595\n",
      "=== epoch:153, train acc:0.3333333333333333, test acc:0.2632 ===\n",
      "train loss:1.969720678927994\n",
      "train loss:1.9445517208049543\n",
      "train loss:1.9277266381018017\n",
      "=== epoch:154, train acc:0.34, test acc:0.2638 ===\n",
      "train loss:2.0774724473669233\n",
      "train loss:1.9641365271450433\n",
      "train loss:1.9957655126585268\n",
      "=== epoch:155, train acc:0.34, test acc:0.2662 ===\n",
      "train loss:1.9375892922085827\n",
      "train loss:2.010059318934352\n",
      "train loss:1.9364413587878533\n",
      "=== epoch:156, train acc:0.33666666666666667, test acc:0.2649 ===\n",
      "train loss:1.9068950820392507\n",
      "train loss:2.0026559727655995\n",
      "train loss:2.0502203811321165\n",
      "=== epoch:157, train acc:0.33, test acc:0.2662 ===\n",
      "train loss:1.8170210330517087\n",
      "train loss:1.9443773176379884\n",
      "train loss:1.919411704245374\n",
      "=== epoch:158, train acc:0.33666666666666667, test acc:0.2669 ===\n",
      "train loss:1.920649279768768\n",
      "train loss:1.9184216373899974\n",
      "train loss:2.024490190710448\n",
      "=== epoch:159, train acc:0.34, test acc:0.2678 ===\n",
      "train loss:1.951044152023751\n",
      "train loss:2.0249130007591716\n",
      "train loss:1.9660162515312378\n",
      "=== epoch:160, train acc:0.3466666666666667, test acc:0.2711 ===\n",
      "train loss:1.8972348726775836\n",
      "train loss:1.990833810475007\n",
      "train loss:1.8555055491948909\n",
      "=== epoch:161, train acc:0.3433333333333333, test acc:0.2706 ===\n",
      "train loss:2.0225194549776826\n",
      "train loss:1.9440140220076108\n",
      "train loss:1.9980009538684869\n",
      "=== epoch:162, train acc:0.3466666666666667, test acc:0.2708 ===\n",
      "train loss:1.9254272086466748\n",
      "train loss:2.0307065173792918\n",
      "train loss:2.008381596962002\n",
      "=== epoch:163, train acc:0.3466666666666667, test acc:0.2735 ===\n",
      "train loss:2.016539412022592\n",
      "train loss:1.9956689777990155\n",
      "train loss:1.984170093366145\n",
      "=== epoch:164, train acc:0.3466666666666667, test acc:0.2751 ===\n",
      "train loss:1.9411797704450244\n",
      "train loss:1.96764408593041\n",
      "train loss:2.048926673388958\n",
      "=== epoch:165, train acc:0.35, test acc:0.2766 ===\n",
      "train loss:1.9499846242655454\n",
      "train loss:1.9130460257802702\n",
      "train loss:1.9710619234028592\n",
      "=== epoch:166, train acc:0.35, test acc:0.2763 ===\n",
      "train loss:1.9043802241945065\n",
      "train loss:1.9583706004181962\n",
      "train loss:1.9031245195243744\n",
      "=== epoch:167, train acc:0.3466666666666667, test acc:0.2758 ===\n",
      "train loss:1.9062347210662154\n",
      "train loss:1.9493844956250794\n",
      "train loss:1.9071355394175773\n",
      "=== epoch:168, train acc:0.3466666666666667, test acc:0.2768 ===\n",
      "train loss:2.0240281957036705\n",
      "train loss:1.9561819635947373\n",
      "train loss:1.8399586607735658\n",
      "=== epoch:169, train acc:0.35, test acc:0.2795 ===\n",
      "train loss:1.9896012927631008\n",
      "train loss:1.830988595568251\n",
      "train loss:1.9382594910162485\n",
      "=== epoch:170, train acc:0.35, test acc:0.2793 ===\n",
      "train loss:2.012869529899594\n",
      "train loss:1.9722668865049164\n",
      "train loss:1.84372392981833\n",
      "=== epoch:171, train acc:0.35, test acc:0.2799 ===\n",
      "train loss:1.9659629510542826\n",
      "train loss:1.8762033576907549\n",
      "train loss:1.9110343407405472\n",
      "=== epoch:172, train acc:0.3466666666666667, test acc:0.2791 ===\n",
      "train loss:1.9935599130037718\n",
      "train loss:1.9677044010921336\n",
      "train loss:1.9570530536060875\n",
      "=== epoch:173, train acc:0.3466666666666667, test acc:0.28 ===\n",
      "train loss:1.9050336236965477\n",
      "train loss:1.8893146808603987\n",
      "train loss:1.937248951207443\n",
      "=== epoch:174, train acc:0.3566666666666667, test acc:0.2812 ===\n",
      "train loss:2.0129007687434908\n",
      "train loss:1.8507440880477242\n",
      "train loss:1.9172139358070337\n",
      "=== epoch:175, train acc:0.3566666666666667, test acc:0.2821 ===\n",
      "train loss:1.8611484880791236\n",
      "train loss:1.9842011083211926\n",
      "train loss:1.9702341877582854\n",
      "=== epoch:176, train acc:0.3566666666666667, test acc:0.2831 ===\n",
      "train loss:1.9505687961453058\n",
      "train loss:1.8790608865088543\n",
      "train loss:1.9202845450752322\n",
      "=== epoch:177, train acc:0.36, test acc:0.2855 ===\n",
      "train loss:1.9166693275231856\n",
      "train loss:1.8575061650223998\n",
      "train loss:1.8711489303742994\n",
      "=== epoch:178, train acc:0.36, test acc:0.2837 ===\n",
      "train loss:1.8463088943000472\n",
      "train loss:1.8510470957129552\n",
      "train loss:1.8779748255987232\n",
      "=== epoch:179, train acc:0.37333333333333335, test acc:0.2847 ===\n",
      "train loss:1.8907847935715134\n",
      "train loss:1.868600067831465\n",
      "train loss:1.970452296562895\n",
      "=== epoch:180, train acc:0.38, test acc:0.2861 ===\n",
      "train loss:1.8674366822881316\n",
      "train loss:1.921144585535963\n",
      "train loss:1.8807798815014383\n",
      "=== epoch:181, train acc:0.37666666666666665, test acc:0.2867 ===\n",
      "train loss:1.8874599954257691\n",
      "train loss:1.8717509249621196\n",
      "train loss:1.8045921844238015\n",
      "=== epoch:182, train acc:0.36, test acc:0.2877 ===\n",
      "train loss:1.8994162210398318\n",
      "train loss:1.8791350420075872\n",
      "train loss:1.8480320991371042\n",
      "=== epoch:183, train acc:0.36, test acc:0.2879 ===\n",
      "train loss:1.799536452818469\n",
      "train loss:1.89795201733964\n",
      "train loss:1.7648443495581463\n",
      "=== epoch:184, train acc:0.36666666666666664, test acc:0.2887 ===\n",
      "train loss:1.835462161742262\n",
      "train loss:1.8294867440417215\n",
      "train loss:1.8984998467611496\n",
      "=== epoch:185, train acc:0.36, test acc:0.2867 ===\n",
      "train loss:1.8943726695842509\n",
      "train loss:1.8173568580152093\n",
      "train loss:1.8470304534702426\n",
      "=== epoch:186, train acc:0.36, test acc:0.2859 ===\n",
      "train loss:2.0573224500330896\n",
      "train loss:1.8266063368945584\n",
      "train loss:1.841408771074559\n",
      "=== epoch:187, train acc:0.36, test acc:0.2853 ===\n",
      "train loss:1.7540531751483739\n",
      "train loss:1.8568064584595232\n",
      "train loss:1.8809472035617443\n",
      "=== epoch:188, train acc:0.36333333333333334, test acc:0.2877 ===\n",
      "train loss:1.9003421485644183\n",
      "train loss:1.8108746974278713\n",
      "train loss:1.9389650756589378\n",
      "=== epoch:189, train acc:0.37, test acc:0.2886 ===\n",
      "train loss:1.7805326809540967\n",
      "train loss:1.8933837060774965\n",
      "train loss:1.7942781465820434\n",
      "=== epoch:190, train acc:0.36666666666666664, test acc:0.288 ===\n",
      "train loss:1.9647016076531691\n",
      "train loss:1.825778087500078\n",
      "train loss:1.9638159705018725\n",
      "=== epoch:191, train acc:0.37, test acc:0.2924 ===\n",
      "train loss:1.8156817237376737\n",
      "train loss:1.7638653091807088\n",
      "train loss:1.8116257598243088\n",
      "=== epoch:192, train acc:0.37, test acc:0.2937 ===\n",
      "train loss:1.7049855380350447\n",
      "train loss:1.9221692803534238\n",
      "train loss:1.9353170293217807\n",
      "=== epoch:193, train acc:0.37333333333333335, test acc:0.2957 ===\n",
      "train loss:1.863317876700271\n",
      "train loss:1.8838389028825713\n",
      "train loss:1.7842611297470667\n",
      "=== epoch:194, train acc:0.37666666666666665, test acc:0.2967 ===\n",
      "train loss:1.9797408999485304\n",
      "train loss:1.7568834758081495\n",
      "train loss:1.7691509836117805\n",
      "=== epoch:195, train acc:0.37333333333333335, test acc:0.299 ===\n",
      "train loss:1.8256000732358004\n",
      "train loss:1.816912173124073\n",
      "train loss:1.8375593559073076\n",
      "=== epoch:196, train acc:0.38333333333333336, test acc:0.3003 ===\n",
      "train loss:1.714850077618391\n",
      "train loss:1.832786185623573\n",
      "train loss:1.7963584029557291\n",
      "=== epoch:197, train acc:0.38, test acc:0.3004 ===\n",
      "train loss:1.8169433828888797\n",
      "train loss:1.790845683502479\n",
      "train loss:1.8686575642600989\n",
      "=== epoch:198, train acc:0.38333333333333336, test acc:0.3016 ===\n",
      "train loss:1.8808915858682178\n",
      "train loss:1.7707733025649628\n",
      "train loss:1.6997953886974657\n",
      "=== epoch:199, train acc:0.37666666666666665, test acc:0.3022 ===\n",
      "train loss:1.78716741442463\n",
      "train loss:1.860503622547692\n",
      "train loss:1.7431574547906379\n",
      "=== epoch:200, train acc:0.37333333333333335, test acc:0.3018 ===\n",
      "train loss:1.805818981800991\n",
      "train loss:1.712304758575884\n",
      "train loss:1.8846704156094787\n",
      "=== epoch:201, train acc:0.38, test acc:0.3032 ===\n",
      "train loss:1.931746713962629\n",
      "train loss:1.9230235577928536\n",
      "train loss:1.9097240637508301\n",
      "=== epoch:202, train acc:0.38, test acc:0.3053 ===\n",
      "train loss:1.784183568048063\n",
      "train loss:1.7856396363598708\n",
      "train loss:1.887761601085864\n",
      "=== epoch:203, train acc:0.38333333333333336, test acc:0.3069 ===\n",
      "train loss:1.7529482498255589\n",
      "train loss:1.7266563268134882\n",
      "train loss:1.830276740349573\n",
      "=== epoch:204, train acc:0.3933333333333333, test acc:0.3082 ===\n",
      "train loss:1.7488745909759187\n",
      "train loss:1.7016843469482184\n",
      "train loss:1.796807819614164\n",
      "=== epoch:205, train acc:0.39666666666666667, test acc:0.3097 ===\n",
      "train loss:1.760797631408918\n",
      "train loss:1.66664928355705\n",
      "train loss:1.8840441966964843\n",
      "=== epoch:206, train acc:0.38333333333333336, test acc:0.3067 ===\n",
      "train loss:1.7651570754348782\n",
      "train loss:1.7773472656666778\n",
      "train loss:1.9343544437633116\n",
      "=== epoch:207, train acc:0.3933333333333333, test acc:0.3074 ===\n",
      "train loss:1.8054348882195037\n",
      "train loss:1.7868764714646281\n",
      "train loss:1.8198119532542705\n",
      "=== epoch:208, train acc:0.4, test acc:0.3113 ===\n",
      "train loss:1.7203671129854226\n",
      "train loss:1.980053568475915\n",
      "train loss:1.7555365491073436\n",
      "=== epoch:209, train acc:0.41333333333333333, test acc:0.3133 ===\n",
      "train loss:1.8795404255051842\n",
      "train loss:1.7558985903076008\n",
      "train loss:1.7365088596142109\n",
      "=== epoch:210, train acc:0.4166666666666667, test acc:0.3165 ===\n",
      "train loss:1.8299119620563624\n",
      "train loss:1.8166757523514045\n",
      "train loss:1.8855081645007779\n",
      "=== epoch:211, train acc:0.41333333333333333, test acc:0.3175 ===\n",
      "train loss:1.842641975544713\n",
      "train loss:1.8022630591956763\n",
      "train loss:1.838034928431486\n",
      "=== epoch:212, train acc:0.4166666666666667, test acc:0.319 ===\n",
      "train loss:1.8013702436381749\n",
      "train loss:1.6346987728243247\n",
      "train loss:1.7000235058385857\n",
      "=== epoch:213, train acc:0.42, test acc:0.3217 ===\n",
      "train loss:1.8566781438456335\n",
      "train loss:1.7041221906697357\n",
      "train loss:1.7625717544995407\n",
      "=== epoch:214, train acc:0.4166666666666667, test acc:0.3225 ===\n",
      "train loss:1.746800230106021\n",
      "train loss:1.8154755450924531\n",
      "train loss:1.7999778863489149\n",
      "=== epoch:215, train acc:0.41, test acc:0.3229 ===\n",
      "train loss:1.8232044704176373\n",
      "train loss:1.81867070440437\n",
      "train loss:1.6592537469851931\n",
      "=== epoch:216, train acc:0.41333333333333333, test acc:0.3253 ===\n",
      "train loss:1.7224332593622613\n",
      "train loss:1.797001090942961\n",
      "train loss:1.723563256648315\n",
      "=== epoch:217, train acc:0.4166666666666667, test acc:0.3262 ===\n",
      "train loss:1.9103900455828677\n",
      "train loss:1.6783983670133293\n",
      "train loss:1.8660601099916494\n",
      "=== epoch:218, train acc:0.43, test acc:0.3308 ===\n",
      "train loss:1.7275653044742054\n",
      "train loss:1.6787546585470248\n",
      "train loss:1.7668240229284908\n",
      "=== epoch:219, train acc:0.42333333333333334, test acc:0.331 ===\n",
      "train loss:1.7340448051992294\n",
      "train loss:1.6732536457113159\n",
      "train loss:1.6583273166418457\n",
      "=== epoch:220, train acc:0.4266666666666667, test acc:0.3328 ===\n",
      "train loss:1.7574452942846839\n",
      "train loss:1.6890455031675053\n",
      "train loss:1.7561452273014115\n",
      "=== epoch:221, train acc:0.42, test acc:0.3306 ===\n",
      "train loss:1.7091489149516605\n",
      "train loss:1.6866254825427336\n",
      "train loss:1.7631917968868753\n",
      "=== epoch:222, train acc:0.4266666666666667, test acc:0.3358 ===\n",
      "train loss:1.6456125319685995\n",
      "train loss:1.766322712191973\n",
      "train loss:1.7357799175172948\n",
      "=== epoch:223, train acc:0.4266666666666667, test acc:0.3377 ===\n",
      "train loss:1.8189521838869198\n",
      "train loss:1.706825597566599\n",
      "train loss:1.7142021038678188\n",
      "=== epoch:224, train acc:0.43666666666666665, test acc:0.3394 ===\n",
      "train loss:1.7475017778124853\n",
      "train loss:1.7613123462262328\n",
      "train loss:1.7742829620636533\n",
      "=== epoch:225, train acc:0.43333333333333335, test acc:0.3391 ===\n",
      "train loss:1.7792912879053078\n",
      "train loss:1.54109739289766\n",
      "train loss:1.789763413491916\n",
      "=== epoch:226, train acc:0.4266666666666667, test acc:0.3384 ===\n",
      "train loss:1.7689114172523093\n",
      "train loss:1.7038899744571572\n",
      "train loss:1.6860352836209762\n",
      "=== epoch:227, train acc:0.43, test acc:0.3418 ===\n",
      "train loss:1.6843603791622135\n",
      "train loss:1.6828303320503832\n",
      "train loss:1.6773643324580283\n",
      "=== epoch:228, train acc:0.43666666666666665, test acc:0.344 ===\n",
      "train loss:1.7375537315323513\n",
      "train loss:1.7115442172649833\n",
      "train loss:1.6823257645203253\n",
      "=== epoch:229, train acc:0.44333333333333336, test acc:0.3468 ===\n",
      "train loss:1.843718088671171\n",
      "train loss:1.6469234788670655\n",
      "train loss:1.6492895705839388\n",
      "=== epoch:230, train acc:0.45666666666666667, test acc:0.3527 ===\n",
      "train loss:1.7979535354936311\n",
      "train loss:1.639535393034727\n",
      "train loss:1.7621136440933869\n",
      "=== epoch:231, train acc:0.46, test acc:0.3559 ===\n",
      "train loss:1.7799639643091836\n",
      "train loss:1.7802017170697684\n",
      "train loss:1.7593057842668214\n",
      "=== epoch:232, train acc:0.4666666666666667, test acc:0.3584 ===\n",
      "train loss:1.574712912153641\n",
      "train loss:1.6998409956827003\n",
      "train loss:1.5989378771112237\n",
      "=== epoch:233, train acc:0.47333333333333333, test acc:0.3618 ===\n",
      "train loss:1.785811848615141\n",
      "train loss:1.534281178675655\n",
      "train loss:1.5198541200634095\n",
      "=== epoch:234, train acc:0.48333333333333334, test acc:0.3648 ===\n",
      "train loss:1.5826778990141586\n",
      "train loss:1.5849035499112525\n",
      "train loss:1.662817350558833\n",
      "=== epoch:235, train acc:0.4766666666666667, test acc:0.3577 ===\n",
      "train loss:1.7177112689108682\n",
      "train loss:1.5539165013980303\n",
      "train loss:1.5876071368935067\n",
      "=== epoch:236, train acc:0.4766666666666667, test acc:0.3558 ===\n",
      "train loss:1.7396575046323906\n",
      "train loss:1.565026242722249\n",
      "train loss:1.6866317624746034\n",
      "=== epoch:237, train acc:0.4666666666666667, test acc:0.3556 ===\n",
      "train loss:1.689891289756031\n",
      "train loss:1.599954018256681\n",
      "train loss:1.634418019922859\n",
      "=== epoch:238, train acc:0.47, test acc:0.3575 ===\n",
      "train loss:1.7076292366605177\n",
      "train loss:1.6758439224153856\n",
      "train loss:1.6618571259585841\n",
      "=== epoch:239, train acc:0.4666666666666667, test acc:0.3607 ===\n",
      "train loss:1.63863858191875\n",
      "train loss:1.779640743257408\n",
      "train loss:1.6473005473806908\n",
      "=== epoch:240, train acc:0.47, test acc:0.3621 ===\n",
      "train loss:1.6414777123726347\n",
      "train loss:1.5745546648368378\n",
      "train loss:1.6312962380357132\n",
      "=== epoch:241, train acc:0.4766666666666667, test acc:0.3677 ===\n",
      "train loss:1.566572388396348\n",
      "train loss:1.5934073557487431\n",
      "train loss:1.628062324786025\n",
      "=== epoch:242, train acc:0.48, test acc:0.3649 ===\n",
      "train loss:1.711320766568537\n",
      "train loss:1.5786321671021764\n",
      "train loss:1.7497772232645865\n",
      "=== epoch:243, train acc:0.4766666666666667, test acc:0.3676 ===\n",
      "train loss:1.7591329159202203\n",
      "train loss:1.6084195054453954\n",
      "train loss:1.5096650884245368\n",
      "=== epoch:244, train acc:0.48, test acc:0.3724 ===\n",
      "train loss:1.7121261197629665\n",
      "train loss:1.5985904788095016\n",
      "train loss:1.5411111356793752\n",
      "=== epoch:245, train acc:0.48333333333333334, test acc:0.3731 ===\n",
      "train loss:1.5982216585334155\n",
      "train loss:1.638834051902801\n",
      "train loss:1.5946055503395176\n",
      "=== epoch:246, train acc:0.4866666666666667, test acc:0.3757 ===\n",
      "train loss:1.656636585720448\n",
      "train loss:1.4612906229616553\n",
      "train loss:1.5941509626244075\n",
      "=== epoch:247, train acc:0.4866666666666667, test acc:0.3796 ===\n",
      "train loss:1.7164016810653757\n",
      "train loss:1.5346580638881253\n",
      "train loss:1.542538901818693\n",
      "=== epoch:248, train acc:0.49, test acc:0.3832 ===\n",
      "train loss:1.7308203984158939\n",
      "train loss:1.5922302426665516\n",
      "train loss:1.6108121535789053\n",
      "=== epoch:249, train acc:0.49, test acc:0.3843 ===\n",
      "train loss:1.5550386136578294\n",
      "train loss:1.5563264600387745\n",
      "train loss:1.614005513399188\n",
      "=== epoch:250, train acc:0.4866666666666667, test acc:0.3842 ===\n",
      "train loss:1.6500828782037262\n",
      "train loss:1.607933999439878\n",
      "train loss:1.6568878532643685\n",
      "=== epoch:251, train acc:0.49333333333333335, test acc:0.3875 ===\n",
      "train loss:1.4758006171269265\n",
      "train loss:1.6532829044147137\n",
      "train loss:1.5876132603249373\n",
      "=== epoch:252, train acc:0.49333333333333335, test acc:0.3909 ===\n",
      "train loss:1.5790953511321293\n",
      "train loss:1.5211766697296012\n",
      "train loss:1.6003585039092116\n",
      "=== epoch:253, train acc:0.4866666666666667, test acc:0.3866 ===\n",
      "train loss:1.5632813699473649\n",
      "train loss:1.6778924478513166\n",
      "train loss:1.7046327661625922\n",
      "=== epoch:254, train acc:0.5066666666666667, test acc:0.3985 ===\n",
      "train loss:1.6072893775061263\n",
      "train loss:1.5160192765962208\n",
      "train loss:1.6472567994911358\n",
      "=== epoch:255, train acc:0.5066666666666667, test acc:0.401 ===\n",
      "train loss:1.824939695036039\n",
      "train loss:1.486832916459613\n",
      "train loss:1.6273239580549872\n",
      "=== epoch:256, train acc:0.52, test acc:0.4062 ===\n",
      "train loss:1.4874601388255393\n",
      "train loss:1.467083064091311\n",
      "train loss:1.5727689542098664\n",
      "=== epoch:257, train acc:0.53, test acc:0.4135 ===\n",
      "train loss:1.6014578918452758\n",
      "train loss:1.6171917806471432\n",
      "train loss:1.6139329028629745\n",
      "=== epoch:258, train acc:0.5333333333333333, test acc:0.4142 ===\n",
      "train loss:1.6156921897466183\n",
      "train loss:1.5796825560037462\n",
      "train loss:1.6303723682756828\n",
      "=== epoch:259, train acc:0.5366666666666666, test acc:0.416 ===\n",
      "train loss:1.67597727636647\n",
      "train loss:1.6359530460000717\n",
      "train loss:1.5909065298419358\n",
      "=== epoch:260, train acc:0.5333333333333333, test acc:0.4128 ===\n",
      "train loss:1.476790153724701\n",
      "train loss:1.645917837760857\n",
      "train loss:1.5046795811581344\n",
      "=== epoch:261, train acc:0.5366666666666666, test acc:0.4136 ===\n",
      "train loss:1.592658864567856\n",
      "train loss:1.6036165890026342\n",
      "train loss:1.48338089704377\n",
      "=== epoch:262, train acc:0.5466666666666666, test acc:0.4151 ===\n",
      "train loss:1.4431031663487912\n",
      "train loss:1.5213506892554178\n",
      "train loss:1.5697001108058286\n",
      "=== epoch:263, train acc:0.5566666666666666, test acc:0.4177 ===\n",
      "train loss:1.5214170317352547\n",
      "train loss:1.5180087638544617\n",
      "train loss:1.5513941088676841\n",
      "=== epoch:264, train acc:0.55, test acc:0.4188 ===\n",
      "train loss:1.5683995482511086\n",
      "train loss:1.4676937019399676\n",
      "train loss:1.4580990824226263\n",
      "=== epoch:265, train acc:0.5433333333333333, test acc:0.4186 ===\n",
      "train loss:1.707480586407072\n",
      "train loss:1.7280677973945473\n",
      "train loss:1.5943649244397458\n",
      "=== epoch:266, train acc:0.5433333333333333, test acc:0.4192 ===\n",
      "train loss:1.6544583690371593\n",
      "train loss:1.583485112615594\n",
      "train loss:1.530855314452777\n",
      "=== epoch:267, train acc:0.5533333333333333, test acc:0.421 ===\n",
      "train loss:1.4163990687864885\n",
      "train loss:1.5623199244026293\n",
      "train loss:1.5682494418913913\n",
      "=== epoch:268, train acc:0.5666666666666667, test acc:0.4257 ===\n",
      "train loss:1.5866676552735208\n",
      "train loss:1.5222086761796292\n",
      "train loss:1.552707033154662\n",
      "=== epoch:269, train acc:0.5666666666666667, test acc:0.4246 ===\n",
      "train loss:1.4304363682995516\n",
      "train loss:1.3942265034198058\n",
      "train loss:1.509513509608932\n",
      "=== epoch:270, train acc:0.5733333333333334, test acc:0.4274 ===\n",
      "train loss:1.5285919174023705\n",
      "train loss:1.523626471058269\n",
      "train loss:1.5459679077521553\n",
      "=== epoch:271, train acc:0.58, test acc:0.4319 ===\n",
      "train loss:1.2754820882863398\n",
      "train loss:1.3306849494085853\n",
      "train loss:1.487829704384883\n",
      "=== epoch:272, train acc:0.5733333333333334, test acc:0.4298 ===\n",
      "train loss:1.693594217374419\n",
      "train loss:1.4789983761448753\n",
      "train loss:1.5293101160774887\n",
      "=== epoch:273, train acc:0.5766666666666667, test acc:0.4322 ===\n",
      "train loss:1.4397633088679538\n",
      "train loss:1.688308959908451\n",
      "train loss:1.591509956330321\n",
      "=== epoch:274, train acc:0.59, test acc:0.4366 ===\n",
      "train loss:1.5196009987955374\n",
      "train loss:1.6142814885675005\n",
      "train loss:1.5086929363310508\n",
      "=== epoch:275, train acc:0.59, test acc:0.4406 ===\n",
      "train loss:1.5089217384314362\n",
      "train loss:1.607481330837734\n",
      "train loss:1.4307427865183475\n",
      "=== epoch:276, train acc:0.5933333333333334, test acc:0.4429 ===\n",
      "train loss:1.449484243903928\n",
      "train loss:1.6394668791191593\n",
      "train loss:1.5061246469661724\n",
      "=== epoch:277, train acc:0.59, test acc:0.4438 ===\n",
      "train loss:1.5177055329009266\n",
      "train loss:1.4645210190011435\n",
      "train loss:1.5788320483727964\n",
      "=== epoch:278, train acc:0.5933333333333334, test acc:0.4446 ===\n",
      "train loss:1.3248500542673856\n",
      "train loss:1.425712461833036\n",
      "train loss:1.5769464795472186\n",
      "=== epoch:279, train acc:0.59, test acc:0.4465 ===\n",
      "train loss:1.5212173941834117\n",
      "train loss:1.4893327404747831\n",
      "train loss:1.4836456868756966\n",
      "=== epoch:280, train acc:0.5966666666666667, test acc:0.4459 ===\n",
      "train loss:1.5759914496885892\n",
      "train loss:1.3897710796790788\n",
      "train loss:1.4847861713034656\n",
      "=== epoch:281, train acc:0.59, test acc:0.4459 ===\n",
      "train loss:1.5329187589934907\n",
      "train loss:1.3924491700739612\n",
      "train loss:1.4864436046449185\n",
      "=== epoch:282, train acc:0.5933333333333334, test acc:0.4482 ===\n",
      "train loss:1.4984844900409855\n",
      "train loss:1.3860508188393887\n",
      "train loss:1.4957577775488926\n",
      "=== epoch:283, train acc:0.6, test acc:0.4473 ===\n",
      "train loss:1.5785863676120204\n",
      "train loss:1.450373119556028\n",
      "train loss:1.3911709595556598\n",
      "=== epoch:284, train acc:0.6, test acc:0.4484 ===\n",
      "train loss:1.4162460238454186\n",
      "train loss:1.5943493677742087\n",
      "train loss:1.2839307441393368\n",
      "=== epoch:285, train acc:0.5933333333333334, test acc:0.4489 ===\n",
      "train loss:1.5771857148568125\n",
      "train loss:1.404284254426774\n",
      "train loss:1.4446479155959886\n",
      "=== epoch:286, train acc:0.5966666666666667, test acc:0.4476 ===\n",
      "train loss:1.3910898457538057\n",
      "train loss:1.5132214887411823\n",
      "train loss:1.4751150145621412\n",
      "=== epoch:287, train acc:0.59, test acc:0.4489 ===\n",
      "train loss:1.4766364963477931\n",
      "train loss:1.4797548246781274\n",
      "train loss:1.420681103522677\n",
      "=== epoch:288, train acc:0.5933333333333334, test acc:0.4511 ===\n",
      "train loss:1.428367776657395\n",
      "train loss:1.359248896336314\n",
      "train loss:1.372411799737209\n",
      "=== epoch:289, train acc:0.5933333333333334, test acc:0.4493 ===\n",
      "train loss:1.5034113818168715\n",
      "train loss:1.3577210499430956\n",
      "train loss:1.2964329128208476\n",
      "=== epoch:290, train acc:0.5966666666666667, test acc:0.45 ===\n",
      "train loss:1.504386786709066\n",
      "train loss:1.5729882332621126\n",
      "train loss:1.2274979096454768\n",
      "=== epoch:291, train acc:0.5933333333333334, test acc:0.4521 ===\n",
      "train loss:1.308004413101983\n",
      "train loss:1.489396218796289\n",
      "train loss:1.3022177695228365\n",
      "=== epoch:292, train acc:0.6033333333333334, test acc:0.4559 ===\n",
      "train loss:1.5313962977249078\n",
      "train loss:1.3349677964717706\n",
      "train loss:1.3605829672214755\n",
      "=== epoch:293, train acc:0.6033333333333334, test acc:0.458 ===\n",
      "train loss:1.409897312064241\n",
      "train loss:1.324359046307616\n",
      "train loss:1.5679920469725541\n",
      "=== epoch:294, train acc:0.6, test acc:0.4576 ===\n",
      "train loss:1.3823909338766933\n",
      "train loss:1.4190104963643273\n",
      "train loss:1.4806251658954224\n",
      "=== epoch:295, train acc:0.5966666666666667, test acc:0.4599 ===\n",
      "train loss:1.4289183837655555\n",
      "train loss:1.3465830333848827\n",
      "train loss:1.4598031711166772\n",
      "=== epoch:296, train acc:0.6, test acc:0.4615 ===\n",
      "train loss:1.4444288546243653\n",
      "train loss:1.4765143250458672\n",
      "train loss:1.41917198141865\n",
      "=== epoch:297, train acc:0.5966666666666667, test acc:0.4616 ===\n",
      "train loss:1.379963562422937\n",
      "train loss:1.4314152309404529\n",
      "train loss:1.519393150293773\n",
      "=== epoch:298, train acc:0.6, test acc:0.463 ===\n",
      "train loss:1.3352178599115019\n",
      "train loss:1.37437421278872\n",
      "train loss:1.3801700621916182\n",
      "=== epoch:299, train acc:0.6066666666666667, test acc:0.4664 ===\n",
      "train loss:1.3744326244886877\n",
      "train loss:1.4929308777758425\n",
      "train loss:1.3930432943610944\n",
      "=== epoch:300, train acc:0.61, test acc:0.468 ===\n",
      "train loss:1.3487368002494748\n",
      "train loss:1.2965541557717892\n",
      "train loss:1.2238049784281597\n",
      "=== epoch:301, train acc:0.5933333333333334, test acc:0.4681 ===\n",
      "train loss:1.4355372170789529\n",
      "train loss:1.4458507216463057\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.4699\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec536a8-e54b-45ff-ba0d-08e9925fa0f1",
   "metadata": {},
   "source": [
    "- 그림과 같이 드롭아웃을 적용하니 훈련 데이터와 시험 데이터에 대한 정확도 차이가 줄었다.\n",
    "- 또, 훈련 데이터에 대한 정확도가 100%에 도달하지도 않게 되었다\n",
    "  - 즉, 표현력을 높이면서도 오버 피팅을 억제할 수 있다. \n",
    "  - 원래는 train의 정확도가 1에 육박하였으며 test와의 정확도 차이도 벌어져 있었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17ad8c-eddd-4300-995c-0834c0b45413",
   "metadata": {},
   "source": [
    "- 기계학습에서의 앙상블 학습은 드롭아웃과 밀접하다. \n",
    "  - 드롭아웃이 학습 때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석할 수 있기 때문이다. \n",
    "  - 그리고 추론 때는 뉴런의 출력에 삭제한 비율을 곱함으로써 앙상블 학습에서 여러 모델의 평균을 내는 것과 같은 효과를 얻는 것이다. \n",
    "  - 즉 드롭아웃은 앙상블 학습과 같은 효과를 하나의 네트워크로 구현했다고 생각할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0862c5-b75c-4c7a-8f6d-421aa97906a2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1faedc-7b87-4207-887b-b1e81d4fbb37",
   "metadata": {},
   "source": [
    "- 적절한 하이퍼파라미터 값 찾기\n",
    "  - 하이퍼파라미터? 각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 학습률과 가중치 감소 등이다. \n",
    "  - 하이퍼 파라미터의 값을 최대한 효율적으로 탐색하는 방법을 설명한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f193a0-0167-4b8d-9bcf-3ddd0083c79a",
   "metadata": {},
   "source": [
    "- 검증 데이터 \n",
    "  - 앞으로 하이퍼파라미터를 다양한 값으로 설정하고 검증할 텐데 여기서 주의할 점은 하이퍼파라미터의 성능을 평가할 때는 시험 데이터를 사용해서 안 된다.\n",
    "    - 시험 데이터를 사용하여 하이퍼파라미터를 조정하면 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문이다.\n",
    "    - 즉, 하이퍼파라미터 값의 좋음을 시험 데이터로 확인하게 되므로 하이퍼파라미터의 값이 시험 데이터에만 적합하도록 조정되어 버린다. \n",
    "    - 그렇게 되면 다른 데이터에는 적응하지 못하니 범용 성능이 떨어지는 모델이 될지도 모른다. \n",
    "    - 따라서 하이퍼파라미터를 조정할 때는 하이퍼파라미터 전용 확인 데이터가 필요하다. 이때 검증 데이터라고 부른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc83dbaf-9c24-4c23-ade8-090a8f34e495",
   "metadata": {},
   "source": [
    "- **훈련 데이터** : 매개변수 학습\n",
    "- **검증 데이터** : 하이퍼파라미터 성능 평가\n",
    "- **시험 데이터** : 신경망의 범용 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b379800-2c55-4398-8955-83d592418a64",
   "metadata": {},
   "source": [
    "- 데이터 셋이 훈련 데이터와 시험 데이터로만 분리되어 있다면 검증 데이터를 직접 분리해주어야 한다. \n",
    "- 훈련 데이터 중 20% 정도를 검증 데이터로 먼저 분리하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261f218-5697-44ff-bacd-d92652def57b",
   "metadata": {},
   "source": [
    "```python\n",
    "def shuffle_dataset(x, t):\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "\n",
    "(x_train,t_train),(x_test,t_test) = load_mnist()\n",
    "\n",
    "# 훈련 데이터를 뒤섞는다.\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = .2\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb16e6d-efed-488a-aafe-334a5bff27a0",
   "metadata": {},
   "source": [
    "- 해당 코드를 통해 훈련 데이터를 분리하기 전에 입력 데이터와 정답 레이블을 뒤섞는다. 데이터 셋 안의 데이터가 치우쳐 있을지도 모르기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7b2e4-aaa0-4a04-a46c-d2ab2bc8fd2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab46fe-5efb-46f2-88fa-32bab295faa6",
   "metadata": {},
   "source": [
    "- 하이퍼파라미터 최적화\n",
    "  - 하이퍼파라미터를 최적화할 때의 핵심은 하이퍼파라미터의 최적값이 존재하는 범위를 조금씩 줄여간다는 것이다. \n",
    "  - 범위를 조금씩 줄이려면, 우선 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼파라미터 값을 골라낸 후, 그 값으로 정확도를 평가한다. 정확도를 잘 살피면서 이 작업을 여러 번 반복하며 하이퍼파라미터의 최적 값의 범위를 좁혀가는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf9ec1a-c5bf-437d-ae27-a777cb0b07b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49bdd9d-e461-4bd4-965d-99274ff653ee",
   "metadata": {},
   "source": [
    "- 정리\n",
    "  - 이번 장에서는 신경망 학습에 중요한 기술 몇 가지를 소개했다. 매개변수 갱신 방법과 가중치의 초깃값을 설정하는 방법, 또 배치 정규화와 드롭아웃 등 현대적인 신경망에서 빼놓을 수 없는 기술들이다.\n",
    "    - 매개변수 갱신 방법 : 확률적 경사 하강법(SGD), 모멘텀, AdaGrad,Adam등이 있다. \n",
    "    - 가중치 초깃값을 정하는 방법은 올바른 학습을 하는 데 매우 중요하다. \n",
    "    - 가충치의 초깃값으로는 Xavier 초깃값과 He 초깃값이 효과적이다.\n",
    "    - 배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값의 영향을 덜 받을 수 있다.\n",
    "    - 오버피팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다. \n",
    "    - 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f5649-04e7-4662-9dc2-13edaedaa741",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cc050-40d7-493c-9f10-dd6307c4d516",
   "metadata": {},
   "source": [
    "- 합성곱 신경망(Convolutional Neural Network, CNN)\n",
    "  - 이미지 인식과 음성 인식 등 다양한 곳에서 사용되는데 특히, 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 다 CNN을 기초로 한다. \n",
    "  - 지금까지 본 신경망과 같이 레고 블록처럼 계층을 조합하여 만들 수 있다. \n",
    "  - 다만, `합성곱 계층`과 `풀링 계층`이 새롭게 등장한다.\n",
    "  - 지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었다. 이를 완전연결, 전결합이라고 하며, 완전히 연결된 계층을 Affine 계층이라는 이름으로 구현했다.\n",
    "  - 완전 연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층 혹은 Sigmoid 계층이 이어진다. 그렇다면 CNN의 구조는 어떻게 다를까\n",
    "    - CNN에서는 새로운 합성곱 계층과 풀링 계층이 추가된다.\n",
    "    - 합성곱 계층 - ReLU - 풀링계층 흐름으로 연결된다. 풀링계층은 생략가능하다.\n",
    "    - 주의* 출력에 가까운 층은 지금까지의 Affine-ReLU 구성을 사용할 수 있다는 것이다. 또 마지막 출력 계층에선 Affine - Softmax 조합을 그대로 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba02f85-2126-45c4-b5b6-88f7ab4d2a2a",
   "metadata": {},
   "source": [
    "- CNN에서는 패딩, 스트라이드 등 CNN 고유의 용어가 등장한다. 또, 각 계층 사이에는 3차원 데이터같이 입체적인 데이터가 흐른다는 점에서 완전연결 신경망과 다르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c08a6-3e37-4caa-8eda-b625274ba4a3",
   "metadata": {},
   "source": [
    "- 완전 연결 계층의 문제점\n",
    "  - 완전연결 계층에서는 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있었다.\n",
    "  - 그렇다면 문제점은 무엇인가? 데이터의 형상이 무시된다는 것이다. \n",
    "    - 만약 입력 데이터가 이미지인 경우를 예로 들면, 이미지는 통상 세로 , 가로, 채널(색상)로 구성된 3차원 데이터이다.\n",
    "    - 그러나 완전연결 계층에 입력할 때는 3차원 데이터를 평평한 1차원 데이터로 평탄화해줘야 한다. \n",
    "    - 완전연결 계층은 형상을 무시하고, 모든 입력 데이터를 동등한 뉴런 즉 같은 차원의 뉴런으로 취급하여 형상에 담긴 정보를 살릴 수 없다. \n",
    "    - 한편, 합성곱 계층은 형상을 유지한다. 이미지도 3차원 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차원 데이터로 전달한다. 그래서 CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 가능성이 있는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f84b52-9416-4064-bfc5-9c166dc59899",
   "metadata": {},
   "source": [
    "- CNN에서는 합성곱 계층의 입출력 데이터를 특징 맵(feature map)이라고도 한다. \n",
    "- 합성곱 계층의 입력 데이터를 입력 특징 맵, 출력 데이터를 출력 특징 맵이라고 하는 식이다. \n",
    "  - 이 책에서는 입출력 데이터와 특징 맵을 같은 의미로 사용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4133506-ec5e-46fe-8f20-f238b5b80d5a",
   "metadata": {},
   "source": [
    "- 합성곱 계층에서의 합성곱 연산을 처리하자. \n",
    "- 합성곱 연산은 이미지 처리에서 말하는 필터 연산에 해당한다. 필터를 커널이라 칭하기도 한다. \n",
    "- 합성곱 연산의 자세한 과정은 교재 231p를 참고한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c5325c-0e1c-45fa-aef9-88e9b4d3ea21",
   "metadata": {},
   "source": [
    "- 완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수가 그동안의 가중치에 해당한다. 그리고 CNN에도 편향이 존재한다. \n",
    "- 편향은 필터를 적용한 후의 데이터에 더해진다. 그리고 편향은 항상 하나만 스칼라로서 존재한다. 그 하나의 값을 필터를 적용한 모든 원소에 더하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126f263-9a3f-4655-aaf3-ee4989a80318",
   "metadata": {},
   "source": [
    "- 패딩\n",
    "  - 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(예컨대 0)으로 채우기도 한다. 이를 패딩이라한다. \n",
    "  - 그렇게되면 출력 데이터의 형상도 달라질 것이다. \n",
    "    - 즉 패딩은 주로 출력 크기를 조정할 목적으로 사용한다. 예를 들어(4,4) 입력데이터에 (3,3) 필터를 적용하면 출력은 (2,2)가 되어 입력보다 2만큼 줄어든게 된다. 이는 합성곱 연산을 몇 번이나 되풀이하는 심층 신경망에서는 문제가 될 수 있다. \n",
    "    - 합성곱 연산을 거칠 때마다 크기가 작아지면 어느 시점에서는 출력 크기가 1이 되어버린다. 더 이상은 합성곱 연산을 적용할 수 없다는 뜻이다. 이러한 사태를 막기 위해 패딩을 적용한다. \n",
    "    - 이를 이용하여 입력 데이터의 공간적 크기를 고정한 채로 다음 계층에 전달할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9737e77-bd37-4c1e-b86c-f3c77dd0d9f0",
   "metadata": {},
   "source": [
    "- 스트라이드\n",
    "  - 필터를 적용하는 위치의 간격을 스트라이드라고 한다. \n",
    "  - 스트라이드를 2로 하면 필터를 적용하는 윈도우가 두 칸씩 이동한다. \n",
    "    - 그렇다면, 스트라이드를 키우면 출력의 크기는 작아진다. 한편, 패딩을 크게하면 출력 크기가 커졌었다. \n",
    "    - 이러한 관계를 수식화한 234p를 참고해보자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
