{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f48672-efc4-49d5-b67b-1aa5024b962a",
   "metadata": {},
   "source": [
    "# 클래스로 신경망 구현, 오차역 전파법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5223a33e-9f23-4be4-81ba-5abe9836b810",
   "metadata": {},
   "source": [
    "- 학습 알고리즘 구현하기\n",
    "  - 신경망 학습의 순서를 확인해보자\n",
    "    - 전제 : 신경망에는 적응 가능한 가중치와 편향이 이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라고 한다. 신경망 학습은 다음과 같이 4단계로 수행된다.\n",
    "    - 1) 미니배치 : 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.\n",
    "    - 2) 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실함수의 값을 가장 작게 하는 방향을 제시한다. \n",
    "    - 3) 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
    "    - 4) 반복  : 1~3단계를 반복한다.\n",
    "  - 이것이 신경망 학습이 이루어지는 순서이다. 이는 경사 하강법으로 매개변수를 갱신하는 방법이며, 이때 데이터를 미니배치로 무작위로 선정하기 때문에 확률적 경사 하강법이라고 부른다. (이하 SGD) 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법이라 의미이다.\n",
    "- 실제로 손글씨 숫자를 학습하는 신경망을 구현해보자\n",
    "- 여기에서는 2층 신경망(은닉층이 1개인 네트워크)을 대상으로 MNIST 데이터셋을 사용하여 학습을 수행한다.\n",
    "- 처음에는 2층 신경망을 하나의 클래스로 구현하는 것부처 시작한다. \n",
    "- 이 클래스의 이름은 TwoLayerNet이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23cd39b-bae3-428c-9a1a-2cd3b410a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8f9fd-f66a-4264-bc25-fecf3d02316d",
   "metadata": {},
   "source": [
    "- 앞에서 다룬 신경망의 순전파 처리 구현과 공통되는 부분이 많고, 새로운 내용은 딱히 없다.\n",
    "- 우선 이 클래스가 사용하는 변수와 메서드를 정리해보자\n",
    "  - 중요해보이는 것 일부만 작성하였으며 그 외의 것은 139p를 참고하자\n",
    "    - params : 신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수)\n",
    "    - grads : 기울기 보관하는 딕셔너리 변수 (numerical_gradient() 메서드의 반환 값)\n",
    "  - TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖는다. \n",
    "  - 자세한 내용은 해당 교재를 참고하자.\n",
    "  - 예를 하나 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a28afd-07cc-498a-83f0-91a70308dd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size= 100, output_size= 10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ae14f-8094-486f-a7bd-2c9d5585f1af",
   "metadata": {},
   "source": [
    "- 이와 같이 params 변수에는 이 신경망에 필요한 매개변수가 모두 저장된다. 그리고 params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용된다. 참고로 예측 처리는 다음과 같이 실행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82f70c1a-4598-461f-a3cd-9b976c42605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100,784) # 더미 입력 데이터 100장 분량\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12e62b-8fa3-4d16-8987-8a0b0bde18e4",
   "metadata": {},
   "source": [
    "- grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다. 예를 들어 다음과 같이 numericla_gradient() 메서드를 사용해 기울기를 계산하면 grads 변수에 기울기 정보가 저장된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7538ee-87b7-4edd-9e99-ff90224b7f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100,784) # 더미 입력 데이터 (100장 분량)\n",
    "t = np.random.rand(100,10) # 더미 정답 레이블 (100장 분량)\n",
    "\n",
    "grads = net.numerical_gradient(x,t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb18ff-d39b-42b7-8201-974267e89541",
   "metadata": {},
   "source": [
    "- 이어서 TwoLayerNet 메서드를 살펴보자\n",
    "  - 우선 __init__ : 메서드는 클래스를 초기화한다. (이 초기화 메서드는 TwoLayerNet을 생성할 때 불리는 메서드이다.)\n",
    "  - 추가 : 신경망 학습은 시간이 오래 걸리니, 시간을 절약하려면 같은 결과를 훨씬 빠르게 얻을 수 있는 오차역전파법으로 각 매개변수의 손실 함수에 대한 기울기를 계산할 수 있다. 이는 다음장에서 학습할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a2f3d-a75c-4c79-b37f-620bae2e524a",
   "metadata": {},
   "source": [
    "- 미니배치 학습 구현하기\n",
    "  - 신경망 학습 구현에는 앞에서 설명한 미니배치 학습을 활용한다. 미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고, 그 미니배치에 대해서 경사법으로 매개변수를 갱신한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1db957a8-b9e3-483d-9993-e65c1ae60802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354a296-9bda-4612-874c-7b68869fe733",
   "metadata": {},
   "source": [
    "- 여기서는 미니배치 크기를 100으로 했다. 즉, 매번 60000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려낸다. 그리고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다. 경사법에 의한 갱신 횟수(반복 횟수)를 10000번으로 설정하고 갱신할 때마다 훈련 데이터에 대한 손실함수를 계산하고 그 값을 배열에 추가한다.\n",
    "- 학습 횟수가 늘어가면서 손실 함수의 값이 줄어들고 이는 학습이 잘 이루어지고 있다는 뜻으로 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미한다. 신경망이 학습하고 있는 것이다. 다시 말해 데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 서서히 다가가고 있는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b917d1-331d-4a1a-86ff-f943486a2228",
   "metadata": {},
   "source": [
    "- 하지만 정확히는 훈련 데이터의 미니배치에 대한 손실 함수의 값이다. 훈련 데이터의 손실 함수 값이 작아지는 것은 잘 학습하고 있다는 방증이지만 이 결과만으로는 다른 데이터셋에서도 비슷한 실력을 발휘할지는 확실하지 않다. \n",
    "- 신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해여 한다. 다른 말로 오버피팅을 일으키지 않는지 확인해야 한다. 오비피팅 되었다는 것은 예를 들어 훈련 데이터 포함된 이미지만 제대로 구분하고 그렇지 않은 이미지는 식별할 수 없다는 뜻이다.\n",
    "- 범용적인 능력의 평가를 위해, 훈련 데이터에 포함되지 않은 데이터를 사용해 평가해봐야 한다.\n",
    "  - 이를 위해 다음 구현에서는 학습 도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록한다. 여기에서는 1에폭별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록한다. \n",
    "    - 에폭은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. 예컨대 훈련 데이터 10000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 소진한게 된다. 이 경우 100회가 1에폭이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1685104-749c-4acf-870d-4f0e680d06c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09915, 0.1009\n",
      "train acc, test acc | 0.7825666666666666, 0.7853\n",
      "train acc, test acc | 0.8771333333333333, 0.8807\n",
      "train acc, test acc | 0.8981166666666667, 0.9014\n",
      "train acc, test acc | 0.9082833333333333, 0.9104\n",
      "train acc, test acc | 0.9147, 0.9171\n",
      "train acc, test acc | 0.9193, 0.9215\n",
      "train acc, test acc | 0.92455, 0.9265\n",
      "train acc, test acc | 0.9277166666666666, 0.9291\n",
      "train acc, test acc | 0.9313833333333333, 0.9322\n",
      "train acc, test acc | 0.93445, 0.9348\n",
      "train acc, test acc | 0.9368333333333333, 0.9367\n",
      "train acc, test acc | 0.939, 0.9385\n",
      "train acc, test acc | 0.9403666666666667, 0.9408\n",
      "train acc, test acc | 0.9431666666666667, 0.942\n",
      "train acc, test acc | 0.9448, 0.9435\n",
      "train acc, test acc | 0.94635, 0.9451\n"
     ]
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf0ec0-47c1-4941-80e6-cf20878298fe",
   "metadata": {},
   "source": [
    "- 이 예에서는 1에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고 그 결과를 기록한다. \n",
    "- 정확도를 1에폭마다 계산하는 이유는 for문 앞에서 매번 계산하기에는 시간이 오래 걸리고 또 그렇게까지 자주 기록할 필요도 없기 때문이다. \n",
    "- 앞의 코드로 얻은 결과를 그래프로 그려보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90d30231-769c-40b9-87d7-07664509a7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83011fb2-471a-4b30-a889-b4083d39b1f0",
   "metadata": {},
   "source": [
    "- 훈련 데이터에 대한 정확도를 실선으로 시험 데이터 에대한 정확도를 점선으로 그렸다. 보다시피 에폭이 진행될수록, 즉 학습이 진행될수록 훈련데이터와 시험데이터를 사용하고 평가한 정확도가 모두 좋아지고 있다. 또 두 정확도에는 차이가 없음을 알 수 있다. 다시 말해 이번 학습에서는 오버피팅이 일어나지 않았음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7c9771-c43b-4bdf-adbc-6d39efa6aab8",
   "metadata": {},
   "source": [
    "- 만약 오버피팅이 일어난다면?\n",
    "  - 훈련이란 훈련 데이터에 대한 정확도를 높이는 방향으로 학습이 이루어지니 그 정확도는 에폭을 반복할 수록 높아진다. 반면 훈련 데이터에 지나치게 적응하면, 즉 오버피팅되면 훈련 데이터와는 다른 데이터를 보면 잘못된 판단을 하기 시작한다. 어느 순간부터 시험 데이터에 대한 정확도가 점차 떨어지기 시작한다는 뜻이다. 이 순간이 오버피팅이 시작되는 순간이다. 여기서 중요한 insight! $\\to$ 이 순간을 포착해 학습을 중단하면 오버피팅을 효과적으로 예방할 수 있을 것이다. 이 기법을 조기 종료라 하며, 가중치 감소, 드롭 아웃과 함께 대표적인 오버피팅 예방법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb85ac7-db58-44cb-83f5-b4f20f6984f5",
   "metadata": {},
   "source": [
    "- 결론 \n",
    "  - 기계학습에서 사용하는 데이터 셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.\n",
    "  - 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.\n",
    "  - 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.\n",
    "  - 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.\n",
    "  - 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.\n",
    "  - 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.\n",
    "  - 수치 미분을 이요한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 다소 복잡한 오차역 전파법은 기울기를 고속으로 구할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2313cc-8925-4d5d-9f31-beaf321c7108",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728b144e-27e7-421b-9b48-861d6541cffd",
   "metadata": {},
   "source": [
    "- 오차역 전파법\n",
    "  - 가중치 매개변수의 기울기 정확히는 가중치 매개변수에 대한 손실 함수의 기울기를 효율적으로 계산하는 오차역 전파법을 배워보자\n",
    "  - 오차역 전파법 이해하기\n",
    "    - 오차역 전파법을 수식으로도 이해할 수 있겠지만, 이번 장에서는 계산 그래프를 사용해서 시작적으로 이해해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a41db1-8023-47fd-a0e8-37329a7fcab4",
   "metadata": {},
   "source": [
    "- 계산 그래프\n",
    "  - 계산 과정을 그래프로 나타낸 것이다.\n",
    "  - 복수의 노드와 에지로 표현된다. 노드 사이의 직선을 에지라고 한다. \n",
    "  - 간단한 문제부터 해결해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba2694-1e6c-4bd0-a599-e2bd550b0aab",
   "metadata": {},
   "source": [
    "- 문제 1) 현빈 군은 슈퍼에서 1개에 100원인 사과를 2개 샀다. 이때 지불 금액을 구하세요. 단, 소비세가 10% 부과된다.\n",
    "  - 계산 그래프는 계산 과정을 노드와 화살표(에지)로 표현한다. \n",
    "  - 원안에 연산 내용을 적고 계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 한다.\n",
    "  - 여기서는 원 대신 괄호로 대체한다.\n",
    "  - (사과) $\\to$ 100 $\\to$ (x2) $\\to$ 200 $\\to$ (x1.1) $\\to$ 220\n",
    "  - 처음에 사과의 100원이 x2 노드로 흐르고 200원이 되어 다음 노드(x1.1)로 전달된다. 이제 200원이 x1.1 노드를 거쳐 220원이 된다. \n",
    "  - 따라서 이 계산 그래프에 따르면 최종 답은 220원이 된다.\n",
    "  - 여기에서는 x2와 x1.1을 각각 하나의 연산으로 취급해 원 안에 표기했지만, 곱셈이 x만을 연산으로 생각할 수도 있다. 이렇게하면 다음과 같이 2와 1.1은 각각 사과의 개수와 소비세 변수가 되어 원밖에 표기하게 된다.\n",
    "  - 사과 $\\to$ 100 $\\to$ (x) $\\to$ 200 $\\to$ (x) $\\to$ 220 $\\to$\n",
    "  - 이제 첫 번째 노드에 2가 대입되고 다음 노드에 1.1이 대입된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc93212-ab8e-4e44-898b-bdb4fe10b42e",
   "metadata": {},
   "source": [
    "- 지금까지 살펴본 계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행된다.\n",
    "  - 1) 계산 그래프를 구성한다.\n",
    "  - 2) 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다. \n",
    "  - ***여기서 2번째 '계산을 왼쪽에서 오른쪽으로 진행'하는 단계를 순전파라고 한다.***\n",
    "  - 순전파의 반대 '역전파'도 존재할 것이다. 역전파는 이후에 미분을 계산할 때 중요한 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c7aa8-f0ec-4c02-ae30-bae72d9d56e6",
   "metadata": {},
   "source": [
    "- 계산 그래프의 특징 : 국소적(자신과 직접 관계된 작은 범위)계산을 전파함으로써 최종 결과를 얻는다.\n",
    "  - 국소적 계산은 결국 전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 점이다.\n",
    "  - 국소적 계산?\n",
    "    - 가령 슈퍼마켓에서 사과 2개를 포함한 여러 식품을 구입하는 경우를 생각해보자. 여러 식품을 구입하여 총 금액이 3000원이 되었다 여기에서 ㅎ개심은 각 노드에서의 계산은 국소적 계산이라는 점이다. 가령 사과와 그 외의 물품 값을 더하는 계산 3000+200에서 3000이라는 숫자가 어떻게 계산되었느냐와는 상관없이 단지 두 숫자(3000과 200)를 더하면 된다는 뜻이다. 각 노드는 자신과 관련한 계산외에는 아무것도 신경 쓸 게 없다는 것이다. \n",
    "    - 이처럼 계산 그래프는 국소적 계산에 집중한다. 전체 계산이 제아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다. 국소적 계산은 단순하지만, 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 해낼 수 있다.\n",
    "    - 또한 계산 그래프는 중간 계산 결과를 모두 보관할 수 있다.\n",
    "    - 또한 역절파를 통해 미분을 효율적으로 계산할 수 있다.\n",
    "      - 위에서 설명한 문제1)을 살펴보자\n",
    "      - 가령 사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지를 알고 싶다고 해보자.\n",
    "      - 이는 사과 과격에 대한 지불 금액의 미분을 구하는 문제에 해당된다. 기호로 나타낸다면 x는 사과 값을, L을 지불 금액이라고 했을때 $\\frac{\\partial L}{\\partial x}$을 구하는 것이다. \n",
    "      - 위 미분 값은 사과 값이 아주 조금 올랐을 때 지불 금액이 얼마나 증가하느냐를 표시한 것이다. \n",
    "      - 즉, 사과 과격에 대한 지불 금액의 미분 같은 값은 계산 그래프에서 역전파를 하면 구할 수 있다.\n",
    "      - 역전파가 어떻게 이루어지느냐는 뒤에서 추가 설명하겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957c5ed-b133-4d57-be8e-518ffb35cbc2",
   "metadata": {},
   "source": [
    "- 이처럼 계산 그래프의 이점은 순전파와 역전파를 활용해서 각 변수의 미분을 효율적으로 구할 수 있다는 것이다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
