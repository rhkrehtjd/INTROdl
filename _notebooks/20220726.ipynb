{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a090c4-92bb-406e-a417-66e21570cf2e",
   "metadata": {},
   "source": [
    "# 20220624"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7517a-4f31-415d-bf60-7ede440dbd76",
   "metadata": {},
   "source": [
    "#### CHAPTER 2 : 퍼셉트론\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4af867-b161-4c77-9b8b-7186d3bddb3e",
   "metadata": {},
   "source": [
    "- 퍼셉트론이란?\n",
    "  - 다수의 신호를 입력으로 받아 하나의 신호를 출력\n",
    "  - 퍼셉트론 신호는 흐른다/안 흐른다의 두 가지 값을 가질 수 있다.\n",
    "  - 입력 신호가 뉴런에 보내질 때는 각각 `고유한` 가중치가 곱해진다.\n",
    "  - 뉴런에서 보내온 신호의 `총합`이 정해진 한계를 넘어설 때만 1을 출력한다.\n",
    "    - 이를 `'뉴런이 활성화한다'`라고 표현한다. 이때 그 한계를 임계값이라 하며 대개 $\\theta$로 표현한다.\n",
    "  - 퍼셉트론은 `복수의 입력 신호 각각`에 `고유한 가중치를 부여`한다.\n",
    "  - `가중치`는 각 신호가 `결과에 주는 영향력을 조절`하는 요소로 적용\n",
    "    - 즉 가중치가 클수록 해당 신호가 그만큼 더 중요함을 뜻함.\n",
    "    - 퍼셉트론의 가중치는 그 값이 클수록 강한 신호를 흘려보낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0190df6c-ca30-4efa-96e2-521d7e31e314",
   "metadata": {},
   "source": [
    "- 단순한 논리 회로\n",
    "  - AND 게이트\n",
    "    - 입력이 둘이고 출력은 하나이다.(참고 : 입력 신호와 출력 신호의 대응 표를 `진리표`라 한다)\n",
    "    - 두 입력이 모두 1일 때만 1을 출력하고, 그 외에는 0을 출력한다.\n",
    "  - NAND 게이트와 OR 게이트\n",
    "    - NAND 게이트 : AND 게이트의 출력을 뒤집은 것\n",
    "    - 두 입력이 모두 1일 때만 0을 출력하고 그 외에는 1을 출력한다.\n",
    "    - 이때 매개변수의 조합은 AND 게이트를 구현하는 매개변수의 부호를 모두 반전하기만 하면 NAND 게이트가 된다.\n",
    "    - OR 게이트 : 입력 신호 중 하나 이상이 1이면 출력이 1이 되는 논리 회로이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0733fdf9-9709-49ea-a799-7119ceac1ee5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc8507-de00-4ea5-9d4d-c0e35283542e",
   "metadata": {},
   "source": [
    "#### 퍼셉트론의 구조는 AND, NAND, OR 게이트 모두에서 똑같다. \n",
    "#### 세 가지 게이트에서 다른 것은 매개변수(가중치와 임계값)의 값 뿐이다.\n",
    "#### 즉, 똑같은 구조의 퍼셉트론이 매개변수의 값만 적절히 조정하여 AND, NAND, OR로 사용할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c345015-f423-4a9e-a6fe-07069bddf0cc",
   "metadata": {},
   "source": [
    "> 퍼셉트론의 매개변수 값을 정하는 것은 컴퓨터가 아니라 인간이다. 인간이 직접 진리표라는 학습 데이터를 보면서 매개변수의 값을 생각한다. 기계학습 문제는 이 매개변수의 값을 정하는 작업을 컴퓨터가 자동으로 하도록 한다. 학습이란 적절한 매개변수 값을 정하는 작업이며, 사람은 퍼셉트론의 구조를 고민하고 컴퓨터에 학습할 데이터를 주는 일을 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e336ef9-4871-4bf9-ade1-e8b250f729af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6327fc7-1f30-4104-acfc-bfad65326740",
   "metadata": {},
   "source": [
    "- 퍼셉트론 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24ddd211-ce26-4085-9c4d-32df0b735ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1, x2):\n",
    "    # 매개변수(가중치와 임계값)는 함수 안에서 초기화한다.\n",
    "    w1, w2, theta = 0.5, 0.5, 0.7\n",
    "    tmp = x1*w1 + x2*w2\n",
    "    \n",
    "    # 이때, 부등식에 등호가 어디에 들어가 있는지 중요!!!\n",
    "    \n",
    "    if tmp <= theta:\n",
    "        return 0\n",
    "    elif tmp> theta:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd898ae5-2d32-42f1-afe5-c07a506c7eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(AND(0,0))\n",
    "print(AND(1,0))\n",
    "print(AND(0,1))\n",
    "print(AND(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd208e-6805-41c3-a495-0c4ebdb69020",
   "metadata": {},
   "source": [
    "- 위 AND 게이트를 좀 더 효율적인 방식으로 수정해보자\n",
    "- 함수 AND에서 tmp <= theta에서 theta를 좌항으로 옮겨서 정리한다.\n",
    "- 즉, b + x1*w1 + x2*w2 <= 0 이 되고\n",
    "- 이때, b를 편향이라고 바꿔부른다. \n",
    "  - 원래 b는 임계값! (=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bc0d3-3f7a-4d3d-97f6-2da8402200ee",
   "metadata": {},
   "source": [
    "> 즉, 퍼셉트론은 입력 신호에 가중치를 곱한 값과 편향을 합하여 그 값이 0을 초과하면 1을 출력하고, 그렇지 않으면 0을 출력한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b901f-3a24-4f98-a80c-00254cc56187",
   "metadata": {},
   "source": [
    "-  넘파이를 이용하여 가중치와 편향 개념 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f27cd07-fa77-476f-8218-5d9703d61e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([0,1]) # 두 입력 값\n",
    "w = np.array([0.5,0.5]) # 가중치\n",
    "b = -0.7 # 편향\n",
    "# 원래 우항에 있던 임계값을 좌항으로 옮겼기 때문에 마이너스! 그런데 이건 게이트마다 임계값이 다르기 때문에 좌항으로 옮겼을 때\n",
    "# 음수일 수도 있고 양수일 수도 있음\n",
    "np.sum(w*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4310999e-44ad-49d1-a6bc-b21995ed46e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19999999999999996"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(w*x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17a5df-fa63-49a3-bf09-0c9351ca4c3e",
   "metadata": {},
   "source": [
    "- 가중치와 편향을 도입한 AND 게이트를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d878810-6fb5-48fd-bd6e-1f5513789601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND(x1,x2):\n",
    "    x = np.array([x1,x2])\n",
    "    w = np.array([0.5,0.5])\n",
    "    b = -0.7\n",
    "    \n",
    "    tmp = np.sum(w*x) + b\n",
    "    \n",
    "    if tmp <= 0 :\n",
    "        return 0 \n",
    "    else :\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18e2f1-e8c0-4147-8416-81f7831964c4",
   "metadata": {},
   "source": [
    "> 가중치는 각 입력 신호가 결과에 주는 영향력(중요도)을 조절하는 매개변수고, 편향은 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력) 하느냐를 조정하는 매개변수이다. (책에 따라 셋 모두(가중치와 편향)를 가중치라고 할 때도 있다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525541cd-d777-4424-856b-5c3d242f578a",
   "metadata": {},
   "source": [
    "- NAND 게이트와 OR 게이트 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d60dd6-7916-4fb7-8365-757fbef93555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NAND(x1,x2):\n",
    "    x = np.array([x1,x2])\n",
    "    w = np.array([-0.5,-0.5])\n",
    "    b = 0.7\n",
    "    \n",
    "    tmp = np.sum(w*x) + b\n",
    "    \n",
    "    if tmp <= 0 :\n",
    "        return 0\n",
    "    else : \n",
    "        return 1\n",
    "\n",
    "def OR(x1,x2):\n",
    "    x = np.array([x1,x2])\n",
    "    w = np.array([0.5,0.5])\n",
    "    b = -0.2\n",
    "    \n",
    "    tmp = np.sum(w*x) + b\n",
    "    \n",
    "    if tmp <= 0 :\n",
    "        return 0\n",
    "    else : \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536a615-866d-4d7d-af61-80a11e1d8dd5",
   "metadata": {},
   "source": [
    "- AND, NAND, OR 게이트 `모두 같은 구조`의 퍼셉트론이며, 차이는 가중치 매개변수의 값 뿐이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaca849-5db6-48b2-a309-e2e6a98f7265",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6382c5-3774-404b-a979-b0089389c654",
   "metadata": {},
   "source": [
    "- 퍼셉트론의 한계\n",
    "  - 무슨 한계?\n",
    "    - 직선 하나로 나눈 영역만 표현할 수 있다는 한계\n",
    "- XOR 게이트 (배타적 논리합)\n",
    "  - 한쪽이 1일 때만 1을 출력\n",
    "  - 지금까지 본 퍼셉트론으로는 XOR 게이트를 구현할 수 없다.\n",
    "    - 왜?\n",
    "      - XOR 게이트는 한 쪽이 1일 때만 1을 출력한다고 했다.\n",
    "      - 이 입력값들을 좌표평면에 나타내면 입력 값들을 하나의 직선으로 분리하는 것은 불가능하다. \n",
    "      - 따라서 기존의 AND, NAND, OR 게이트의 퍼셉트론으로는 XOR 게이트를 구현할 수 없다. \n",
    "      - 책 55p ~ 56p 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba17dc3-db78-4ea2-9be1-4fbd5f97be73",
   "metadata": {},
   "source": [
    "- 이때, 직선이라는 제약을 없앤다면 주어진 입력값들을 두 구간으로 분류할 수 있을 것이다. \n",
    "- 퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가 있다. \n",
    "  - 그렇다면 방법은?\n",
    "  - 층을 쌓아, 다층 퍼셉트론을 만든다.\n",
    "  - 층을 하나 더 쌓아 XOR를 표현해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec9448-346b-40c4-93a0-121a2075eb10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66968f30-7860-4569-9ad5-bef28ec49712",
   "metadata": {},
   "source": [
    "> 0627~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d5dc1-f11b-4b31-9218-9f8858dc6b78",
   "metadata": {},
   "source": [
    "- 기존 게이트 조합하기\n",
    "  - AND, NAND, OR 게이트를 조합하기\n",
    "    - 퍼셉트론의 한계 : 단층 퍼셉트론으로는 XOR 게이트를 표현할 수 없음\n",
    "    - 즉, 단층 퍼셉트론으로는 비선형 영역을 분리할 수 없음\n",
    "    - 따라서 퍼셉트론을 조합하여, 즉 층을 쌓아서 XOR 게이트를 구현하면 됨\n",
    "  - 이때 XOR 게이트는 입력 중 하나만 1일 때, 출력 1이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa45cd2d-1317-427e-92a9-c7e6ec904e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지금까지 정의한 함수 AND, NAND, OR를 사용하면 XOR 게이트를 쉽게 구현할 수 있음\n",
    "def XOR(x1,x2) : \n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    y = AND(s1,s2)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84ea2f0-0f51-42ba-a641-eb46de97bdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(XOR(0,0))\n",
    "print(XOR(0,1)) # 이때 1을 출력해야 함\n",
    "print(XOR(1,0)) # # 이때 1을 출력해야 함\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc5c3f-3138-4046-a1ad-7109fd10e7be",
   "metadata": {},
   "source": [
    "- 단층 퍼셉트론으로는 표현하지 못한 것을 층을 하나 늘려 구현할 수 있게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49ee2a-09d3-4193-b907-2fbfe41dec1e",
   "metadata": {},
   "source": [
    "> 정리\n",
    "\n",
    "    1. 퍼셉트론에서는 가중치와 편향을 매개변수로 설정\n",
    "    2. XOR 게이트는 단층 퍼셉트론으로는 표현할 수 없다\n",
    "    3. 2층 퍼셉트론을 이용하면 XOR 게이트를 표현할 수 있다\n",
    "    4. 단층 퍼셉트론은 직선형 영역만 표현할 수 있고, 다층 퍼셉트론은 비선형 영역도 표현할 수 있다\n",
    "    5. 다층 퍼셉트론은 (이론상) 컴퓨터를 표현할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ba476-9913-4875-be23-53cd8731135d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452da363-e57f-4200-8530-3e2d78867ed8",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24cde1-66f4-49f4-b59c-72aafac46cd5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e35256-1981-47e2-8e60-eebb1597c61b",
   "metadata": {},
   "source": [
    "### 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b67cb8-9ae9-427e-b023-8bafcfeae22a",
   "metadata": {},
   "source": [
    "#### - 퍼셉트론에서 신경망으로\n",
    "- 신경망은 퍼셉트론과 유사한 점이 많으나, 퍼셉트론과 다른 점을 중심으로 신경망의 구조를 설명해보자\n",
    "- 신경망은 입력층, 은닉층, 출력층으로 구성되어 있음\n",
    "  - 이때 은닉층의 뉴런은 입력층이나 출력층과 달리 사람 눈에 보이지 않음\n",
    "    - (아마 이게 블랙박스라고 일컬었던 것 같음)\n",
    "  - rewiew\n",
    "    - 편향 : 뉴런이 얼마나 쉽게 활성화되느냐를 제어\n",
    "    - 가중치 : 각 신호의 영향력을 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfad790-4389-4e14-b955-98bf00fafa36",
   "metadata": {},
   "source": [
    "- 활성화 함수의 등장\n",
    "  - 입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 활성화 함수라 한다.\n",
    "  - 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e384822-d03b-4bf2-8915-0726270a9771",
   "metadata": {},
   "source": [
    "- 종전에 설명한 활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이런 함수를 계단 함수라 한다.\n",
    "  - 그래서 '퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다'고 할 수 있다.\n",
    "  - 즉, 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 계단 함수를 채용한 것\n",
    "    - 그 외, 시그모이드 함수도 활성화 함수로 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09745ebf-c106-44c9-ba16-c976c592780e",
   "metadata": {},
   "source": [
    "- 활성화 함수 中 시그모이드 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9348f188-451b-4e3d-844b-4254dd7ab57c",
   "metadata": {},
   "source": [
    "$$h(x) = {\\frac{1}{1+exp(-x)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b10a7-87c9-462e-903e-9cc5d0435961",
   "metadata": {},
   "source": [
    "- 활성화 함수로 이용되는 시그모이드 함수를 계단 함수와 비교하면서 자세히 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "649ba133-2f61-463e-a9be-46a8aa361f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    if x>0:\n",
    "        return 1\n",
    "    else :\n",
    "        return 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0ab06-16ff-4462-b7b3-f9ad02083fe3",
   "metadata": {},
   "source": [
    "- 해당 함수는 실수만 받아들임\n",
    "  - 즉, 넘파이 배열을 인수로 넣을 순 없음\n",
    "    - 이를테면, np.array([1.3,23.3])\n",
    "    - 가능하게 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07f8691-bc10-4319-b13d-2f474a5542d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(x):\n",
    "    y = x>0\n",
    "    return y.astype(np.int) # 원하는 자료형(e.g. np.int)을 인수로 지정해주면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2c9e0ac-b417-4c1e-a600-24da055bc9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/_b9p7msx2x5d5j728xn99ps80000gn/T/ipykernel_4375/1740302745.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return y.astype(np.int) # 원하는 자료형(e.g. np.int)을 인수로 지정해주면 된다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_function(np.array([23.123,-213,123,333.2,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c017d-7428-4e55-97b4-59de33cfc333",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8515d-48b3-4167-ae40-0d462f317ef6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9154dd9-cddb-4cb6-ac39-d7315734b703",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88620e4-0135-40a2-af3b-d4c50e010210",
   "metadata": {},
   "source": [
    "> 0628"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6255f-827c-4067-8afa-ab80a515d91d",
   "metadata": {},
   "source": [
    "- 계단 함수의 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274483a7-b1e0-4bd1-b98b-a6478218ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/_b9p7msx2x5d5j728xn99ps80000gn/T/ipykernel_4375/2993925797.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array(x>0, dtype = np.int) # 배열 생성하면서 동시에 자료형까지 지정\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x>0, dtype = np.int) # 배열 생성하면서 동시에 자료형까지 지정\n",
    "\n",
    "x = np.arange(-5,5,0.1) # -5부터 5까지 0.1의 간격으로 수열 생성\n",
    "y = step_function(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf01c8f-9e74-4d6b-aec1-b4d9d523e815",
   "metadata": {},
   "source": [
    "- 시그모이드 함수 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7019d87f-f0cb-4038-b1db-c9d738ce300d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce06db6-88b6-40e0-9d7c-573409577e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26894142, 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-1,1,2])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40fa3273-e772-473f-971b-703e4345da32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-5,5,0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a610f14-de91-42ce-aeeb-fabed90a3614",
   "metadata": {},
   "source": [
    "**S자모양**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c469a2-f049-45db-9a7b-d379a5f59cb8",
   "metadata": {},
   "source": [
    "- 시그모이드 함수와 계단 함수 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa12427-b39e-4fdf-97d4-b383187d5c13",
   "metadata": {},
   "source": [
    "시그모이드는 부드러운 곡선이며 입력에 딸 출력이 연속적으로 변화한다.\n",
    "\n",
    "한편 계단 함수는 0을 경계로 출력이 갑자기 바뀌어 버린다. \n",
    "\n",
    "이때 시그모이드 함수의 이 매끈함이 신경망 학습에서 아주 중요한 역할을 하게 된다. \n",
    "\n",
    "계단 함수가 0과 1 중 하나의 값만 돌려주는 반면 시그모이드 함수는 실수를 돌려준다. \n",
    "\n",
    "다시말해, 퍼셉트론에서는 뉴런 사이에 0혹은 1이 흘렀다면 신경망에서는 연속적인 실수가 흐른다.(신경망과 퍼셉트론은 유사하나 상이한 부분도 분명 있음. 다른 개념이다.)\n",
    "\n",
    "비유하자면, 시그모이드 함수는 물레방아처럼 흘러온 물의 양에 비례해 흐르는 물의 양을 조절한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd53bd-707a-4484-9746-4f15047facce",
   "metadata": {},
   "source": [
    "- 시그모이드 함수와 계단 함수의 공통점?\n",
    "  - 큰 관점에서 보면 둘은 같은 모양을 하고 있다. \n",
    "  - 둘 다 입력이 작을 때의 출력은 0에 가깝거나 0이고, 입력이 커지면 출력이 1에 가까워지는 혹은 1이 되는 구조이다. \n",
    "  - 즉, 계단 함수와 시그모이드 함수는 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다.\n",
    "    - 중요도?\n",
    "  - 그리고 입력이 아무리 작거나 커도 출력은 0에서 1 사이라는 것도 둘의 공통점이다. \n",
    "  - 또한 두 함수 모두 ### 비선형 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11a6588-0ce0-4213-ad91-6176a56ef69f",
   "metadata": {},
   "source": [
    "- 비선형 함수?\n",
    "  - 직선 1개로는 그릴 수 없는 함수\n",
    "  - 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다\n",
    "    - 즉, 선형 함수는 사용해선 안 된다.\n",
    "    - 왜? 선형 함수를 사용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문이다. \n",
    "    - 선형 함수의 문제? \n",
    "      - 층을 아무리 깊게 해도 은닉층이 없는 네트워크(즉, 입력층과 출력층만 있는??)로도 똑같은 기능을 할 수 있다는 데 있다.\n",
    "      - 이 부분 먼저 설명해보자\n",
    "      - 선형 함수인 h(x) = cx를 활성화 함수로 사용한 3층 네트워크를 떠올려보라\n",
    "      - 이를 식으로 나타내면 y(x) = h(h(h(x)))가 되면 계산은 c*c*c*x지만, 결국은 특정 상수 곱하기 엑스가 되는 것\n",
    "      - 즉, y(x) = h(h(h(x))) = ax로도 표현가능하다는 것.\n",
    "        - 은닉층이 없는 네트워크로도 표현이 가능해진 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99590ee-de7a-422b-91f4-0fc22c0135a2",
   "metadata": {},
   "source": [
    "> 즉! 선형 함수를 이용해서는 여러 층으로 구성하는 이점을 살릴 수 없다. \n",
    "\n",
    "> 왜? 선형 함수로 여러 층을 구성해봤자 그냥 은닉층 없는 네트워크로도 동일 기능을 수행할 수 있기 때문에"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3ef72-cb3b-4bf1-92d2-a6a834c5a81d",
   "metadata": {},
   "source": [
    "#### 따라서 층을 쌓는 혜택을 얻기 위해선 활성화 함수를 비선형 함수로 채택해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49db5a1-6c2e-483f-8e04-3a5c1a3124b3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45496a5-59d0-4cc6-b91b-02cd0e53aace",
   "metadata": {},
   "source": [
    "- ReLU 함수\n",
    "  - 입력이 0을 넘으면 그 입력을 그대로 출력, 그렇지 않으면 무조건 0을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e63c8086-1781-438b-a1f1-80aff480ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c2be5-f738-479d-9a1e-49ecda399c0c",
   "metadata": {},
   "source": [
    "- 넘파이 행렬을 써서 신경망을 구현해보자 \n",
    "- 해당 예에서는 편향과 활성화 함수를 생략하고 가중치만 갖는 신경망을 구현한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9695fc33-4640-4348-a272-b03860405faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 11 17]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1,2]) # 입력값\n",
    "W = np.array([[1,3,5],[2,4,6]]) # 가중치.\n",
    "# 해당 예에서는 편향과 활성화 함수를 생략하고 가중치만 갖는 신경망을 구현한다고 했음\n",
    "# 이때, 가중치는 2x3 크기의 matrix이다\n",
    "Y = np.dot(X,W)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860657ed-9522-4390-845b-8bab42bccdab",
   "metadata": {},
   "source": [
    "> 주의 : Y = np.dot(X,W)에서 입력값과 각각 대응되는 가중치들이 적절히 곱해지도록 행렬곱이 이루어짐\n",
    "\n",
    "> 이를테면 X가 1x2 크기의 행렬이고 W가 2x3 크기의 행렬인데 W에 원소들을 각각 어떻게 배치하느냐에 따라 입력값에 곱해져야 하는 가중치들이 적절히 곱해질 수도 안 곱해질 수도 있음 \n",
    "\n",
    "> 따라서 행렬곱을 수행해줄 때 입력값 각각에 할당된 가중치들이 입력값과 적절히 곱해질 수 있도록 가중치 행렬을 구성해주어야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398e731f-deae-44d0-bbdc-1ac136fc5bc7",
   "metadata": {},
   "source": [
    "위 예에서 볼 수 있듯 행렬의 곱으로 한꺼번에 계산해주는 기능은 신경망을 구현할 때 매우 중요함을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e12e8ea-0df8-4bc4-a17a-c6973295b1bb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9752247-37b6-401c-8290-ff4e756c8a20",
   "metadata": {},
   "source": [
    "- 3층 신경망에서 수행되는 입력부터 출력까지의 처리(순방향 처리)를 구현해보자\n",
    "- 넘파이 배열을 잘 구사하면 아주 적은 코드만으로 신경망의 순방향 처리를 완성할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948bd2f-50b0-4b9f-ba75-6e80e4512d35",
   "metadata": {},
   "source": [
    "- 85p 그림 3-17을 확인해보면, 편향을 0층 뉴런에 따로 할당하여 처리해줄 수 있다.\n",
    "- 해당 예에서는 편향을 1로 설정하여 두 입력값과 가중치가 곱해진 결과의 합과 편향을 더하여 다음 뉴런에 전달해주었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b9d73a-5c8e-4120-a6f1-e8e22cb81c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[0.3 0.7 1.1]\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([1,0.5])\n",
    "W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "B1 = np.array([0.1,0.2,0.3])\n",
    "\n",
    "print(X.shape) # 1x2 \n",
    "print(W1.shape) # 2x3\n",
    "print(B1.shape) # 1x3\n",
    "\n",
    "A1 = np.dot(X,W1) + B1\n",
    "print(A1) # 1x3\n",
    "print(A1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196243f8-b78d-47b6-b27c-cc3226bcc85f",
   "metadata": {},
   "source": [
    "- 0층이 아닌 1층에서의 활성화 함수를 시그모이드로 설정해보고 코드를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c10654c-0d0e-4c16-97e2-1e816c1fb171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "Z1 = sigmoid(A1)\n",
    "print(A1)\n",
    "print(Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd518b0-7412-4ac6-9d1f-c52c4154d6ca",
   "metadata": {},
   "source": [
    "- 1층에서 2층으로 가는 과정을 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ea8cd30-5792-43fe-ae8d-35a520810099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "W2 = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "B2 = np.array([0.1,0.2])\n",
    "\n",
    "print(Z1.shape)\n",
    "print(W2.shape)\n",
    "print(B2.shape)\n",
    "\n",
    "A2 = np.dot(Z1,W2) + B2\n",
    "Z2 = sigmoid(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f7d34-944e-4d45-9685-2c8317671f99",
   "metadata": {},
   "source": [
    "- 해당 과정은 1층의 출력 Z1이 2층의 입력이 된다는 점을 제외하면 0층에서 1층으로의 구현과 대동소이하다.\n",
    "- 이처럼 넘파이 배열을 사용하면 층 사이의 신호 전달을 쉽게 구현할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd995fa6-5eb7-463e-9479-c985f9d503b0",
   "metadata": {},
   "source": [
    "- 이제 2층에서 3층으로 즉, 2층에서 출력층으로의 신호 전달을 살펴보자\n",
    "- 출력층의 구현도 그 동안의 구현과 일맥상통하나 활성화 함수만 지금까지의 은닉층과 다르다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5d79d4c-b9ad-4481-9983-e81a9b815979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "# 여기서는 활성화함수로서 항등함수를 설정한 것\n",
    "\n",
    "\n",
    "W3 = np.array([[0.1,0.3],[0.2,0.4]])\n",
    "B3 = np.array([0.1,0.2])\n",
    "\n",
    "A3 = np.dot(Z2,W3) + B3\n",
    "Y = identity_function(A3) # 혹은 Y = A3 , 활성화 함수로 항등함수를 설정했기 때문에"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cafefb9-4e3d-4bce-9b1f-aac42626d26e",
   "metadata": {},
   "source": [
    "> 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 설정한다\n",
    "\n",
    "> 이를테면, 회귀에는 항등함수를 설정하며 이진 분류에는 시그모이드 함수를 설정하고 다중분류에는 소프트맥스 함수를 사용하는 것이 일반적이다\n",
    "\n",
    "> 해당 예에서는 회귀에서 항등함수를 설정하는 경우를 상정하고 출력층의 활성화함수를 설정해준 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b44c75-8e20-45b6-9cd2-165051f86d90",
   "metadata": {},
   "source": [
    "### 구현 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba63ee8f-dabc-4ef1-9ee2-68a8b40bf9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "def init_network():\n",
    "    network = {} # 빈 dict 설정\n",
    "    network['W1'] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "    network['b1'] = np.array([0.1,0.2,0.3])\n",
    "    network['W2'] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "    network['b2'] = np.array([0.1,0.2])\n",
    "    network['W3'] = np.array([[0.1,0.3],[0.2,0.4]])\n",
    "    network['b3'] = np.array([0.1,0.2])\n",
    "    \n",
    "    return network\n",
    "\n",
    "# 해당 함수를 실행시키면 3층짜리 신경망에서 필요한 가중치와 편향을 불러올 수 있음\n",
    "\n",
    "# forward 함수를 실행시키기 위해서는 당연히 network가 설정되어 있어야 하고 network가 설정되기 위해서는 init_network함수가 불러와져야 함\n",
    "def forward(network,x) : \n",
    "    W1,W2,W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1,b2,b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x,W1) + b1\n",
    "    z1 = sigmoid(a1) # 입력층(0층)에서 1층으로 가며 얻은 출력값\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2,W3) + b3\n",
    "    y = identity_function(a3)\n",
    "    \n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1,0.5]) # 0층에서의 입력값\n",
    "y = forward(network,x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974cd5c-5b78-45d3-856e-be8a2d82c420",
   "metadata": {},
   "source": [
    "> 이때 함수 이름을 forward라고 명해준 이유는 신호가 순방향(입력에서 출력 방향)으로 전달됨(순전파)을 알리기 위함\n",
    "\n",
    "> 순전파가 있다는 것은 역전파도 있다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded0f6ff-4647-4fec-8697-c79f14df4787",
   "metadata": {},
   "source": [
    "### 출력층 설계하기\n",
    "- 신경망을 분류와 회귀 모두에 이용할 수 있다. \n",
    "- 다만 둘 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라진다.\n",
    "- 일반적으로 회귀에는 항등 함수를, 분류에는 소프트 맥스 함수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d75f7-9a8c-404b-a729-a66345fc6e8b",
   "metadata": {},
   "source": [
    "> ### 항등 함수와 소프트 맥스 함수 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf3ce4-70d8-427a-83ba-6e48c71e587d",
   "metadata": {},
   "source": [
    "분류에서 사용하는 소프트 맥스의 식을 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567f9a0-6a6a-478b-b132-28c6f4400f05",
   "metadata": {},
   "source": [
    "$$y_k = \\frac{exp(a_k)}{\\Sigma exp(a_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83fe4a2-45f8-46c6-ace8-fe054dc059bf",
   "metadata": {},
   "source": [
    "- 92p의 그림에서 볼 수 있듯, 소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받는다. 소프트맥스 식의 분모에서 볼 수 있듯 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16ad13-7347-44cb-939b-c2352d848b2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1cb06f-3653-49de-8686-16ee0ca67a7f",
   "metadata": {},
   "source": [
    "> 0704~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b8fd4-7a62-4308-9a02-c6804cefd4a9",
   "metadata": {},
   "source": [
    "- 소프트맥스 함수를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96785b00-97eb-4463-9ad4-8084fed3a6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3,2.9,4])\n",
    "\n",
    "exp_a = np.exp(a)\n",
    "sum_exp_a = np.sum(exp_a)\n",
    "\n",
    "y = exp_a/sum_exp_a\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edcd6fc0-bb19-420d-b62f-e809379e6f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2cf8c8-dbf0-4fb4-9d0c-38dda710628d",
   "metadata": {},
   "source": [
    "- 소프트 맥스함수를 컴퓨터로 계산할 때는 결함이 있다.\n",
    "### 바로 오버플로 문제이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35282488-af26-4973-8e10-9d1617ac8550",
   "metadata": {},
   "source": [
    "- 지수함수는 매우 큰 수까지 다루게 되는데 이런 큰 값끼리 나눗셈을 하게 되면 결과 수치가 불안정해진다\n",
    "- 이 문제를 해결하도록 소프트 맥스 함수를 구현 개선해보자\n",
    "- 자세한 식은 93p를 참고한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19bb0b-87f2-4989-9a7b-f4ba3dd5edda",
   "metadata": {},
   "source": [
    "> 오버플로 문제를 해결할 구체적인 예를 하나 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b58cecb-7cef-4aca-ac9e-17c36643c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/_b9p7msx2x5d5j728xn99ps80000gn/T/ipykernel_4375/4152157764.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(a)/ np.sum(np.exp(a))\n",
      "/var/folders/qj/_b9p7msx2x5d5j728xn99ps80000gn/T/ipykernel_4375/4152157764.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  np.exp(a)/ np.sum(np.exp(a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1010,1000,990])\n",
    "np.exp(a)/ np.sum(np.exp(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a7287-3a20-417f-9ce9-5c26e9a2d1b6",
   "metadata": {},
   "source": [
    "- 값이 너무 커 제대로 계산되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe8a69a2-3be4-4560-983f-05ded316f761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, -10, -20])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.max(a)\n",
    "a-c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a335d5b-a59e-44af-9baf-dc3d09304033",
   "metadata": {},
   "source": [
    "> 이렇게 오버플로를 막을 목적으로는 입력 신호 중 최댓값을 이용하는 것이 일반적이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4b5fe51-e4f1-4455-9384-c4e310f33422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(a-c) / np.sum(np.exp(a-c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148cac1-5ac4-4802-bfa7-a534b091d3fa",
   "metadata": {},
   "source": [
    "> 이렇게 입력 신호 중 최댓값을 빼주면 올바르게 계산할 수 있다.\n",
    "\n",
    "> 이를 바탕으로 소프트맥스 함수를 다시 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc2ee07c-ceb2-475f-8eaa-23d43a09ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4d294-63df-4f17-bd74-f3493da906dc",
   "metadata": {},
   "source": [
    "- 소프트맥스 함수의 특징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "432721a9-a210-49af-92e2-d79639ea051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01821127 0.24519181 0.73659691]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([0.3,2.9,4])\n",
    "y = softmax(a)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b31aa80-7644-4c96-aac1-5132de83f982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f568a91-298f-454b-a33a-aac45fdba613",
   "metadata": {},
   "source": [
    "> ### `특징 : 소프트 맥스 함수의 출력의 총합은 1이 된다`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c96d50-264f-4477-b018-1d1c6f65806f",
   "metadata": {},
   "source": [
    "- 따라서 확률로 이해할 수 있게 되는 것이다\n",
    "  - 이를테면 결과 확률들로부터 \"2번째 원소의 확률이 가장 높으니 답은 2번째 클래스다\"라고 할 수 있는 것이다.\n",
    "  - 즉, 소프트맥스 함수를 이용함으로써 문제를 확률적으로 대응할 수 있게 되는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec2d55-3948-492d-a02f-7cb1e1907cef",
   "metadata": {},
   "source": [
    "- 주의점\n",
    "  - 지수 함수(단조 증가 함수)로 이루어진 특성 때문에 소프트맥스 함수를 적용해도 각 원소의 대소관계는 변하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bd5a1-9a77-4134-be2d-8ad5b6bfd74d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14435fb6-2fb4-4916-9bf8-c9a31c8080a5",
   "metadata": {},
   "source": [
    "- 신경망을 이용한 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식한다.\n",
    "- 그리고 소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않는다.\n",
    "  - 위에서 기술했다.\n",
    "- 결과적으로 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 된다.\n",
    "- 현업에서도 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는 것이 일반적이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdadb0e7-6d10-43c3-ba62-82330d6c2f82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a62732-2120-45bc-8d4a-ca4a2f0c1b8f",
   "metadata": {},
   "source": [
    "> 출력층의 뉴런 수 정하기\n",
    "\n",
    "    출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정한다.\n",
    "    분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84798f31-1d27-4e60-a5d0-d30a430e720b",
   "metadata": {},
   "source": [
    "> ### 손글씨 숫자 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f87f704-e7ef-4f65-af54-4dfa58ad0545",
   "metadata": {},
   "source": [
    "(기계학습과 마찬가지로 신경망도 두 단계를 거쳐 문제를 해결한다. 먼저 훈련 데이터를 사용해 가중치 매개변수를 학습하고, 추론 단계에서는 앞서 학습한 매개변수를 사용하여 입력 데이터를 분류한다. )\n",
    "\n",
    "이번 절에서는 이미 학습된 매개변수를 사용하기 때문에, 가중치 매개변수를 학습하는 학습 과정은 생략하고, 추론과정만 구현한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6c52a-b64b-4eda-bc55-8f7a66f1613c",
   "metadata": {},
   "source": [
    "> ### MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "509043c5-6496-4fc0-86c4-3fa0672e057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train),(x_test,t_test) = load_mnist(flatten=True, normalize = False)\n",
    "# 순서대로\n",
    "# 훈련 이미지, 훈련 레이블, 시험 이미지, 시험 레이블\n",
    "\n",
    "# 첫번째 인수인 normalize는 입력 이미지의 픽셀 값을 0~1 사이의 값으로 정규화할지를 정한다.\n",
    "# 두번째 인수인 flatten은 입력 이미지를 평탄하게, 즉 1차원 배열로 만들지를 정한다. False로 정하면 입력 이미지를 1x28x28의 3차원 배열로\n",
    "# 그렇지 않으면 784개의 원소로 이뤄진 1차원 배열로 저장한다. \n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b377773f-bd5d-4e73-a281-0183dc7cc899",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52225af8-e1f0-4b72-ad76-51ffbc47178d",
   "metadata": {},
   "source": [
    "- 파이썬에는 pickle이라는 기능이 있다.\n",
    "  - 프로그램 실행 중에 특정 객체를 파일로 저장하는 기능\n",
    "  - 저장해둔 pickle 파일을 로드하면 실행 당시의 객체를 즉시 복원할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7575335-4d19-4a13-9e2d-90640bea4d82",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dee1b066-1201-467f-94b0-f54e61dcdb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "    \n",
    "(x_train,t_train), (x_test, t_test) = load_mnist(flatten=True, normalize = False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label) # 5라는 label이 붙어있는 데이터임, x_train[0] = 5임\n",
    "\n",
    "print(img.shape)\n",
    "img = img.reshape(28,28) # 원래 이미지의 모양으로 변형\n",
    "print(img.shape)\n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad3d40-ca30-4241-a57f-a900734adbb1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc65c73-102f-4c5b-86bd-579e2feeaa83",
   "metadata": {},
   "source": [
    "> ### 신경망의 추론 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333922d-acd1-426f-bf3b-18926d2f10fd",
   "metadata": {},
   "source": [
    "- 추론을 수행하는 신경망을 구현해보자\n",
    "- 입력층 뉴런을 784개, 출력층 뉴런을 10개로 구성한다.\n",
    "  - 각 데이터가 784개의 픽셀로 구성되어져 있기 때문에 입력층 뉴런은 784개가 필요하다.\n",
    "  - 출력층 뉴런이 10개인 이유는 이 문제가 0에서 9가지의 숫자를 구분하는 문제이기 때문이다.\n",
    "- 한편, 은닉층은 총 두 개로 첫 번째 은닉층에는 50개의 뉴런을, 두 번째 은닝층에는 100갸의 뉴런을 배치할 것이다. \n",
    "  - 여기서 50과 100은 임의로 정한 값이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0899339-58ca-4692-a344-cc2c2aa19c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, flatten = True, one_hot_label = False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\",'rb') as f :\n",
    "        network = pickle.load(f)\n",
    "        \n",
    "    return network\n",
    "\n",
    "def predict(network,x):\n",
    "    W1,W2,W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1,b2,b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x,W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1,W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2,W3) + b3\n",
    "    y = softmax(a3)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52781ddc-3146-478d-9b9d-c130e095b1d1",
   "metadata": {},
   "source": [
    "> 함수 init_network()에서는 pickle 파일인 sample_weight.pkl에 저장된 학습된 가중치 매개변수를 읽는다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f020d4e-af3b-4c25-b4ec-bf7e48a9f240",
   "metadata": {},
   "source": [
    "해당 예제에서는 학습된 가중치 매개변수를 활용할 것이라고 전설하였음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f5b41-81bd-434b-ab98-ac789b13a23a",
   "metadata": {},
   "source": [
    "해당 파일에는 가중치와 편향 매개변수가 딕셔너리 변수로 저장되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d19aac-d076-4367-b3a0-a36aa95f6f19",
   "metadata": {},
   "source": [
    "> 이제 이 세 함수를 사용해 신경망에 의한 추론을 수행해보고, 정확도도 평가해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "920e592f-d447-4beb-bba9-e11ab9742b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd46801-a32e-48ec-9687-c75296e5eae9",
   "metadata": {},
   "source": [
    "```python\n",
    "x,t = get_data() # 이때 x에 할당된 data는 (x_train, t_train)으로서 훈련이미지와 훈련이미지 레이블이다.\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.\n",
    "    if p == t[i]: # 확률이 가장 높은 원소의 인덱스와 시험 데이터의 인덱스가 동일하면 accuracy_cnt를 하나 올려준다.\n",
    "        accuracy_cnt += 1\n",
    "    \n",
    "print(\"Accuracy: \"+ str(float(accuracy_cnt) / len(x)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657166d9-ae7c-4946-aadc-f1f855051264",
   "metadata": {},
   "source": [
    "- 이 예에서는 load_mnist 함수의 인수인 normalize를 True로 설정했다.\n",
    "- normalize를 True로 설정하면 0\\~255 범위인 각 픽셀의 값을 0\\~1 범위로 변환한다.\n",
    "- 단순히 픽셀의 값을 255로 나눈다.\n",
    "- 이처럼 데이터를 특정 범위로 변환하는 처리를 정규화라 하고, 신경망의 입력 데이터에 특정 변환을 가하는 것을 전처리라 한다.\n",
    "- 여기에서는 입력 이미지 데이터에 대한 전처리 작업으로 정규화를 수행한 셈이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2924d4-05c5-4a33-b8e2-d7576befa5eb",
   "metadata": {},
   "source": [
    "- 현업에서는 데이터 전체 평균과 표준편차를 이용하여 데이터들이 0을 중심으로 분포하도록 이동하거나 데이터의 확신 범위를 제한하는 정규화를 수행한다.\n",
    "- 혹은 전체 데이터를 균일하게 분포시키는 데이터 백색화등도 있따. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c964dbc3-c9a4-443b-90ba-c5af221b9889",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf82219-0881-4bc0-bc31-3ba7b4e88126",
   "metadata": {},
   "source": [
    "> ### 배치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7ea0ce-393d-4ff6-9aa5-7335154fecfb",
   "metadata": {},
   "source": [
    "배치?\n",
    "- 하나로 묶은 입력 데이터\n",
    "- 곧 묶음을 의미함\n",
    "- 이미지가 지폐처럼 다발로 묶여있는 형태"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7c4dd-07cd-470a-8037-827d6b6ffc16",
   "metadata": {},
   "source": [
    "배치 처리를 수행함으로써 큰 배열로 이뤄진 계산을 하게 되는데 컴퓨터에서는 큰 배열을 한꺼번에 계산하는 것이 분할된 작은 배열을 여러 번 계산하는 것보다 빠르다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02547c0-9c5f-4af5-8681-9ea8f5bb72e6",
   "metadata": {},
   "source": [
    "배치 처리를 구현해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649e5b7-4ccf-47de-b452-9506b5672121",
   "metadata": {},
   "source": [
    "```python\n",
    "x,t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0,len(x),batch_size):\n",
    "    x_batch = x[i:i+batch_size] # 입력데이터를 지폐처럼 묶음으로 묶는 과정\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1) # 최댓값의 index\n",
    "    # 이때, axis=1의 역할 : 100x10의 배열 중 1번째 차원을 구성하는 각 원소에서 최댓값의 인덱스를 찾도록 한 것\n",
    "    # index는 0부터 시작하므로 실질적으로는 2번째 차원을 의미함\n",
    "    # 아래에서 예를 살펴보자\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt)/len(x)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f15ca2c9-904d-4c93-a758-011f0478f1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0]\n",
      "[3 0 1]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.1,0.8,0.1],[0.3,0.1,0.6],[0.2,0.5,0.3],[0.8,0.1,0.1]])\n",
    "y = np.argmax(x, axis=1)\n",
    "y1 = np.argmax(x,axis=0) \n",
    "y2 = np.argmax(x)\n",
    "print(y) # 행차원\n",
    "print(y1) # 열차원\n",
    "print(y2) # 별 효용성은 없어보이지만 아래 예시를 통해 알아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fc4e7e0-30c3-49dc-8d7e-a81113f49c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1,2,3],[2,10,4],[5,6,7]])\n",
    "np.argmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1701a8-2502-41cf-bf6f-4af5ea3b9a74",
   "metadata": {},
   "source": [
    "axis를 설정해주지 않으면 그냥 1차원적인 배열로 생각하고 index 찾는 것 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af366a33-4bf7-40da-a248-a4fd97771a33",
   "metadata": {},
   "source": [
    "> data를 배치로 처리함으로써 효율적이고 빠르게 처리할 수 있었음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9593e51-2867-463e-bcfc-709acc60672e",
   "metadata": {},
   "source": [
    "> Conclusion\n",
    "\n",
    "    지금까지 신경망의 순전파를 살펴보았다.\n",
    "    이번 장에서 설명한 `신경망`은 각 층의 뉴런들이 다음 층의 뉴런으로 신호를 전달한다는 점에서 앞 장의 퍼셉트론과 같다.\n",
    "    하지만 다음 뉴런으로 갈 때 신호를 변화시키는 활성화 함수에 큰 차이가 있었다. \n",
    "    신경망에서는 매끄럽게 변화하는 시그모이드 함수를 퍼셉트론에서는 갑자기 변화하는 계단 함수를 활성화 함수로 사용했다. \n",
    "    이 차이가 신경망 학습에 중요하다.\n",
    "    다음 장에서 부연 설명한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ea1f81-6bb2-4a7d-a63b-26e489757257",
   "metadata": {},
   "source": [
    "- 신경망에서는 활성화 함수로 시그모이드 함수와 ReLU 함수 같은 매끄럽게 변화하는 함수를 이용\n",
    "- 출력층의 활성화 함수로는 회귀에서는 주로 항등 함수를, 분류에서는 주로 소프트 맥스 함수를 이용한다. \n",
    "- 분류에서는 출력층의 뉴런 수를 분류하려는 클래스 수와 같게 설정한다. \n",
    "- 입력 데이터를 묶은 것을 배치라 하며, 추론 처리를 이 배치 단위로 진행하면 결과를 훨씬 빠르게 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad0532-bea9-472a-bdfe-a2fd53a74259",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8ab021-cabd-41cb-9e85-8b04be4baca8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1db83f-75c0-4d04-9b59-6a3ee47a1826",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4278ebaa-3a75-4ecc-a116-3a04fbeb6e0a",
   "metadata": {},
   "source": [
    "> ### 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4df90-e5fa-41af-bfb9-7bfb40d47c33",
   "metadata": {},
   "source": [
    "신경망 학습\n",
    "- 여기서 학습이란? 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 의미.\n",
    "- 신경망이 학습할 수 있도록 해주는 지표인 손실 함수에 대해 알아보자\n",
    "- 이 `손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것`이 학습 목표이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d94d5f-72a8-42bb-9cbf-bc70b98fa836",
   "metadata": {},
   "source": [
    "->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7421ae-800a-4cc6-aeea-b9a86933c1b6",
   "metadata": {},
   "source": [
    "한 가지 예를 들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbaad4e-e93d-430d-95fc-9dba7d310163",
   "metadata": {},
   "source": [
    "여러 사람이 자필로 작성한 5라는 숫자를 인식하고자 할 때, 5를 인식하는 알고리즘을 밑바닥부터 설계하는 대신, 주어진 데이터를 잘 활용해 해결할 수 있다. 이미지에서 feature를 추출하고 그 feature의 패턴을 기계학습 기술로 학습하는 방법이다. 여기서 말하는 feature는 입력 데이터(입력 이미지)에서 본질적인 데이터를 정확하게 추출할 수 있도록 설계된 변화기를 가리킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c6bee-38f4-4c36-8f2f-e55e96e6f87f",
   "metadata": {},
   "source": [
    "이미지의 특징은 보통 vector로 기술하고, 컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 특징을 많이 사용한다. 이런 특징을 사용하여 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN등으로 학습할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155029d-faa2-4d63-91f9-dbd15f622b7a",
   "metadata": {},
   "source": [
    "- 위에서 언급한 기계학습은 딥러닝을 제외한 머신러닝을 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6ed4c7-be29-407f-a829-a5caf4e73f56",
   "metadata": {},
   "source": [
    "- 기계학습에서 단순 이미지를 벡터로 변환할 때 사용하는 특징은 사람이 설계해야 한다.\n",
    "  - 하지만 신경망은 이미지에 포함된 중요한 특징까지도 기계가 스스로 학습한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd5cf5-565d-4708-a40e-93b07a102659",
   "metadata": {},
   "source": [
    "- 따라서 신경망의 이점은 모든 문제를 같은 맥락에서 풀 수 있다는 점이다.\n",
    "- 예를 들어, 5를 인식하는 문제든, 개를 인식하는 문제든 아니면 사람의 얼굴을 인식하는 문제든 세부사항과 관계없이 신경망은 주어진 데이터를 온전히 학습하고, 주어진 문제의 패턴을 발견하려 시도한다.\n",
    "- 즉, 신경망은 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해 'end-to-end'로 학습한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d0943-919e-4d6c-875e-07add42b9bd0",
   "metadata": {},
   "source": [
    "->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb7642-676e-4bbf-842a-f1e1f5bdab3b",
   "metadata": {},
   "source": [
    "모델의 범용능력을 제대로 평가하기 위해 기계학습에서는 훈련 데이터와 시험 데이터를 구분하여 사용한다.\n",
    "  - 참고로 한 데이터셋에만 지나치게 최적화된 상태를 오버피팅이라고 한다. \n",
    "  - 오버피팅의 해결은 기계학습의 중요한 과제이기도 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b49845-2f5b-463c-afc0-240e178f41b3",
   "metadata": {},
   "source": [
    "->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2fd648-ef9d-4ee7-8009-97d693883f7d",
   "metadata": {},
   "source": [
    "> ### 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b9d8f-b23f-4777-9b71-d8b03f4609db",
   "metadata": {},
   "source": [
    "신경망 학습에서 사용하는 지표로서 이 손실 함수는 임의의 함수를 사용할 수도 있지만, 일반적으로는 오차제곱합과 교차 엔트로피 오차를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea188d3-4f85-41fc-8cae-0c16f438696e",
   "metadata": {},
   "source": [
    "> ### 오차제곱합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688fc45f-8e41-4c2e-8b28-d14c1e69f592",
   "metadata": {},
   "source": [
    "- 신경망의 출력(신경망이 추정한 값), 정답 레이블의 차이의 제곱 합 => $E$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6994a9b4-c202-4dd1-a9d4-6be837f3f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y,t) :\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "# 정답은 2\n",
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "# 예1 : '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y1 = [0.1, 0.05, 0.6, 0, 0.05, 0.1, 0, 0.1, 0, 0]\n",
    "\n",
    "# 예2 : '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y2 = [0.1, 0.05, 0.1, 0, 0.05, 0.1, 0, 0.6, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4929ec37-3ddb-4b18-a4fb-21af0b9a2057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares_error(np.array(y1), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "177d8746-5796-48f5-add9-297a9bc4f149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_squares_error(np.array(y2), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1daf3b0-4270-4dfb-90ec-44721838db48",
   "metadata": {},
   "source": [
    "- 예1\n",
    "  - 정답이 '2'이고 신경망의 출력도 '2'에서 가장 높은 경우\n",
    "- 예2\n",
    "  - 정답이 '2'이고 신경망의 출력은 '7'에서 가장 높은 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524a88c-ba4e-4ab6-9e2e-8af5f39aafca",
   "metadata": {},
   "source": [
    "$\\to$ 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe70c9-36ea-4391-858f-a5c1f5728cb1",
   "metadata": {},
   "source": [
    "$\\to$ 즉, 오차제곱합 기준으로는 첫 번째 추정 결과가 오차가 더 작으니 정답에 더 가까울 것으로 판단할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8a31f-4eaa-452c-89a2-15170866a82e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8bacf-fdeb-41f9-9662-4530faa4bd82",
   "metadata": {},
   "source": [
    "> ### 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6392cd7-2b69-452d-8506-2972f83163c2",
   "metadata": {},
   "source": [
    "또 다른 손실함수로서 이용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca522d-7ea2-459b-93cd-b4a2cc42d5f7",
   "metadata": {},
   "source": [
    "교차 엔트로피 수식 : 실질적으로 (원핫인코딩의 표현 방식 때문에) 정답일 때의 추정의 자연로그를 계산하는 식이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7e064-0df3-4c77-aaad-5329a0961a51",
   "metadata": {},
   "source": [
    "즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0427f9-6aa4-49a1-ab9c-af7e1c1fd134",
   "metadata": {},
   "source": [
    "자연로그의 특징에 의해, 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때, 0이 된다.\n",
    "\n",
    "반대로 정답일 때의 출력이 작아질수록 오차는 커진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38db888b-b5d0-4ade-a96a-905978ca8822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/_b9p7msx2x5d5j728xn99ps80000gn/T/ipykernel_4375/879071729.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  plt.plot(x,np.log(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa2f0a50b80>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0,1)\n",
    "plt.plot(x,np.log(x))\n",
    "\n",
    "# 그림을 살펴보면, 정답에 해당하는 출력의 값이 작아질수록 그 값은 커짐을 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95eef8ad-038f-4bb8-a112-016c10930a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))\n",
    "\n",
    "# 아주 작은 값에 해당하는 delta를 더해주었다.\n",
    "# np.log() 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산이 진행되지 않는 것을 방지하는 것이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "05eafec7-980e-4b6e-b13c-58a2a1335a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y1), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afc76c2e-e8fc-4ebe-bbfe-48d28bea4260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_error(np.array(y2), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a023a-4a32-41ab-8f4c-b94f716e7d68",
   "metadata": {},
   "source": [
    "결과를 보면 확인할 수 있듯, \n",
    "\n",
    "자연로그의 특징에 의해 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때 0이 된다. \n",
    "\n",
    "따라서 정답일 때의 출력이 작아질수록 오차는 커진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388d04f-4bdd-423c-808f-42f88ec7d2ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a4fb4-547d-456d-93f9-baa085a84cf2",
   "metadata": {},
   "source": [
    "> ### 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f5526-3a6c-4239-b67c-6902cceebfe5",
   "metadata": {},
   "source": [
    "훈련 데이터 모두에 대한 손실 함수의 합을 구하는 방법을 생각해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccc778-7f82-4478-b04b-55bc9aed7eb8",
   "metadata": {},
   "source": [
    "데이터 하나에 대한 손실 함수를 N개의 데이터로 확장, 마지막에 N으로 나누어 정규화, N으로 나눔으로써 평균 손실 함수를 구하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fd648-2f66-4608-83d7-c02d37df806d",
   "metadata": {},
   "source": [
    "이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c2050-63f5-4515-a01d-55c526775d7e",
   "metadata": {},
   "source": [
    "하지만, 빅데이터 수준이 되면 그 수는 수백만에서 수천만도 넘는 거대한 값이 되기도 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857cfe4-9049-4eb2-b65b-32125efdcf62",
   "metadata": {},
   "source": [
    "따라서 이 많은 데이터를 일일이 손실 함수를 계산하는 것은 현실적이지 않으며, 이런 경우 데이터 일부를 추려 전체의 근사치로 이용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf67e29-a44f-473c-9855-a888729d6de0",
   "metadata": {},
   "source": [
    "- 신경망 학습에서도 훈련 데이터로부터 일부만 골라 학습을 수행한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3a64f-c65b-49a4-bcb9-a01ba66b78cb",
   "metadata": {},
   "source": [
    "- 이 일부를 미니배치라 한다. \n",
    "  - 가령, 60,000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장만을 사용하여 학습하는 것, 이러한 학습 방법을 미니배치 학습이라 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7c40878-0eb7-4e7e-864e-946e8cb5286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 지정한 수의 데이터를 무작위로 골라내는 코드를 작성해보자\n",
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(X_train, t_train), (x_train, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_size = x_train.shape[0] # 입력 데이터 수\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size) # 입력 데이터 수 60,000개 중에서 10개만 뽑는 것\n",
    "# np.random.choice(a,b) => a중에서 b개만 randomly 뽑는 것\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff2a6d-5015-4c57-bd02-7c816506bda2",
   "metadata": {},
   "source": [
    "- 이제 이 무작위로 선택한 index를 통해 미니배치를 뽑아내면 된다. \n",
    "- 무작위 index는 아래와 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d0be543-714a-457e-9b7b-91e8d979ace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_batch)\n",
    "print(t_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d0fba-e9f5-493b-8069-636b03b90955",
   "metadata": {},
   "source": [
    "> ### 배치용 교차 엔트로피 오차 구현하기\n",
    "\n",
    "    미니배치 같은 배치 데이터를 지원하는 교차 엔트로피 오차를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58bb7cb9-bbf4-4245-b26a-95f4cd0cc420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    # 이때 y는 신경망의 출력, t는 정답레이블이다.\n",
    "    # y가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는 경우엔 reshape 함수로 데이터의 형상을 바꿔준다\n",
    "    # 그리고 배치 크기고 나눠 정규화하고, 이미지 1장당 평균의 교차 엔트로피 오차를 계산한다.\n",
    "    if y.ndim == 1 :\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t*np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbebd1-b5f1-4b9a-aca0-2eb4b0a7e519",
   "metadata": {},
   "source": [
    "- 정답 레이블이 원-핫 인코딩이 아니라 '2'나'7'등의 숫자 레이블로 주어졌을 때의 교차 엔트로피는 다음과 같이 구할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "267c42fb-96cd-45a9-b54a-d2fe241bcf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1 : \n",
    "        t = t.reshape(1,t.size)\n",
    "        y = t.reshape(1,y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arrange(batch_size),t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b83a6-a8f4-4848-94dd-5e4dafbd22fd",
   "metadata": {},
   "source": [
    "이 구현에서는 원핫인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심이다. 다시 말하면 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab6c1b-761e-48df-baed-9db89a994de9",
   "metadata": {},
   "source": [
    "그래서 원-핫 인코딩 시와 레이블 표현시의 교차엔르로피를 통한 손실함수 수식 값이 달라진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953aa23-cb41-457f-bb7a-501f8e687a8a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea6aeb-f2ef-4a93-8a7b-e650664f7bee",
   "metadata": {},
   "source": [
    "> 0708 ~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b8e39-2bb4-402a-bfb5-b38f80210584",
   "metadata": {},
   "source": [
    "- 왜 손실 함수를 설정하는가?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f940ce6-1410-4507-ae9a-9e3fe306120e",
   "metadata": {},
   "source": [
    "가령 여기에 가상의 신경망이 있고 그 신경망의 어느 한 가중치 매개변수에 주목한다 해보자. 이때 그 가중치 매개변수의 손실 함수의 미분이란 가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하나라는 의미이다. 만약 이 미분 값이 음수면 그 가중치 매개변수르르 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다. 그러나 미분 값으 0이면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않는다. 그래서 가중치 매개변수의 갱신은 거기서 멈춘다.\n",
    "\n",
    "정확도를 지표로 삼아서는 안 되는 이유는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문이다. \n",
    "\n",
    "정확도는 매개변수의 미세한 변화에는 거의 반응을 보이지 않고, 반응이 있더라도 그 값이 불연속적으로 갑자기 변화한다. \n",
    "\n",
    "기울기가 0이 되지 않는 덕분에 신경망에 올바르게 학습할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ea2053-da6b-4ad9-86fa-245dc203cc0b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764b830-d3cf-40c7-b50e-76e7acdf6b2a",
   "metadata": {},
   "source": [
    "미분은 특정 순간의 변화량을 뜻한다. \n",
    "\n",
    "그래서 예를 들어 10분이라는 시간을 가능한 한 줄여 한 순간의 변화량, 즉 어느 순간의 속도를 얻는 것이다. \n",
    "\n",
    "x의 작은 변화가 함수 f(x)를 얼마나 변화시키느냐를 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a18ca603-490b-4dc5-973a-d83a10d61453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h) / (2*h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c81cdb-21fa-4f1b-b4a5-52e4581b2af3",
   "metadata": {},
   "source": [
    "- 모든 변수의 편미분을 벡터로 정리한 것을 기울기라고 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58c7403e-183f-4809-baba-31258d1c2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x) :\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같지만 원소는 모두 0인 배열 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / 2*h\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982a2a9-938d-4bb9-982a-f4230ec9c5ae",
   "metadata": {},
   "source": [
    "- (3,4)의 기울기는 (6,8)이다.\n",
    "- 그런데 이 기울기라는 게 의미하는 게 뭘까?\n",
    "  - 129p의 그림 참고(기울기의 결과에 마이너스를 붙인 벡터를 그린 것)\n",
    "- 기울기 그림은 129p의 그림처럼 방향을 가진 벡터로 그려진다. \n",
    "- 기울기는 함수의 가장 낮은 장소(최솟값)를 가리키는 것 같다. \n",
    "- 가장 낮은 곳에서 멀어질수록 화살표의 크기가 커짐을 알 수 있다. \n",
    "- 해당 그림에서 기울기는 가장 낮은 장소를 가리키지만 실제는 반드시 그렇다고는 할 수 없다\n",
    "- 사실 기울기는 각 지점에서 낮아지는 방향을 가리킨다. \n",
    "- ### 더 정확히 말하면 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf4ddfc-7227-4d9e-b822-e12b8d9e6a54",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ef84e-0051-4ae8-aa20-6ef6e3b83acd",
   "metadata": {},
   "source": [
    "각 지점에서 함수의 값을 낮추는 (~이때 함수는 손실 함수~) 방안을 제시하는 지표가 기울기라는 것이다. \n",
    "\n",
    "그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽이 정말로 나아갈 방향인지는 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc1313-9540-4a7b-81b8-4033c1f56443",
   "metadata": {},
   "source": [
    "기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있다. \n",
    "\n",
    "그래서 최솟값이 되는 장소를 찾는 문제(아니면 가능한 한 작은 값이 되는 장소를 찾는 문제)에서는 기울기 정보를 단서로 나아갈 방향을 정해야 한다. \n",
    "\n",
    "따라서 ### 경사법 ###이 등장한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30726208-e736-4f5e-9ec9-8b0b1ea8f28b",
   "metadata": {},
   "source": [
    "경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. \n",
    "\n",
    "그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. \n",
    "\n",
    "이렇게 해서 함수의 값을 점차 줄이는 것이 경사법이다. \n",
    "\n",
    "(특히 신경망 학습에는 경사법을 많이 사용한다)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d21faa-9559-497d-8e98-ab7321b89c9e",
   "metadata": {},
   "source": [
    "- 신경망 학습에서 각 변수를 갱신하는 양을 나타내는 $\\eta$를 학습률이라 한다.\n",
    "\n",
    "- 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6eebd9-e1ab-409c-b259-2007cefc7692",
   "metadata": {},
   "source": [
    "- 또한 학습률 값은 0.01이나 0.001 등 미리 특정 값으로 정해두어야 하는데, 일반적으로 이 값이 너무 크거나 작으면 좋은 장소를 찾아갈 수 없다. \n",
    "\n",
    "신경망 학습에서는 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3129fc4e-3d7e-4e60-948f-3c362b15acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr = 0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed4904-1dde-47bc-b962-7915e8e1392e",
   "metadata": {},
   "source": [
    "인수 f는 최적화하려는 함수, init_x는 초깃값, lr은 학습률, step_num은 경사법에 따른 반복 횟수를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b737b310-0973-4be3-8797-8b1d7c8ee0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9999994,  3.9999992])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문제 : 경사법으로 f(xo,x1) = x0^2 + x1^2의 최솟값을 구하라\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0,4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada70058-a9f8-44c8-88a0-7a3fef2a02e1",
   "metadata": {},
   "source": [
    "- 학습률이 너무 크거나 작으면 함수값이 큰 값으로 발산하거나 혹은 거의 갱신되지 않을 채 끝나버린다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8064c3c-91f9-494f-b31f-642ecff2dd96",
   "metadata": {},
   "source": [
    "- 적절한 학습률 설정이 중요한 이유를 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c017278-2e29-4dc3-b86b-d4d77e0193e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726d426-5e62-4fdc-8887-fa170dfcbb8a",
   "metadata": {},
   "source": [
    "> 0711~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e749eba6-8cf8-489b-a378-0e5a6ac0dff4",
   "metadata": {},
   "source": [
    "신경망 학습에서의 기울기, 즉 가중치 매개변수에 대한 손실 함수의 기울기를 의미함\n",
    "\n",
    "이때 손실함수를 각 매개변수에 대해 편미분한 것들은 이를테면 $w_1$을 조금 변경했을 때 손실 함수 L이 얼마나 변화하느냐를 나타낸다.\n",
    "\n",
    "여기서 중요한 점은 $\\frac{\\partial L}{\\partial W}$의 형상이 $W$와 같다는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798ee35-49f3-4fc3-91ef-699802e4aa46",
   "metadata": {},
   "source": [
    "- 간단한 신경망을 예로 들어 실제로 기울기를 구하는 코드를 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4b642be-f39d-4cf1-900b-78110bd1025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "        \n",
    "    def predict(self,x): # 입력 데이터 x와 가중치 매개변수 W의 행렬곱 계싼\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093b895-d79d-4e40-8f53-efbd7d1ca5a3",
   "metadata": {},
   "source": [
    "simpleNet 클래스는 형상이 2x3인 가중치 매개변수 하나를 인스턴스 변수로 갖는다. \n",
    "\n",
    "메서드는 2개(predict, loss)인데 예측을 수행하는 것과 손실 함수의 값을 구하는 메서드가 있다.\n",
    "\n",
    "(클래스 안에서의 함수를 메서드라 부른다. )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190361c-e6ba-4abf-aa44-f7b505a4bab2",
   "metadata": {},
   "source": [
    "여기에서 인수 x는 입력 데이터, t는 정답 레이블을 의미한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8cc8bf46-a9ff-471d-80b8-d0699ceab9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.95126913  0.84464907  0.86704898]\n",
      " [-0.90022306 -0.62969364  0.5790269 ]]\n",
      "[-1.98096223 -0.05993483  1.0413536 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32290245434682585"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)\n",
    "# 정규분포로 randomly 추출하기 때문에 실행시마다 상이한 결과값이 도출됨을 알 수 있다. \n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "\n",
    "np.argmax(p) # 최댓값의 인덱스\n",
    "\n",
    "t = np.array([0,0,1]) # 정답 레이블\n",
    "net.loss(x,t) # 손실 함수의 값 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d714d7-3488-44d3-81b4-d1b7aa88598f",
   "metadata": {},
   "source": [
    "- 기울기를 구해보자\n",
    "  - numericla_gradient(f,x)를 써서 구한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "356ace25-4a09-404e-8f3b-24ec5485f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02115151  0.14442184 -0.16557335]\n",
      " [ 0.03172726  0.21663277 -0.24836003]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "# 여기에서 정의한 f(W) 함수의 인수 W는 더미로 만든 것이다. \n",
    "# numerical_gradient(f,x) 내부에서 f(x)를 실행하는데, 그와의 일관성을 위해 f(W)를 정의한 것이다. \n",
    "\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab911436-231c-42e5-a4a4-1818c026bc1b",
   "metadata": {},
   "source": [
    "numerical_gradient(f,x)의 인수 f는 함수, x는 함수 f의 인수이다. 그래서 여기에서는 net.W를 인수로 받아 손실 함수를 계산하는 새로운 함수 f를 정의한다. \n",
    "\n",
    "그리고 이 새로 정의한 함수를 numericla_gradient(f,x)에 넘긴다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3e522-4f35-470d-b4c7-60496f4fc161",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360646a-e931-48bf-9a83-6c383d16a991",
   "metadata": {},
   "source": [
    "> 간단한 함수라면 새 함수를 정의할 때 람다를 사용하면 더 용이하게 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97f68368-8626-4c70-b8d1-b815bbeaea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w : net.loss(x,t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a81c695-6b6e-4981-9222-a5b897d1a5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x : x+2)(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3eb8da-ae9f-4cff-9480-5b737f44c114",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7411af-d170-4164-b71b-0c1225d0e4db",
   "metadata": {},
   "source": [
    "> ### 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f82030-32c7-4e27-a9d5-3d616156045a",
   "metadata": {},
   "source": [
    "신경망 학습의 절차는 다음과 같다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40481b-a44c-476e-aab3-78b73c5f0c9d",
   "metadata": {},
   "source": [
    "전제 > 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d00b2-80ea-4214-bb43-533a7ee9437a",
   "metadata": {},
   "source": [
    "1. 미니배치\n",
    "  - 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다. \n",
    "  \n",
    "2. 기울기 산출\n",
    "  - 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "  \n",
    "3. 매개변수 갱신\n",
    "  - 가중치 매개변수를 기울기 방향으로 `아주 조금` 갱신한다. \n",
    "  \n",
    "4. 반복\n",
    "  - 1~3단계를 반복한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14704038-45f1-40da-9805-986b3f164dfb",
   "metadata": {},
   "source": [
    "- 이것이 신경망 학습이 이뤄지는 순서이다. 이는 경사 하강법으로 매개변수를 갱신하는 방법이며, 이때 데이터를 미니배치로 무작위 선정하기 때문에 확률적(미니배치라는 특성 때문에)경사 하강법이라고 부른다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d255435-fb9c-4fd7-a061-c2e66e104db7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442d4e2-42f7-48f6-9b3e-66c50a07529e",
   "metadata": {},
   "source": [
    "- 실제로 손글씨 숫자를 학습하는 신경망을 구현해보자\n",
    "- 여기에서는 2층 신경망(은닉층이 1개인 네트워크)을 대상으로 MNIST 데이터셋을 사용하여 학습을 수행한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27ca559b-adb6-43e9-b816-64207b6871cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "        # __init__메서드 : 클래스를 초기화 한다.\n",
    "        # 초기화 메서드인 __init__메서드는 TwoLayerNet을 생성할 때 불리는 메서드임\n",
    "        # 즉 객체가 생성될 때만 불려지는 메서드이다. \n",
    "        # 은닉층의 개수인 hidden_size는 적당한 값을 설정한다.\n",
    "        # 이 초기화 메서드 __init__에서는 가중치 매개변수도 초기화한다. (변수 params를 의미함 )\n",
    "        # 가중치 매개변수의 초깃값을 무엇으로 설정하냐가 신경망 학습의 성공을 좌우하기도 한다.\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화 수행\n",
    "        # 인수 => 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수\n",
    "        # 이때 TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 가짐.\n",
    "        # params 변수에는 가중치 매개변수가 저장되는데, 예를 들어 1번째 층의 가중치 매개변수는 params['W1']키에 넘파이 배열로 저장된다. \n",
    "        # 마찬가지로 1번째 층의 편향은 params['b1']로 접근한다. \n",
    "        # 즉, params 변수에는 이 신경망에 필요한 매개변수가 모두 저장된다. \n",
    "        # 그리고 params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용된다. \n",
    "        # 참고로 예측 처리는 다음과 같이 실행할 수 있다. \n",
    "        # x = np.random.rand(100,784) # 더미 입력 데이터(100장 분량)\n",
    "        # y = net.predict(x)\n",
    "        # 이때 net은 클래스 TwoLayerNet의 객체임 (net = TwoLayerNet() like this way)\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x): # 이때 인수 x는 입력 이밈지 데이터를 의미한다. \n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 손실 함수의 값을 구하는 메서드이며, 해당 메서드는 predict()의 결과와 정답 레이블을 바탕으로 교차 엔트로피 오차를 구하도록 구현했다.\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    # 각 매개변수의 기울기를 계산한다. \n",
    "    # 수치 미분 방식으로 각 매개변수의 손실 함수에 대한 기울기를 계산한다. \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        # grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다. \n",
    "        # 예를 들어 다음과 같이 numerical_gradient() 메서드를 사용해 기울기를 계싼하면 grads 변수에 기울기 정보가 저장된다.\n",
    "        # 다음과 같이 사용할 수 있다. \n",
    "        # x = np.random.rand(100,784) # 더미 입력 데이터(100장 분량)\n",
    "        # t = np.random.rand(100,10) # 더미 정답 레이블(100장 분량)\n",
    "        # grads = net.numerical_gradient(x, t) # 기울기 계산\n",
    "        # 이때 net은 해당 클래스의 객체 이름임\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354334f-26f4-4763-83eb-3c661b3bfe32",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669919e0-70a3-45e1-8951-89540f046947",
   "metadata": {},
   "source": [
    "> 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12324a88-ab50-4cb2-aebe-f712c8171c72",
   "metadata": {},
   "source": [
    "신경망 학습 구현에는 앞에서 설명한 미니배치 학습을 활용한다.\n",
    "\n",
    "미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고(미니배치), 그 미니배치에 대해서 경사법으로 매개변수를 갱신한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a6a6567-60c3-460b-af8c-d314904042b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09741666666666667, 0.0982\n",
      "train acc, test acc | 0.79715, 0.8016\n",
      "train acc, test acc | 0.8785666666666667, 0.8815\n",
      "train acc, test acc | 0.8977833333333334, 0.8994\n",
      "train acc, test acc | 0.9075166666666666, 0.9107\n",
      "train acc, test acc | 0.9134833333333333, 0.9158\n",
      "train acc, test acc | 0.9190666666666667, 0.9208\n",
      "train acc, test acc | 0.92385, 0.9255\n",
      "train acc, test acc | 0.9273, 0.928\n",
      "train acc, test acc | 0.9302166666666667, 0.9325\n",
      "train acc, test acc | 0.9345, 0.936\n",
      "train acc, test acc | 0.9371166666666667, 0.9383\n",
      "train acc, test acc | 0.9394166666666667, 0.9398\n",
      "train acc, test acc | 0.9415666666666667, 0.9418\n",
      "train acc, test acc | 0.9431666666666667, 0.942\n",
      "train acc, test acc | 0.94515, 0.9444\n",
      "train acc, test acc | 0.9472, 0.9463\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "# 미니배치 크기를 100으로 한다. \n",
    "# 즉, 매번 60,000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려낸다.\n",
    "# 그리고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다. \n",
    "# 경사법에 의한 갱신 횟수(반복 횟수) 10,000번으로 설정하고, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고 그 값을 배열에 추가한다. \n",
    "# 이 손실 함수의 값이 변화하는 추이를 그래프로 나타내면\n",
    "# 학습 횟수가 늘어가면서 손실 함수의 값이 줄어듦을 알 수 있다. \n",
    "# 이는 학습이 잘 되고 있다는 뜻으로, 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미한다.\n",
    "# 다시 말해, 데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 서서히 다가서고 있는 것\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "        \n",
    "        \n",
    "#############################################################################################\n",
    "        \n",
    "        \n",
    "# 그래프 그리기\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52684f-fa4a-461e-8316-c2cf51f2ec5b",
   "metadata": {},
   "source": [
    "해당 그림에서는 1에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고, 그 결과를 기록한다. \n",
    "\n",
    "정확도를 1에폭마다 계산하는 이유는 for문 안에서 매번 계산하기에는 시간이 오래 걸리고, 또 그렇게가지 자주 기록할 필요도 없기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc22fc7-62a9-46eb-8a00-9314f772af76",
   "metadata": {},
   "source": [
    "그래프에서 볼 수 있듯, 에폭이 진행될수록 즉, 학습이 진행될수록 훈련 데이터와 시험 데이터를 사용하고 평가한 정확도가 모두 좋아지고 있다. \n",
    "\n",
    "또, 두 정확도에는 차이가 없음을 알 수 있다. 다시 말해 이번 학습에서는 오버피팅이 일어나지 않았음을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29abcd4a-a602-482b-9e4f-f8a1bd54b816",
   "metadata": {},
   "source": [
    "> 만약 오버피팅이 일어난다면 이 모습은 어떻게 달라질까?\n",
    "\n",
    "    훈련이란 훈련데이터에 대한 정확도를 높이는 방향으로 학습하는 것이니 그 정확도는 에폭을 반복할 수록 높아진다. \n",
    "    반면 훈련 데이터에 지나치게 적응하면, 즉 오버피팅되면 훈련 데이터와는 다른 데이터를 보면 잘못된 판단을 하기 시작한다. \n",
    "    어느 순간부터 시험 데이터에 대한 정확도가 점차 떨어지게 된다는 것이다. \n",
    "    \n",
    "    **이 순간이 오버피팅이 시작되는 순간이다. 이 순간을 포착해 학습을 중단하면 오버피팅을 효과적으로 예방할 수 있다. 이 기법을 조기종료라고 하며 가중치 감소, 드롭 아웃이 오버피팅의 대표적인 예방법이라고 할 수 있다**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fa0e2b-0cb1-46be-8d7f-583a29930b54",
   "metadata": {},
   "source": [
    "> ### Conclusion\n",
    "\n",
    "    신경망 학습에 대해 배웠다.\n",
    "    가장 먼저 신경망이 학습을 수행할 수 있도록 손실 함수라는 지표를 도입했다. \n",
    "    이 손실 함수를 기준으로 그 값이 가장 작아지는 가중치 매개변수 값을 찾아내는 것이 신경망 학습의 목표이다.\n",
    "    또, 가능한 한 작은 손실 함수의 값을 찾는 수법으로 경사법을 소개했다. \n",
    "    경사법은 함수의 기울기를 이용하는 방법이다. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda47eb-17c8-4818-bb57-34b91d329ef8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c758a26-3ece-4aa6-8e1b-4f1088e67aaa",
   "metadata": {},
   "source": [
    "### 오차역전파법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b2d0ff-8f19-45fb-94bf-11f28dc5dd36",
   "metadata": {},
   "source": [
    "종전에 설명했던 신경망의 가중치 매개변수의 기울기는 수치 미분을 사용해 구했다.\n",
    "\n",
    "수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸린다는 게 단점이다. \n",
    "\n",
    "이번 장에서는 가중치 매개변수의 기울기를 효율적으로 계산하는 오차역전파법을 배워본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7fa4ce-1220-476c-af46-0001d00f226e",
   "metadata": {},
   "source": [
    "> 계산 그래프를 통해 오차역전파법에 대해 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c150e-1fa7-4092-afbf-c4fd4b02757c",
   "metadata": {},
   "source": [
    "계산 그래프는 계산 과정을 그래프로 나타낸 것이다. 이때, 그래프는 복수의 노드와 에지로 표현된다. \n",
    "\n",
    "노드 사이의 직선을 에지라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c355e-62a4-47a4-8290-ec91db2e4f51",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8953a8-f0aa-4f6a-8196-461bfaceb9c4",
   "metadata": {},
   "source": [
    "> 0718~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b485ec1-7969-4421-8e81-09b7775c2f65",
   "metadata": {},
   "source": [
    "계산 그래프를 이해해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3140bb-1a2d-48ed-aa09-b5d119419422",
   "metadata": {},
   "source": [
    "계산 그래프는 계산 과정을 노드와 화살표로 표현한다. 노드는 원으로 표기하고 원 안에 연산 내용을 적는다. 또, 계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ad915-fec1-47d4-8139-9b2cb606fb00",
   "metadata": {},
   "source": [
    "1. 계산 그래프를 구성한다. \n",
    "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다. \n",
    "  - 이때, 계산을 왼쪽에서 오른쪽으로 진행하는 단계를 순전파라고 한다. \n",
    "  - 순전파는 계산 그래프의 출발점부터 종착점으로의 전파이다. \n",
    "  - 반대로 오른쪽에서 왼쪽으로 진행하는 역전파도 있을 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef7b0cf-8e9f-4cf9-a0f2-4b8273726a67",
   "metadata": {},
   "source": [
    "계산 그래프의 특징은 `국소적 계산`을 전파함으로써 최종 결과를 얻는다는 점에 있다. \n",
    "\n",
    "국소적 계산은 전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 것이다. \n",
    "\n",
    "즉, 각 노드는 자신과 관련한 계산외에는 아무것도 신경 쓸 게 없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb733c8-2a4a-4200-b04b-572ae5fdb717",
   "metadata": {},
   "source": [
    "-> 이처럼 계산 그래프는 국소적 계산에 집중한다. 전체 계산이 제아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다. 국소적인 계산은 단순하지만, 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 해낼 수 있다. \n",
    "\n",
    "-> 또한 중간 계산 결과를 모두 보관할 수 있다. \n",
    "\n",
    "-> ***역전파를 통해 미분을 효율적으로 계산할 수 있다.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00494d4-9e17-49d3-8b60-72e567c982f0",
   "metadata": {},
   "source": [
    "역전파는 국소적인 미분을 순방향과는 반대인 오른쪽에서 왼쪽으로 전달한다. \n",
    "\n",
    "또한 이 국소적 미분을 전달하는 원리는 연쇄법칙에 따른 것이다. \n",
    "\n",
    "역전파의 계산 절차는 신호 E에 국소적 미분, $\\frac{\\partial y }{\\partial x}$을 곱한 후 다음 노드로 전달하는 것이다. \n",
    "\n",
    "-> 즉, $E\\frac{\\partial y }{\\partial x}$를 다음 노드로 전달한다. 여기에서 국소적 미분은 순전파 때의 y=f(x) 계산의 미분을 구한다는 것이며, 이는 x에 대한 y의 미분을 구한다는 뜻이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c33cc10-fb4c-4bfe-ba57-cc2cfae36776",
   "metadata": {},
   "source": [
    "- 이것이 역전파의 계산 순서인데, 이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 이 전파의 핵심이다. 왜 그런 일이 가능한가는 연쇄법칙의 원리로 설명할 수 있다. 연쇄법칙에 대해 알아보자\n",
    "- 연쇄법칙은 합성 함수의 미분에 대한 성질이며, 다음과 같이 정의된다. \n",
    "  - 합성 함수의 미분은 함성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505617cd-32a3-410a-918d-5adb5b3538e5",
   "metadata": {},
   "source": [
    "- $z = t^2$\n",
    "- $t = x+y$\n",
    "- 즉, $z = (x+y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0115a-086d-4123-9c07-75d4f3989b33",
   "metadata": {},
   "source": [
    "- 이때, z를 x에 대해 미분해보자\n",
    "- $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial t} \\frac{\\partial t}{\\partial x}$\n",
    "- $\\frac{\\partial z}{\\partial t} = 2t$\n",
    "- $\\frac{\\partial t}{\\partial x} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb9543c-0e4c-4518-a3cb-098df10b7625",
   "metadata": {},
   "source": [
    "- 따라서 $\\frac{\\partial z}{\\partial x}$는 $2t = 2(x+y)$임을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cca58e-3928-4328-ad5a-16f9bdcb6c9a",
   "metadata": {},
   "source": [
    "> #### 계산 그래프의 역전파가 연쇄법칙에 따라 진행된다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e2af3-cbe1-44b2-83dd-f02a0a7210a3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4339cb6-54ea-436d-9691-34f8affefcf3",
   "metadata": {},
   "source": [
    "#### 덧셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38124789-e08f-43d0-af92-49ec943db997",
   "metadata": {},
   "source": [
    "덧셈 노드의 역전파때는 상류에서 전해진 미분에 1을 곱하여 하류로 흘린다. 따라서 입력된 값을 그대로 다음 노드로 보내게 되는 것이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c837c-22ec-4dbf-8a72-4c978f0491f2",
   "metadata": {},
   "source": [
    "(최종 출력으로 가는 계산의 중간에 덧셈 노드가 존재한다. 역전파에서는 국소적 미분이 가장 오른쪽의 출력에서 시작하여 노드를 타고 역방향으로 전파된다. )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780d616-6f30-42ef-8fdb-82d8913d0229",
   "metadata": {},
   "source": [
    "덧셈 노드 역전파는 입력 신호를 다음 노드로 출력할 뿐이므로 입력된 값을 그대로 다음 노드로 전달하는 것이 핵심이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b32db8-5cf3-4ad4-994e-dccd8df79ceb",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae44705-8dba-40bd-b8b4-1297d79d26c4",
   "metadata": {},
   "source": [
    "#### 곱셈 노드의 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd209a3-d9ff-4d2f-aa95-143d5a22e5db",
   "metadata": {},
   "source": [
    "곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들(위치적으론 왼쪽에 위치해있던 신호들)을 서로 바꾼 값을 곱해서 하류로 보낸다. \n",
    "\n",
    "서로 바꾼 값이란 순전파 때 x였다면 역전파에서는 y, 순전파 때 y였다면 역전파에서는 x로 바꾼다는 의미이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de4f82-258b-4f98-86ce-459087c08531",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914d1af-c7aa-4764-aea7-e4b38171edb0",
   "metadata": {},
   "source": [
    "> 덧셈의 역전파에서는 상류의 값(위치상 오른쪽에 위치해있던 값)을 그대로 흘려보내서 순방향 입력 신호의 값을 필요하지 않았지만, 곱셈의 역전파는 순방향 입력 신호의 값이 필요하다. 그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해둔다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9f9db-657b-4b74-be22-b48c267b178f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7934b0-04e7-4856-a7ba-88bf86f9e8c7",
   "metadata": {},
   "source": [
    "> #### 사과쇼핑의 예를 파이썬으로 구현해보자\n",
    "\n",
    "> 계산 그래프의 곱셈 노드를 'MulLayer', 덧셈 노드를 'AddLayer'라는 이름으로 구현한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71e977-f7e8-4c86-bf07-0a691a84c770",
   "metadata": {},
   "source": [
    "1) 곱셈 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65787bb5-8d1e-430c-b005-be844d213470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self): # 인스턴스 변수인 x와 y를 초기화한다. 이 두 변수는 순전파 시의 입력 값을 유지하기 위해서 사용한다. \n",
    "        self.x = None \n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y                \n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): # 이때 dout는 상류에서 넘어온 미분값. \n",
    "        # 상류에서 넘어온 미분인 dout에 순전파 때의 값을 서로 바꿔 곱한 후 하류로 흘린다. \n",
    "        dx = dout * self.y \n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy\n",
    "\n",
    "\n",
    "class AddLayer:\n",
    "    \n",
    "    # 덧셈 계층에서는 초기화가 필요 없으니 __init__()에서는 아무 일도 하지 않는다. \n",
    "    # 곱셈 계층에서는 초기화가 필요했었음. . .\n",
    "    # 내 생각인데\n",
    "    # 곰셈 계층의 역전파에서는 입력값이 지정이 돼있어야 역전파의 진행이 가능했었음\n",
    "    # 그런데 덧셈 계층의 역전파에서는 그냥 입력값을 그대로 흘려보내주는 역할만 하기 때문에\n",
    "    # 덧셈 계층의 역전파에서는 입력값이 따로 필요 없음\n",
    "    # 즉, 초기화라는 단어의 의미는 우리가 흔히 아는 reset의 의미보다는\n",
    "    # 값을 지정해준다? 정도로 기억하면 될 것 같음\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "    # 즉, backward()에서는 상류에서 내려온 미분을 그대로 하류로 흘릴뿐이다. \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "19ee8170-0356-466f-9c44-484f1609d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "# 개당 가격이 100원인 사과 2개의 가격에 소비세 1.1을 곱한 결과는?\n",
    "# 220임을 알 수 있다. \n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c3396-78fd-4422-99de-c77ad6c31c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0a12d0d-2a0b-4c0f-9862-69d5dca44038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# 위치상 가장 오른쪽에 위치해있는 입력값\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60e065-b7e8-41b6-aeab-4c6b66069dac",
   "metadata": {},
   "source": [
    "- backward() 호출 순서는 forward() 때와는 반대이다. 또, backward()가 받는 인수는 순전파의 출력에 대한 미분임에 주의해야 한다. 가령 mul_apple_layer라는 곱셈 계층은 순전파 때는 apple_price를 출력하지만, 역전파 때는 apple_price의 미분 값인 dapple_price를 인수로 받는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62179920-387f-4a0d-87c2-a6422c8d8626",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb3b5f-3bdc-4a67-8170-a449919f19fa",
   "metadata": {},
   "source": [
    "> ### 활성화 함수 계층 구현하기\n",
    "\n",
    "    계산 그래프를 신경망에 적용해보자.\n",
    "    여기에서는 신경망을 구성하는 층 각각을 클래스 하나로 구현한다. \n",
    "    우선은 활성화 함수인 ReLU와 Sigmoid 계층을 구현하자. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96dcf5-bbf8-40f3-a1f1-fdc44b2fa0e7",
   "metadata": {},
   "source": [
    "> #### ReLU 계층?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "234665ed-5936-4bf5-abbf-fee98f30e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        # mask는 True/False로 구성된 넘파이 배열로, 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그 외는 Flase로 유지한다.\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb558f-4af7-4435-ae9f-607907966f7a",
   "metadata": {},
   "source": [
    "> 순전파 때의 입력 값이 0 이하면 역전파 때의 값은 0이 돼야 한다. 그래서 역전파 때는 순전파 때 만들어둔 mask를 써서 mask의 원소가 True인 곳(즉, 입력 x값이 0이하인 곳)에는 상류에서 전파된 dout을 0으로 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a6475-4d7d-466c-82b7-1a0145113f1e",
   "metadata": {},
   "source": [
    "> ReLU 계층은 전기 회로의 스위치에 비유할 수 있다. 순전파 때 전류가 흐르고 있으면 스위치를 ON으로 하고, 흐르지 않으면 off한다. 역전파 때는 스위치가 ON이라면 전류가 그대로 흐르고(1을 곱하기 때문에) OFF면 더 이상 흐르지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d7932-1ef9-4758-b6cb-75e15595dd95",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97c521-02e7-4bc9-8ae4-389f302d6413",
   "metadata": {},
   "source": [
    "> ## Sigmoid 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14280528-6741-46ef-bfa2-45cf515da931",
   "metadata": {},
   "source": [
    "다음은 시그모이드 함수식이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d3d57-5571-4085-9980-f82c92846963",
   "metadata": {},
   "source": [
    "### $y = \\frac{1}{1 + exp(-x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1f899-019b-4f75-8d0c-e81bddecd821",
   "metadata": {},
   "source": [
    "위의 시그모이드 함수식도 국소적 계산의 전파로서 이해할 수 있음\n",
    "\n",
    "즉, 역전파도 행할 수 있다는 것이고, 오른쪽에서 왼쪽으로 한 단계씩 짚어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b528d0-1df8-4277-afd3-d1ae50f827a6",
   "metadata": {},
   "source": [
    "$\\to$ 역전파의 과정을 살펴보면, 역전파의 최종 출력인 $\\frac{\\partial L}{\\partial y}y^2exp(-x)$ 를 순전파의 입력 x와 출력 y만으로 계산할 수 있음을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91c948b-928f-494a-bbc6-8c1bef6564f9",
   "metadata": {},
   "source": [
    "$\\to$ 또한 $\\frac{\\partial L}{\\partial y}y^2exp(-x)$를 정리해서 써보면 Sigmoid 계층의 역전파는 순전파의 출력 y만으로 계산할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3e2d5f6-1de6-4405-866e-753467518551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    # 순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파 때 그 계산을 사용한다.\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb50120-e704-4455-a926-7893b510c60e",
   "metadata": {},
   "source": [
    "했을 때 다음에 할 수 있는 말에 대해 고를 수 있는 건 다름 아닌 바로 그 다음 단계로 나아갈 수 있는 것이다. 지금은 이 셀들이 진행중인 것이고 앞으로 다가가는 approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6323f-f472-4b93-a0a5-54a849eea6e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c89fc8c-412f-4812-ae57-38b910dcb87c",
   "metadata": {},
   "source": [
    "> ### 0726~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51fcff-fb17-485d-99cc-822915a2db76",
   "metadata": {},
   "source": [
    "### Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f715e-398f-477e-b884-a2a5ec3487c5",
   "metadata": {},
   "source": [
    "- 행렬의 곱 계산은 대응하는 차원의 원소 수를 일치시키는 게 핵심이다. \n",
    "- 신경망의 순전파 때 수행하는 행렬의 곱은 기하학에서는 어파인 변환이라고 한다.\n",
    "- 따라서 해당 교재에서는 어파인 변환을 수행하는 처리를 Affine 계층이라는 이름으로 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e7d44f-2061-4c63-bd20-031e9e07b370",
   "metadata": {},
   "source": [
    "- 지금까지의 계산 그래프는 노드 사이에 스칼라값이 흘렀는 데 반해, 이 예에서는 행렬이 흐르고 있는 것이다.\n",
    "- 행렬 곱(dot 노드)의 역전파는 행렬의 대응하는 차원의 원소 수가 일치하도록 곱을 조립하여 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f1136-5e97-46e0-95f4-1ddd07237191",
   "metadata": {},
   "source": [
    "> ### 배치용 Affine 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb6963-b1b3-4828-a2d8-53e199658abe",
   "metadata": {},
   "source": [
    "지금까지 설명한 Affine 계층은 입력 데이터로 X 하나만을 고려한 것이다. 이번엔 데이터 N개를 묶어 순전파하는 경우, 즉 배치용 Affine 계층을 생각해보자\n",
    "\n",
    "(이떄 묶은 데이터를 배치라고 부른다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d099f88-2f04-457b-934b-bc4da30df1bf",
   "metadata": {},
   "source": [
    "***역전파 때는 행렬의 형상에 주의하여 곱을 조정한다***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9484164-b256-435a-ad75-6a8140977a5b",
   "metadata": {},
   "source": [
    "순전파의 편향 덧셈은 각각의 데이터에 더해진다.\n",
    "\n",
    "그래서 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.\n",
    " \n",
    "편향의 역전파는 그 두 데이터에 대한 미분을 데이터마다 더해서 구한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "117066c3-6d66-4b44-a805-f11a318abe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10672ba5-0aef-4996-a697-5ccecac19dc5",
   "metadata": {},
   "source": [
    "### Softmax-with-loss 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc7317-1221-4bba-807b-e8552674451d",
   "metadata": {},
   "source": [
    "출력층에서 사용하는 소프트맥스 함수에 관해 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7677c5-d596-4673-a3cb-bd4d8140b5f6",
   "metadata": {},
   "source": [
    "소프트맥스 함수는 입력 값을 정규화하여 출력한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4d8357-8479-435d-a3da-87f5eb2700c6",
   "metadata": {},
   "source": [
    "입력이미지가 Affine 계층과 ReLU 계층을 통과하며 변환되고, 마지막 Softmax 계층에 의해서 10개의 입력이 정규화되는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a82dd6-35df-4b64-b427-d04cb0137f74",
   "metadata": {},
   "source": [
    "주목할 것은 역전파의 결과이다. Softmax 계층의 역전파는 꽤 말끔한 결과를 내놓는다.\n",
    "\n",
    "Softmax 계층의 출력과 정답 레이블의 차분을 전달한다. \n",
    "\n",
    "신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해지는 것이다. \n",
    "\n",
    "이는 신경망 학습의 중요한 성질이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46bad9ce-5446-4107-a78e-68dbe5024b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55f2af-1bcc-4b0d-a8c0-8275ec89c787",
   "metadata": {},
   "source": [
    "또, 역전파때는 전파하는 값을 배치의 수로 나눠서 데이터 1개당 오차를 앞 계층으로 전파하는 점에 주의해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294d8fd-e707-4f9c-a732-648218cc53f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdce2ff-469d-433d-a6b7-91c533733f66",
   "metadata": {},
   "source": [
    "> 지금까지 구현한 계층을 조합해서 신경망을 구축해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39adeae-1b41-42e3-bc43-fce1671151fb",
   "metadata": {},
   "source": [
    "> 신경망 학습의 전체 그림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3fc2cc-b76b-424d-946b-510fdf892cff",
   "metadata": {},
   "source": [
    "전제) 신경망에는 적용 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다. \n",
    "\n",
    "1단계) 미니배치\n",
    "- 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다.\n",
    "\n",
    "2단계) 기울기 산출\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다. \n",
    "\n",
    "3단계) 매개변수 갱신\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다. \n",
    "\n",
    "4단계) 반복\n",
    "- 앞의 1~3단계를 반복한다. \n",
    "\n",
    "> 이때 오차역전파법이 등장하는 단계는 두 번째인 기울기 산출이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539c8c5-b8de-4b35-805e-627699397a5a",
   "metadata": {},
   "source": [
    "> 앞 장에서는 이 기울기를 구하기 위해서 수치 미분을 사용했던 것이다.  그런데 수치 미분은 구현하기는 쉽지만 계산이 오래걸렸다.  오차역전파법을 이용하면 느린 수치 미분과 달리 기울기를 효율적이고 빠르게 구할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae1286-0028-4bad-bd2e-c4edc4e7b3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
