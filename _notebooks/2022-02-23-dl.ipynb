{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66756f6-96a3-4e5a-8257-5c581e079fb7",
   "metadata": {},
   "source": [
    "# 딥러닝의 미래"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd51d87-3d2c-461b-8d6d-caca7fcb9288",
   "metadata": {},
   "source": [
    "- 이미지 스타일(화풍) 변환\n",
    "  - 딥러닝을 활용해 화가처럼 그림을 그리는 것\n",
    "  - 콘텐츠 이미지와 스타일 이미를 조합해 새로운 그림을 그려준다.\n",
    "  - 이 기술은 네트워크의 중간 데이터가 콘텐츠 이미지의 중간 데이터와 비슷해지도록 학습한다. 이렇게 하면 입력 이미지를 콘텐츠 이미지의 형태를 흉내낼 수 있다.\n",
    "  - 또, 스타일 이미지의 화풍을 흡수하기 위해 스타일 행렬이라는 개념을 도입한다. 그 스타일 행렬의 오차를 줄이도록 학습하여 입력 이미지를 고흐의 화풍과 비슷해지게 만들 수 있는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be202075-480a-40a5-aaef-0342af1e3640",
   "metadata": {},
   "source": [
    "- 이미지 생성\n",
    "  - 이미지 스타일 변환은 새로운 그림을 생성하려면 이미지 두 장을 입력해야 했다. \n",
    "  - 하지만 이 방법은 입력 이미지 없이도 새로운 이미지를 그려내는 연구이다. \n",
    "    - DCGAN 기법이 있다. \n",
    "    - 해당 기법의 핵심은 생성자와 식별자로 불리는 2개의 신경망을 이용한다는 점이다. \n",
    "    - 생성자가 진짜와 똑같은 이미지를 생성하고 식별자는 그것이 진짜인지(생성자가 생성한 이미지인지, 아니면 실제로 촬영된이미지인지)를 판정한다. 이런 방식으로 둘을 겨루도록 학습시켜, 생성자는 더 정교한 가짜 이미지 생성 기술을 학습하고 식별자는 더 정확하게 간파할 수 있는 감정사로 성장하는 것이다. 이렇게 둘의 능력을 수련시킨다는 개념이 GAN이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f25f13-b15b-4f83-a39c-20741b90b6e1",
   "metadata": {},
   "source": [
    "- 자율 주행\n",
    "  - 자율 주행은 주행 경로를 정하는 경로 계획 기술과 카메라나 레이저 등의 탐사 기술 그리고 주위 환경을 올바르게 인식하는 기술이 필요하다 그 중 주위 환경을 올바르게 인식하는 기술이 가장 중요하다. \n",
    "  - 예를 들어 SegNet이라는 CNN기반 신경망은 주변 환경을 정확하게 인식해낸다. \n",
    "    - 입력 이미지를 분할(픽셀 수준에서 판정)하고 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4291f9c-cd9f-4c42-baf4-65a97ebb6075",
   "metadata": {},
   "source": [
    "- Deep Q-Network(강화학습)\n",
    "  - 사람이 자전거를 배울 때 시행착오를 겪으며 배우듯, 컴퓨터도 시행착오 과정에서 스스로 학습하게 하려는 분야이다.\n",
    "  - 가르침에 의존하는 지도 학습과는 다른 분야이다.\n",
    "  - 에이전트라는 것이 환경에 맞게 행동을 선택하고, 그 행동에 의해서 환경이 변한다는 게 기본적인 틀이다. 환경이 변화하면 에이전트는 어떠한 보상을 얻는다. 강화학습의 목적은 더 나은 보상을 받는 쪽으로 에이전트의 행동 지침을 바로잡는 것이다. \n",
    "  - 이때 보상은 정해진 것이 아니라 예상 보상이다. 예를 들어, 게임 캐릭터 마리오를 오른쪽으로 이동시켰을 때 얻는 보상이 항상 명확하진 않다. 상황에 따라 그것이 100원이 될 수도 혹은 장애물이 될 수도 혹은 1000원이 될 수도 있듯 어떤 상황에서 이동한 것이냐에 따라 보상은 천차만별이 될 수 있다. 이런 불명확한 상황에서는 게임 점수(동전을 먹거나 적을 쓰러뜨리는 등)나 게임 종료 등의 명확한 지표로부터 역산해서 예상 보상을 정해야 한다. \n",
    "  - DQN\n",
    "    - Q라는 강화학습 알고리즘을 기초로 한다. Q학습에서는 최적 행동 가치 함수로 최적인 행동을 정한다. 이 함수를 딥러닝으로 비슷하게 흉내 내어 사용하는 것이 DQN이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d645a727-6cba-4894-aab6-3cb985e5588a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7890618-3489-4d38-ad41-087d856113e5",
   "metadata": {},
   "source": [
    "- 간단히 1회독이 종료됐다.\n",
    "- 2회독부터는 진도 속도를 늦추되 좀 더 정밀히 공부해보자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
