<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>드롭 아웃 &amp; 합성곱 신경망(CNN) | INTROdl</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="드롭 아웃 &amp; 합성곱 신경망(CNN)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<meta property="og:description" content="An easy to use blogging platform with support for Jupyter Notebooks." />
<link rel="canonical" href="https://rhkrehtjd.github.io/INTROdl/2022/02/18/dl.html" />
<meta property="og:url" content="https://rhkrehtjd.github.io/INTROdl/2022/02/18/dl.html" />
<meta property="og:site_name" content="INTROdl" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-18T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="드롭 아웃 &amp; 합성곱 신경망(CNN)" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-18T00:00:00-06:00","datePublished":"2022-02-18T00:00:00-06:00","description":"An easy to use blogging platform with support for Jupyter Notebooks.","headline":"드롭 아웃 &amp; 합성곱 신경망(CNN)","mainEntityOfPage":{"@type":"WebPage","@id":"https://rhkrehtjd.github.io/INTROdl/2022/02/18/dl.html"},"url":"https://rhkrehtjd.github.io/INTROdl/2022/02/18/dl.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/INTROdl/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://rhkrehtjd.github.io/INTROdl/feed.xml" title="INTROdl" /><link rel="shortcut icon" type="image/x-icon" href="/INTROdl/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/INTROdl/">INTROdl</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/INTROdl/about/">About Me</a><a class="page-link" href="/INTROdl/search/">Search</a><a class="page-link" href="/INTROdl/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">드롭 아웃 &amp; 합성곱 신경망(CNN)</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-18T00:00:00-06:00" itemprop="datePublished">
        Feb 18, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      28 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/rhkrehtjd/INTROdl/tree/master/_notebooks/2022-02-18-dl.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/INTROdl/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/rhkrehtjd/INTROdl/master?filepath=_notebooks%2F2022-02-18-dl.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/INTROdl/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/rhkrehtjd/INTROdl/blob/master/_notebooks/2022-02-18-dl.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/INTROdl/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Frhkrehtjd%2FINTROdl%2Fblob%2Fmaster%2F_notebooks%2F2022-02-18-dl.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/INTROdl/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-18-dl.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>오버피팅을 억제하는 방법으로 드롭아웃이라는 기법을 사용할 수 있다. <ul>
<li>뉴런을 임의로 삭제하면서 학습하는 방법이다. </li>
<li>훈련 때 뉴런을 무작위로 골라 삭제한다. 삭제된 뉴런은 신호를 전달하지 않는다. 훈련 때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고, 시험 때는 모든 뉴런에 신호를 전달한다. </li>
<li>단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다.</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Dropout</span> <span class="p">:</span> 
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout_ratio</span><span class="o">=.</span><span class="mi">5</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ratio</span> <span class="o">=</span> <span class="n">dropout_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="o">=</span><span class="kc">None</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">train_flg</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">train_flg</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ratio</span>
            <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span>
        <span class="k">else</span> <span class="p">:</span> <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_ratio</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dout</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">dout</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>여기에서의 핵심은 훈련 시에는 순전파 때마다 self.mask에 삭제할 뉴런을 False로 표시한다는 것이다. </li>
<li>self.mask는 x와 형상이 같은 배열을 무작위로 생성하고, 그 값이 dropout_ratio보다 큰 원소만 True로 설정한다.</li>
<li>역전파때의 동작은 ReLU와 같다. </li>
<li>드롭아웃의 효과를 MNIST 데이터셋으로 확인해보자</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">pardir</span><span class="p">)</span>  <span class="c1"># 부모 디렉터리의 파일을 가져올 수 있도록 설정</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">dataset.mnist</span> <span class="kn">import</span> <span class="n">load_mnist</span>
<span class="kn">from</span> <span class="nn">common.multi_layer_net_extend</span> <span class="kn">import</span> <span class="n">MultiLayerNetExtend</span>
<span class="kn">from</span> <span class="nn">common.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># 오버피팅을 재현하기 위해 학습 데이터 수를 줄임</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:</span><span class="mi">300</span><span class="p">]</span>
<span class="n">t_train</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[:</span><span class="mi">300</span><span class="p">]</span>

<span class="c1"># 드롭아웃 사용 유무와 비울 설정 ========================</span>
<span class="n">use_dropout</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># 드롭아웃을 쓰지 않을 때는 False</span>
<span class="n">dropout_ratio</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="c1"># ====================================================</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">MultiLayerNetExtend</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size_list</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
                              <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">use_dropout</span><span class="o">=</span><span class="n">use_dropout</span><span class="p">,</span> <span class="n">dropout_ration</span><span class="o">=</span><span class="n">dropout_ratio</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">,</span>
                  <span class="n">epochs</span><span class="o">=</span><span class="mi">301</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">optimizer_param</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">train_acc_list</span><span class="p">,</span> <span class="n">test_acc_list</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train_acc_list</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test_acc_list</span>

<span class="c1"># 그래프 그리기==========</span>
<span class="n">markers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="s1">&#39;s&#39;</span><span class="p">}</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_acc_list</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_acc_list</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">test_acc_list</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="n">markevery</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;epochs&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train loss:2.324100259095491
=== epoch:1, train acc:0.06, test acc:0.0519 ===
train loss:2.3367240344717546
train loss:2.3198828799791507
train loss:2.334138201403393
=== epoch:2, train acc:0.06333333333333334, test acc:0.0534 ===
train loss:2.368374415559161
train loss:2.3381363551504286
train loss:2.3330928399833835
=== epoch:3, train acc:0.06333333333333334, test acc:0.0548 ===
train loss:2.3385353626060423
train loss:2.321698027938764
train loss:2.342470764732769
=== epoch:4, train acc:0.06666666666666667, test acc:0.0566 ===
train loss:2.3181256091192863
train loss:2.318016527906595
train loss:2.3341585909597278
=== epoch:5, train acc:0.07, test acc:0.0585 ===
train loss:2.3223824145629153
train loss:2.3127880613389453
train loss:2.3268800220936066
=== epoch:6, train acc:0.07, test acc:0.0612 ===
train loss:2.3323298965987163
train loss:2.3165949954128062
train loss:2.3143363185934573
=== epoch:7, train acc:0.08333333333333333, test acc:0.0633 ===
train loss:2.315828117487278
train loss:2.333196760768538
train loss:2.306113270261319
=== epoch:8, train acc:0.08333333333333333, test acc:0.0665 ===
train loss:2.3230691151629
train loss:2.302295036765928
train loss:2.3050098518514877
=== epoch:9, train acc:0.09, test acc:0.0681 ===
train loss:2.302610372179809
train loss:2.3192645732440322
train loss:2.322518870537485
=== epoch:10, train acc:0.10666666666666667, test acc:0.0707 ===
train loss:2.307218021902985
train loss:2.3061724911770507
train loss:2.3010604532329983
=== epoch:11, train acc:0.11, test acc:0.0737 ===
train loss:2.30023057573994
train loss:2.3051623665362864
train loss:2.2938154369504575
=== epoch:12, train acc:0.11, test acc:0.0813 ===
train loss:2.3134678116607215
train loss:2.2945534263230702
train loss:2.3100487292414247
=== epoch:13, train acc:0.12333333333333334, test acc:0.0859 ===
train loss:2.3037582367612863
train loss:2.308003699202601
train loss:2.2988065268228475
=== epoch:14, train acc:0.12666666666666668, test acc:0.0923 ===
train loss:2.286301855338051
train loss:2.301356815456316
train loss:2.310309438466073
=== epoch:15, train acc:0.12666666666666668, test acc:0.0969 ===
train loss:2.2859136734966503
train loss:2.320434074887247
train loss:2.3022566690845334
=== epoch:16, train acc:0.13333333333333333, test acc:0.105 ===
train loss:2.303994382649848
train loss:2.2981563636787774
train loss:2.278755644740708
=== epoch:17, train acc:0.13666666666666666, test acc:0.1095 ===
train loss:2.292802302269951
train loss:2.2967010449612086
train loss:2.297434898716603
=== epoch:18, train acc:0.14, test acc:0.1122 ===
train loss:2.307140414078851
train loss:2.3112541114077234
train loss:2.2912007308676308
=== epoch:19, train acc:0.14666666666666667, test acc:0.1173 ===
train loss:2.305011525489231
train loss:2.2933798585530156
train loss:2.2842759863784563
=== epoch:20, train acc:0.14666666666666667, test acc:0.1212 ===
train loss:2.294419306615848
train loss:2.29882546005831
train loss:2.30492862773529
=== epoch:21, train acc:0.15, test acc:0.1253 ===
train loss:2.3081045476194086
train loss:2.283894393780756
train loss:2.291670885938608
=== epoch:22, train acc:0.15666666666666668, test acc:0.1295 ===
train loss:2.2890985760190974
train loss:2.2840575010962896
train loss:2.2822402773763595
=== epoch:23, train acc:0.16, test acc:0.1336 ===
train loss:2.271743863077227
train loss:2.280909345191916
train loss:2.2788580115653647
=== epoch:24, train acc:0.15333333333333332, test acc:0.1396 ===
train loss:2.2962624753864516
train loss:2.2857594410543243
train loss:2.2734956258291743
=== epoch:25, train acc:0.16, test acc:0.1433 ===
train loss:2.281196733898885
train loss:2.2855165343491333
train loss:2.272800202200122
=== epoch:26, train acc:0.17, test acc:0.146 ===
train loss:2.2710752373125755
train loss:2.2708058215894718
train loss:2.281390365009942
=== epoch:27, train acc:0.17666666666666667, test acc:0.149 ===
train loss:2.276096368479296
train loss:2.2868398592049792
train loss:2.2533021288181128
=== epoch:28, train acc:0.18, test acc:0.1531 ===
train loss:2.279790935445175
train loss:2.290175069849667
train loss:2.28755381923219
=== epoch:29, train acc:0.18666666666666668, test acc:0.1584 ===
train loss:2.268625340428652
train loss:2.271549787381626
train loss:2.277468101452462
=== epoch:30, train acc:0.20666666666666667, test acc:0.1625 ===
train loss:2.274422617681268
train loss:2.27950797654735
train loss:2.2595019502888984
=== epoch:31, train acc:0.21, test acc:0.1682 ===
train loss:2.2547736778519254
train loss:2.2955682215285855
train loss:2.281911820002909
=== epoch:32, train acc:0.21, test acc:0.1706 ===
train loss:2.2849632248118397
train loss:2.270001897878741
train loss:2.2721893742474184
=== epoch:33, train acc:0.20666666666666667, test acc:0.1744 ===
train loss:2.2702002840967537
train loss:2.2636977488932697
train loss:2.2457445209173006
=== epoch:34, train acc:0.21333333333333335, test acc:0.1782 ===
train loss:2.282174901289564
train loss:2.2719907929706573
train loss:2.278374323648339
=== epoch:35, train acc:0.21333333333333335, test acc:0.1822 ===
train loss:2.2780296393134916
train loss:2.260936883419436
train loss:2.2791379144450414
=== epoch:36, train acc:0.21333333333333335, test acc:0.1847 ===
train loss:2.2682905978520376
train loss:2.244083967617716
train loss:2.284818421389002
=== epoch:37, train acc:0.22666666666666666, test acc:0.1884 ===
train loss:2.2703278778363924
train loss:2.27787084368482
train loss:2.264595328350379
=== epoch:38, train acc:0.23, test acc:0.192 ===
train loss:2.2564862825603043
train loss:2.248164370992879
train loss:2.2527666356919998
=== epoch:39, train acc:0.23333333333333334, test acc:0.1943 ===
train loss:2.2527864764123833
train loss:2.2609731773162323
train loss:2.2690263147617125
=== epoch:40, train acc:0.23333333333333334, test acc:0.1952 ===
train loss:2.2728311806303987
train loss:2.2538989367626523
train loss:2.2480245940540566
=== epoch:41, train acc:0.23333333333333334, test acc:0.1975 ===
train loss:2.265938876735138
train loss:2.2582592056648663
train loss:2.2479282408121732
=== epoch:42, train acc:0.23333333333333334, test acc:0.1964 ===
train loss:2.266068436358752
train loss:2.2576908657265338
train loss:2.274389230169998
=== epoch:43, train acc:0.23333333333333334, test acc:0.1976 ===
train loss:2.2699133277976635
train loss:2.2822305782291945
train loss:2.2583668340850913
=== epoch:44, train acc:0.24, test acc:0.2027 ===
train loss:2.2614999803016334
train loss:2.2715128262870534
train loss:2.2588317789592836
=== epoch:45, train acc:0.25333333333333335, test acc:0.2062 ===
train loss:2.2638318019294044
train loss:2.2537424854780173
train loss:2.2731680110310455
=== epoch:46, train acc:0.25333333333333335, test acc:0.2067 ===
train loss:2.2671178394983924
train loss:2.2449131639062205
train loss:2.2437541809558295
=== epoch:47, train acc:0.25666666666666665, test acc:0.2074 ===
train loss:2.263695618550946
train loss:2.244976087974502
train loss:2.2411468468562883
=== epoch:48, train acc:0.25333333333333335, test acc:0.2082 ===
train loss:2.26033814844771
train loss:2.242678644044038
train loss:2.248558420132348
=== epoch:49, train acc:0.25, test acc:0.2068 ===
train loss:2.2207890938560095
train loss:2.2312159532346563
train loss:2.2579461674859767
=== epoch:50, train acc:0.25333333333333335, test acc:0.2077 ===
train loss:2.2477645504298174
train loss:2.2476478990710826
train loss:2.263757677075739
=== epoch:51, train acc:0.25, test acc:0.2086 ===
train loss:2.2348103691313668
train loss:2.2545860567012075
train loss:2.255671015156717
=== epoch:52, train acc:0.25666666666666665, test acc:0.2096 ===
train loss:2.2441162497387506
train loss:2.255375576433403
train loss:2.2346755166153973
=== epoch:53, train acc:0.26, test acc:0.2108 ===
train loss:2.2570070909264732
train loss:2.2245861219591716
train loss:2.2565773725303098
=== epoch:54, train acc:0.26, test acc:0.2117 ===
train loss:2.226096569672674
train loss:2.2218325076971666
train loss:2.227156394991192
=== epoch:55, train acc:0.25666666666666665, test acc:0.2105 ===
train loss:2.2244379733138535
train loss:2.2325094973723605
train loss:2.241027209234398
=== epoch:56, train acc:0.25666666666666665, test acc:0.2097 ===
train loss:2.260287003375664
train loss:2.229487178465668
train loss:2.220148957477374
=== epoch:57, train acc:0.25666666666666665, test acc:0.2094 ===
train loss:2.239870454972404
train loss:2.235996812833804
train loss:2.229316333709944
=== epoch:58, train acc:0.25666666666666665, test acc:0.2091 ===
train loss:2.233170338009764
train loss:2.259041256588281
train loss:2.2487296817763007
=== epoch:59, train acc:0.26, test acc:0.2097 ===
train loss:2.228170631739541
train loss:2.2589295915082332
train loss:2.244326933571112
=== epoch:60, train acc:0.2633333333333333, test acc:0.2126 ===
train loss:2.2364859471415284
train loss:2.2225963595525378
train loss:2.2464349094017746
=== epoch:61, train acc:0.2633333333333333, test acc:0.2136 ===
train loss:2.2360741988667225
train loss:2.2176610983003044
train loss:2.2504923570537336
=== epoch:62, train acc:0.25666666666666665, test acc:0.2161 ===
train loss:2.2377980569671685
train loss:2.2149168805210655
train loss:2.2266473653024543
=== epoch:63, train acc:0.26, test acc:0.2161 ===
train loss:2.257205434155905
train loss:2.23834838368647
train loss:2.2165933792455035
=== epoch:64, train acc:0.26, test acc:0.2179 ===
train loss:2.21724329659052
train loss:2.212005481209344
train loss:2.2086091616077375
=== epoch:65, train acc:0.26, test acc:0.2154 ===
train loss:2.19237124426394
train loss:2.1841162031880064
train loss:2.225928661918044
=== epoch:66, train acc:0.25333333333333335, test acc:0.2138 ===
train loss:2.2489977310895224
train loss:2.1993606346466934
train loss:2.2489438258992642
=== epoch:67, train acc:0.25333333333333335, test acc:0.2148 ===
train loss:2.2274704687524407
train loss:2.2267277709549926
train loss:2.2124939673748223
=== epoch:68, train acc:0.25666666666666665, test acc:0.2154 ===
train loss:2.21276779651636
train loss:2.223876646024247
train loss:2.2140444856538646
=== epoch:69, train acc:0.25666666666666665, test acc:0.2163 ===
train loss:2.1904753882346104
train loss:2.2475553205352745
train loss:2.1829183218013553
=== epoch:70, train acc:0.25666666666666665, test acc:0.2167 ===
train loss:2.221431769059627
train loss:2.239988305199648
train loss:2.1887003426742577
=== epoch:71, train acc:0.25333333333333335, test acc:0.2168 ===
train loss:2.2314761993500203
train loss:2.235210919162627
train loss:2.194756686548659
=== epoch:72, train acc:0.25666666666666665, test acc:0.2173 ===
train loss:2.2058456540427045
train loss:2.2334618712692835
train loss:2.1839681190162086
=== epoch:73, train acc:0.27, test acc:0.2188 ===
train loss:2.2102695288167293
train loss:2.168932264118608
train loss:2.2454311199247416
=== epoch:74, train acc:0.2633333333333333, test acc:0.2189 ===
train loss:2.2210089627153304
train loss:2.2127253844490204
train loss:2.187347815206704
=== epoch:75, train acc:0.2633333333333333, test acc:0.2196 ===
train loss:2.209102544892577
train loss:2.1826570566323915
train loss:2.2309548372006645
=== epoch:76, train acc:0.27, test acc:0.2198 ===
train loss:2.1992447116750014
train loss:2.180957522375729
train loss:2.218867388209114
=== epoch:77, train acc:0.26666666666666666, test acc:0.2198 ===
train loss:2.1675144642464685
train loss:2.2068994329399634
train loss:2.2018092638357847
=== epoch:78, train acc:0.2633333333333333, test acc:0.2188 ===
train loss:2.1847385704055253
train loss:2.180533813260938
train loss:2.1697845427004205
=== epoch:79, train acc:0.27, test acc:0.2184 ===
train loss:2.2072852284818087
train loss:2.1970507122089566
train loss:2.1797600512638184
=== epoch:80, train acc:0.2633333333333333, test acc:0.2185 ===
train loss:2.2161420395002245
train loss:2.2094001411863213
train loss:2.2460413384658
=== epoch:81, train acc:0.2633333333333333, test acc:0.2201 ===
train loss:2.1806652633553365
train loss:2.198152397792485
train loss:2.1607521011605084
=== epoch:82, train acc:0.2633333333333333, test acc:0.2188 ===
train loss:2.1807034162148136
train loss:2.2130427621985707
train loss:2.154109095438752
=== epoch:83, train acc:0.26, test acc:0.2188 ===
train loss:2.189663737590424
train loss:2.155494630287265
train loss:2.2159501574240745
=== epoch:84, train acc:0.26, test acc:0.2198 ===
train loss:2.162539240008073
train loss:2.195543512092732
train loss:2.1853185353311493
=== epoch:85, train acc:0.2633333333333333, test acc:0.2208 ===
train loss:2.1600057055782638
train loss:2.1818113822289478
train loss:2.1328014536694466
=== epoch:86, train acc:0.2633333333333333, test acc:0.2193 ===
train loss:2.1887660681677885
train loss:2.1884111205495524
train loss:2.193076194970264
=== epoch:87, train acc:0.26, test acc:0.2204 ===
train loss:2.152790054758269
train loss:2.187410599046354
train loss:2.1901313150457593
=== epoch:88, train acc:0.2633333333333333, test acc:0.2205 ===
train loss:2.167731209783988
train loss:2.1935874284839825
train loss:2.1537553190642615
=== epoch:89, train acc:0.26, test acc:0.2211 ===
train loss:2.150969900292231
train loss:2.193534128106561
train loss:2.161949285578587
=== epoch:90, train acc:0.2633333333333333, test acc:0.2218 ===
train loss:2.163776864663713
train loss:2.187102902202226
train loss:2.136568041162818
=== epoch:91, train acc:0.26666666666666666, test acc:0.2227 ===
train loss:2.149692629425911
train loss:2.1352815933093523
train loss:2.1981598394195743
=== epoch:92, train acc:0.2733333333333333, test acc:0.2232 ===
train loss:2.144301004755044
train loss:2.2054131763238263
train loss:2.1654375496545053
=== epoch:93, train acc:0.2733333333333333, test acc:0.2233 ===
train loss:2.1408959125166023
train loss:2.2156218333940774
train loss:2.1763032139023584
=== epoch:94, train acc:0.28, test acc:0.224 ===
train loss:2.147698204758112
train loss:2.183318985046108
train loss:2.118187234473891
=== epoch:95, train acc:0.28, test acc:0.2251 ===
train loss:2.1444901122248985
train loss:2.1933666832314045
train loss:2.124085736784169
=== epoch:96, train acc:0.2833333333333333, test acc:0.2252 ===
train loss:2.152872063824104
train loss:2.1633217497260144
train loss:2.173299458971835
=== epoch:97, train acc:0.2833333333333333, test acc:0.2261 ===
train loss:2.1060404749821835
train loss:2.1602194770159246
train loss:2.1683339156636467
=== epoch:98, train acc:0.27666666666666667, test acc:0.2262 ===
train loss:2.1394693562006832
train loss:2.1734312638358966
train loss:2.1380837001883064
=== epoch:99, train acc:0.2833333333333333, test acc:0.227 ===
train loss:2.155884162173334
train loss:2.1427548765428406
train loss:2.0945199304591653
=== epoch:100, train acc:0.27666666666666667, test acc:0.2262 ===
train loss:2.1313715872888093
train loss:2.1608023802178153
train loss:2.075899404413534
=== epoch:101, train acc:0.27, test acc:0.2252 ===
train loss:2.1304498390585507
train loss:2.0392660263772164
train loss:2.0775353393252787
=== epoch:102, train acc:0.27, test acc:0.2238 ===
train loss:2.063112995513289
train loss:2.140493741714319
train loss:2.1104146611017107
=== epoch:103, train acc:0.2733333333333333, test acc:0.2236 ===
train loss:2.1176756524604308
train loss:2.177458480517272
train loss:2.0863229827353367
=== epoch:104, train acc:0.2833333333333333, test acc:0.2248 ===
train loss:2.0859330117226564
train loss:2.122782503716861
train loss:2.0864938964362127
=== epoch:105, train acc:0.27666666666666667, test acc:0.2247 ===
train loss:2.081051745566688
train loss:2.1361028681037437
train loss:2.22147822839819
=== epoch:106, train acc:0.27666666666666667, test acc:0.2256 ===
train loss:2.0015204448745227
train loss:2.1104614728950937
train loss:2.113990270074682
=== epoch:107, train acc:0.27666666666666667, test acc:0.2249 ===
train loss:2.065367547469283
train loss:2.1381304178186964
train loss:2.0455887997103117
=== epoch:108, train acc:0.28, test acc:0.2251 ===
train loss:2.06493140690479
train loss:2.0508167423796717
train loss:2.1529074587919537
=== epoch:109, train acc:0.28, test acc:0.2255 ===
train loss:2.1667413655310095
train loss:2.103452449869255
train loss:2.0790366549237493
=== epoch:110, train acc:0.2833333333333333, test acc:0.2263 ===
train loss:2.1774945792404194
train loss:2.0385256422624334
train loss:2.0629139150764084
=== epoch:111, train acc:0.2833333333333333, test acc:0.2266 ===
train loss:2.074240722121803
train loss:2.1010259158418596
train loss:2.0074007529022735
=== epoch:112, train acc:0.2866666666666667, test acc:0.2265 ===
train loss:2.1528450740157044
train loss:2.08511780682749
train loss:2.0901948848345926
=== epoch:113, train acc:0.2866666666666667, test acc:0.2278 ===
train loss:2.166109902076436
train loss:2.09336347639171
train loss:2.1613331299208287
=== epoch:114, train acc:0.29, test acc:0.2308 ===
train loss:2.002040289162185
train loss:2.097397864386531
train loss:2.098147022018419
=== epoch:115, train acc:0.2966666666666667, test acc:0.2318 ===
train loss:2.0381972726258972
train loss:2.15172476114923
train loss:2.1027479429246907
=== epoch:116, train acc:0.2966666666666667, test acc:0.2334 ===
train loss:2.07739882818642
train loss:2.135866705591904
train loss:2.1189283073357306
=== epoch:117, train acc:0.29, test acc:0.2338 ===
train loss:2.1116926320733493
train loss:2.099161725550368
train loss:2.0873500758779087
=== epoch:118, train acc:0.29333333333333333, test acc:0.2351 ===
train loss:2.099495182896229
train loss:2.0126648327487664
train loss:2.0601573227547645
=== epoch:119, train acc:0.29, test acc:0.2351 ===
train loss:2.074034520229973
train loss:2.1378415870297904
train loss:2.1141976674230287
=== epoch:120, train acc:0.2966666666666667, test acc:0.2383 ===
train loss:2.1194542249195916
train loss:2.0985911423683614
train loss:2.122046816594318
=== epoch:121, train acc:0.30666666666666664, test acc:0.2393 ===
train loss:2.079052667678309
train loss:2.046963236671442
train loss:2.1335675274216577
=== epoch:122, train acc:0.31, test acc:0.2421 ===
train loss:2.127066274108861
train loss:2.100447609300942
train loss:2.098331735511033
=== epoch:123, train acc:0.31, test acc:0.244 ===
train loss:2.0469081741676955
train loss:2.1039289102085332
train loss:1.9575786135807827
=== epoch:124, train acc:0.31, test acc:0.243 ===
train loss:1.97676936296635
train loss:2.056448119174276
train loss:2.070794672709109
=== epoch:125, train acc:0.31, test acc:0.2439 ===
train loss:1.9718699908195887
train loss:2.1200802686016287
train loss:2.014578938068621
=== epoch:126, train acc:0.31, test acc:0.2446 ===
train loss:1.9547181661505006
train loss:2.0686912310767838
train loss:2.1844177786643537
=== epoch:127, train acc:0.30666666666666664, test acc:0.2438 ===
train loss:1.9812634982241502
train loss:2.15078634646344
train loss:2.0881807215005583
=== epoch:128, train acc:0.31, test acc:0.2438 ===
train loss:2.034935910585923
train loss:2.0164586052624873
train loss:2.112468141598271
=== epoch:129, train acc:0.31, test acc:0.2449 ===
train loss:2.0907644950072735
train loss:1.9975399152081927
train loss:2.0736024599035736
=== epoch:130, train acc:0.31, test acc:0.2464 ===
train loss:1.9387292142127248
train loss:2.041868755617978
train loss:2.046324630042435
=== epoch:131, train acc:0.31, test acc:0.2473 ===
train loss:1.9727510993177229
train loss:2.043531455268269
train loss:2.0240402732367677
=== epoch:132, train acc:0.31, test acc:0.2458 ===
train loss:2.04070453818416
train loss:2.079140562625451
train loss:2.021468476008009
=== epoch:133, train acc:0.31666666666666665, test acc:0.2476 ===
train loss:1.935517946457607
train loss:2.051982446203123
train loss:2.100136613442308
=== epoch:134, train acc:0.31333333333333335, test acc:0.2495 ===
train loss:1.9448120641057778
train loss:2.0112830470003633
train loss:2.057390431157715
=== epoch:135, train acc:0.31333333333333335, test acc:0.2484 ===
train loss:2.0341794582941435
train loss:2.025210458234844
train loss:2.0281735103776906
=== epoch:136, train acc:0.3233333333333333, test acc:0.2488 ===
train loss:1.969998278174036
train loss:2.0030721293636775
train loss:1.876649732732676
=== epoch:137, train acc:0.31666666666666665, test acc:0.2479 ===
train loss:2.023792049309528
train loss:2.099909568899363
train loss:2.0489493873325912
=== epoch:138, train acc:0.31666666666666665, test acc:0.2477 ===
train loss:2.045474970547569
train loss:1.9696447491452977
train loss:1.9603566304596434
=== epoch:139, train acc:0.31666666666666665, test acc:0.2479 ===
train loss:1.9298739300918757
train loss:2.028965973441627
train loss:1.9836225923680686
=== epoch:140, train acc:0.31666666666666665, test acc:0.2485 ===
train loss:2.0368180039384147
train loss:2.0666601418106563
train loss:2.052390861421367
=== epoch:141, train acc:0.31666666666666665, test acc:0.2503 ===
train loss:2.039428538652129
train loss:2.1011665299140505
train loss:1.9527534298844116
=== epoch:142, train acc:0.33, test acc:0.2534 ===
train loss:1.9917154522367349
train loss:1.989418501654413
train loss:2.0873160884058253
=== epoch:143, train acc:0.33666666666666667, test acc:0.2564 ===
train loss:2.063290643753276
train loss:1.9756256132963648
train loss:1.9965410166665116
=== epoch:144, train acc:0.3433333333333333, test acc:0.2585 ===
train loss:1.9643850841753834
train loss:2.0175293728607238
train loss:2.0103351770632343
=== epoch:145, train acc:0.34, test acc:0.2592 ===
train loss:2.1078657793706843
train loss:1.8411574963386443
train loss:1.922042281137775
=== epoch:146, train acc:0.34, test acc:0.261 ===
train loss:2.0825775896245937
train loss:2.126813271397813
train loss:1.9528955552909528
=== epoch:147, train acc:0.34, test acc:0.2611 ===
train loss:2.03098018321778
train loss:1.9632901055441285
train loss:1.862457814085648
=== epoch:148, train acc:0.34, test acc:0.2609 ===
train loss:1.9685806847460605
train loss:1.923425037316148
train loss:1.957268510494084
=== epoch:149, train acc:0.34, test acc:0.2611 ===
train loss:2.043878630017103
train loss:1.9744954960327386
train loss:1.9848518415320993
=== epoch:150, train acc:0.34, test acc:0.2622 ===
train loss:2.051841403997525
train loss:2.011274044477478
train loss:1.8818355118043169
=== epoch:151, train acc:0.34, test acc:0.261 ===
train loss:2.0560602621623874
train loss:2.004623926602401
train loss:1.8932646503316422
=== epoch:152, train acc:0.33666666666666667, test acc:0.2601 ===
train loss:1.9834598038826332
train loss:2.0182768513882636
train loss:1.9986896365191595
=== epoch:153, train acc:0.3333333333333333, test acc:0.2632 ===
train loss:1.969720678927994
train loss:1.9445517208049543
train loss:1.9277266381018017
=== epoch:154, train acc:0.34, test acc:0.2638 ===
train loss:2.0774724473669233
train loss:1.9641365271450433
train loss:1.9957655126585268
=== epoch:155, train acc:0.34, test acc:0.2662 ===
train loss:1.9375892922085827
train loss:2.010059318934352
train loss:1.9364413587878533
=== epoch:156, train acc:0.33666666666666667, test acc:0.2649 ===
train loss:1.9068950820392507
train loss:2.0026559727655995
train loss:2.0502203811321165
=== epoch:157, train acc:0.33, test acc:0.2662 ===
train loss:1.8170210330517087
train loss:1.9443773176379884
train loss:1.919411704245374
=== epoch:158, train acc:0.33666666666666667, test acc:0.2669 ===
train loss:1.920649279768768
train loss:1.9184216373899974
train loss:2.024490190710448
=== epoch:159, train acc:0.34, test acc:0.2678 ===
train loss:1.951044152023751
train loss:2.0249130007591716
train loss:1.9660162515312378
=== epoch:160, train acc:0.3466666666666667, test acc:0.2711 ===
train loss:1.8972348726775836
train loss:1.990833810475007
train loss:1.8555055491948909
=== epoch:161, train acc:0.3433333333333333, test acc:0.2706 ===
train loss:2.0225194549776826
train loss:1.9440140220076108
train loss:1.9980009538684869
=== epoch:162, train acc:0.3466666666666667, test acc:0.2708 ===
train loss:1.9254272086466748
train loss:2.0307065173792918
train loss:2.008381596962002
=== epoch:163, train acc:0.3466666666666667, test acc:0.2735 ===
train loss:2.016539412022592
train loss:1.9956689777990155
train loss:1.984170093366145
=== epoch:164, train acc:0.3466666666666667, test acc:0.2751 ===
train loss:1.9411797704450244
train loss:1.96764408593041
train loss:2.048926673388958
=== epoch:165, train acc:0.35, test acc:0.2766 ===
train loss:1.9499846242655454
train loss:1.9130460257802702
train loss:1.9710619234028592
=== epoch:166, train acc:0.35, test acc:0.2763 ===
train loss:1.9043802241945065
train loss:1.9583706004181962
train loss:1.9031245195243744
=== epoch:167, train acc:0.3466666666666667, test acc:0.2758 ===
train loss:1.9062347210662154
train loss:1.9493844956250794
train loss:1.9071355394175773
=== epoch:168, train acc:0.3466666666666667, test acc:0.2768 ===
train loss:2.0240281957036705
train loss:1.9561819635947373
train loss:1.8399586607735658
=== epoch:169, train acc:0.35, test acc:0.2795 ===
train loss:1.9896012927631008
train loss:1.830988595568251
train loss:1.9382594910162485
=== epoch:170, train acc:0.35, test acc:0.2793 ===
train loss:2.012869529899594
train loss:1.9722668865049164
train loss:1.84372392981833
=== epoch:171, train acc:0.35, test acc:0.2799 ===
train loss:1.9659629510542826
train loss:1.8762033576907549
train loss:1.9110343407405472
=== epoch:172, train acc:0.3466666666666667, test acc:0.2791 ===
train loss:1.9935599130037718
train loss:1.9677044010921336
train loss:1.9570530536060875
=== epoch:173, train acc:0.3466666666666667, test acc:0.28 ===
train loss:1.9050336236965477
train loss:1.8893146808603987
train loss:1.937248951207443
=== epoch:174, train acc:0.3566666666666667, test acc:0.2812 ===
train loss:2.0129007687434908
train loss:1.8507440880477242
train loss:1.9172139358070337
=== epoch:175, train acc:0.3566666666666667, test acc:0.2821 ===
train loss:1.8611484880791236
train loss:1.9842011083211926
train loss:1.9702341877582854
=== epoch:176, train acc:0.3566666666666667, test acc:0.2831 ===
train loss:1.9505687961453058
train loss:1.8790608865088543
train loss:1.9202845450752322
=== epoch:177, train acc:0.36, test acc:0.2855 ===
train loss:1.9166693275231856
train loss:1.8575061650223998
train loss:1.8711489303742994
=== epoch:178, train acc:0.36, test acc:0.2837 ===
train loss:1.8463088943000472
train loss:1.8510470957129552
train loss:1.8779748255987232
=== epoch:179, train acc:0.37333333333333335, test acc:0.2847 ===
train loss:1.8907847935715134
train loss:1.868600067831465
train loss:1.970452296562895
=== epoch:180, train acc:0.38, test acc:0.2861 ===
train loss:1.8674366822881316
train loss:1.921144585535963
train loss:1.8807798815014383
=== epoch:181, train acc:0.37666666666666665, test acc:0.2867 ===
train loss:1.8874599954257691
train loss:1.8717509249621196
train loss:1.8045921844238015
=== epoch:182, train acc:0.36, test acc:0.2877 ===
train loss:1.8994162210398318
train loss:1.8791350420075872
train loss:1.8480320991371042
=== epoch:183, train acc:0.36, test acc:0.2879 ===
train loss:1.799536452818469
train loss:1.89795201733964
train loss:1.7648443495581463
=== epoch:184, train acc:0.36666666666666664, test acc:0.2887 ===
train loss:1.835462161742262
train loss:1.8294867440417215
train loss:1.8984998467611496
=== epoch:185, train acc:0.36, test acc:0.2867 ===
train loss:1.8943726695842509
train loss:1.8173568580152093
train loss:1.8470304534702426
=== epoch:186, train acc:0.36, test acc:0.2859 ===
train loss:2.0573224500330896
train loss:1.8266063368945584
train loss:1.841408771074559
=== epoch:187, train acc:0.36, test acc:0.2853 ===
train loss:1.7540531751483739
train loss:1.8568064584595232
train loss:1.8809472035617443
=== epoch:188, train acc:0.36333333333333334, test acc:0.2877 ===
train loss:1.9003421485644183
train loss:1.8108746974278713
train loss:1.9389650756589378
=== epoch:189, train acc:0.37, test acc:0.2886 ===
train loss:1.7805326809540967
train loss:1.8933837060774965
train loss:1.7942781465820434
=== epoch:190, train acc:0.36666666666666664, test acc:0.288 ===
train loss:1.9647016076531691
train loss:1.825778087500078
train loss:1.9638159705018725
=== epoch:191, train acc:0.37, test acc:0.2924 ===
train loss:1.8156817237376737
train loss:1.7638653091807088
train loss:1.8116257598243088
=== epoch:192, train acc:0.37, test acc:0.2937 ===
train loss:1.7049855380350447
train loss:1.9221692803534238
train loss:1.9353170293217807
=== epoch:193, train acc:0.37333333333333335, test acc:0.2957 ===
train loss:1.863317876700271
train loss:1.8838389028825713
train loss:1.7842611297470667
=== epoch:194, train acc:0.37666666666666665, test acc:0.2967 ===
train loss:1.9797408999485304
train loss:1.7568834758081495
train loss:1.7691509836117805
=== epoch:195, train acc:0.37333333333333335, test acc:0.299 ===
train loss:1.8256000732358004
train loss:1.816912173124073
train loss:1.8375593559073076
=== epoch:196, train acc:0.38333333333333336, test acc:0.3003 ===
train loss:1.714850077618391
train loss:1.832786185623573
train loss:1.7963584029557291
=== epoch:197, train acc:0.38, test acc:0.3004 ===
train loss:1.8169433828888797
train loss:1.790845683502479
train loss:1.8686575642600989
=== epoch:198, train acc:0.38333333333333336, test acc:0.3016 ===
train loss:1.8808915858682178
train loss:1.7707733025649628
train loss:1.6997953886974657
=== epoch:199, train acc:0.37666666666666665, test acc:0.3022 ===
train loss:1.78716741442463
train loss:1.860503622547692
train loss:1.7431574547906379
=== epoch:200, train acc:0.37333333333333335, test acc:0.3018 ===
train loss:1.805818981800991
train loss:1.712304758575884
train loss:1.8846704156094787
=== epoch:201, train acc:0.38, test acc:0.3032 ===
train loss:1.931746713962629
train loss:1.9230235577928536
train loss:1.9097240637508301
=== epoch:202, train acc:0.38, test acc:0.3053 ===
train loss:1.784183568048063
train loss:1.7856396363598708
train loss:1.887761601085864
=== epoch:203, train acc:0.38333333333333336, test acc:0.3069 ===
train loss:1.7529482498255589
train loss:1.7266563268134882
train loss:1.830276740349573
=== epoch:204, train acc:0.3933333333333333, test acc:0.3082 ===
train loss:1.7488745909759187
train loss:1.7016843469482184
train loss:1.796807819614164
=== epoch:205, train acc:0.39666666666666667, test acc:0.3097 ===
train loss:1.760797631408918
train loss:1.66664928355705
train loss:1.8840441966964843
=== epoch:206, train acc:0.38333333333333336, test acc:0.3067 ===
train loss:1.7651570754348782
train loss:1.7773472656666778
train loss:1.9343544437633116
=== epoch:207, train acc:0.3933333333333333, test acc:0.3074 ===
train loss:1.8054348882195037
train loss:1.7868764714646281
train loss:1.8198119532542705
=== epoch:208, train acc:0.4, test acc:0.3113 ===
train loss:1.7203671129854226
train loss:1.980053568475915
train loss:1.7555365491073436
=== epoch:209, train acc:0.41333333333333333, test acc:0.3133 ===
train loss:1.8795404255051842
train loss:1.7558985903076008
train loss:1.7365088596142109
=== epoch:210, train acc:0.4166666666666667, test acc:0.3165 ===
train loss:1.8299119620563624
train loss:1.8166757523514045
train loss:1.8855081645007779
=== epoch:211, train acc:0.41333333333333333, test acc:0.3175 ===
train loss:1.842641975544713
train loss:1.8022630591956763
train loss:1.838034928431486
=== epoch:212, train acc:0.4166666666666667, test acc:0.319 ===
train loss:1.8013702436381749
train loss:1.6346987728243247
train loss:1.7000235058385857
=== epoch:213, train acc:0.42, test acc:0.3217 ===
train loss:1.8566781438456335
train loss:1.7041221906697357
train loss:1.7625717544995407
=== epoch:214, train acc:0.4166666666666667, test acc:0.3225 ===
train loss:1.746800230106021
train loss:1.8154755450924531
train loss:1.7999778863489149
=== epoch:215, train acc:0.41, test acc:0.3229 ===
train loss:1.8232044704176373
train loss:1.81867070440437
train loss:1.6592537469851931
=== epoch:216, train acc:0.41333333333333333, test acc:0.3253 ===
train loss:1.7224332593622613
train loss:1.797001090942961
train loss:1.723563256648315
=== epoch:217, train acc:0.4166666666666667, test acc:0.3262 ===
train loss:1.9103900455828677
train loss:1.6783983670133293
train loss:1.8660601099916494
=== epoch:218, train acc:0.43, test acc:0.3308 ===
train loss:1.7275653044742054
train loss:1.6787546585470248
train loss:1.7668240229284908
=== epoch:219, train acc:0.42333333333333334, test acc:0.331 ===
train loss:1.7340448051992294
train loss:1.6732536457113159
train loss:1.6583273166418457
=== epoch:220, train acc:0.4266666666666667, test acc:0.3328 ===
train loss:1.7574452942846839
train loss:1.6890455031675053
train loss:1.7561452273014115
=== epoch:221, train acc:0.42, test acc:0.3306 ===
train loss:1.7091489149516605
train loss:1.6866254825427336
train loss:1.7631917968868753
=== epoch:222, train acc:0.4266666666666667, test acc:0.3358 ===
train loss:1.6456125319685995
train loss:1.766322712191973
train loss:1.7357799175172948
=== epoch:223, train acc:0.4266666666666667, test acc:0.3377 ===
train loss:1.8189521838869198
train loss:1.706825597566599
train loss:1.7142021038678188
=== epoch:224, train acc:0.43666666666666665, test acc:0.3394 ===
train loss:1.7475017778124853
train loss:1.7613123462262328
train loss:1.7742829620636533
=== epoch:225, train acc:0.43333333333333335, test acc:0.3391 ===
train loss:1.7792912879053078
train loss:1.54109739289766
train loss:1.789763413491916
=== epoch:226, train acc:0.4266666666666667, test acc:0.3384 ===
train loss:1.7689114172523093
train loss:1.7038899744571572
train loss:1.6860352836209762
=== epoch:227, train acc:0.43, test acc:0.3418 ===
train loss:1.6843603791622135
train loss:1.6828303320503832
train loss:1.6773643324580283
=== epoch:228, train acc:0.43666666666666665, test acc:0.344 ===
train loss:1.7375537315323513
train loss:1.7115442172649833
train loss:1.6823257645203253
=== epoch:229, train acc:0.44333333333333336, test acc:0.3468 ===
train loss:1.843718088671171
train loss:1.6469234788670655
train loss:1.6492895705839388
=== epoch:230, train acc:0.45666666666666667, test acc:0.3527 ===
train loss:1.7979535354936311
train loss:1.639535393034727
train loss:1.7621136440933869
=== epoch:231, train acc:0.46, test acc:0.3559 ===
train loss:1.7799639643091836
train loss:1.7802017170697684
train loss:1.7593057842668214
=== epoch:232, train acc:0.4666666666666667, test acc:0.3584 ===
train loss:1.574712912153641
train loss:1.6998409956827003
train loss:1.5989378771112237
=== epoch:233, train acc:0.47333333333333333, test acc:0.3618 ===
train loss:1.785811848615141
train loss:1.534281178675655
train loss:1.5198541200634095
=== epoch:234, train acc:0.48333333333333334, test acc:0.3648 ===
train loss:1.5826778990141586
train loss:1.5849035499112525
train loss:1.662817350558833
=== epoch:235, train acc:0.4766666666666667, test acc:0.3577 ===
train loss:1.7177112689108682
train loss:1.5539165013980303
train loss:1.5876071368935067
=== epoch:236, train acc:0.4766666666666667, test acc:0.3558 ===
train loss:1.7396575046323906
train loss:1.565026242722249
train loss:1.6866317624746034
=== epoch:237, train acc:0.4666666666666667, test acc:0.3556 ===
train loss:1.689891289756031
train loss:1.599954018256681
train loss:1.634418019922859
=== epoch:238, train acc:0.47, test acc:0.3575 ===
train loss:1.7076292366605177
train loss:1.6758439224153856
train loss:1.6618571259585841
=== epoch:239, train acc:0.4666666666666667, test acc:0.3607 ===
train loss:1.63863858191875
train loss:1.779640743257408
train loss:1.6473005473806908
=== epoch:240, train acc:0.47, test acc:0.3621 ===
train loss:1.6414777123726347
train loss:1.5745546648368378
train loss:1.6312962380357132
=== epoch:241, train acc:0.4766666666666667, test acc:0.3677 ===
train loss:1.566572388396348
train loss:1.5934073557487431
train loss:1.628062324786025
=== epoch:242, train acc:0.48, test acc:0.3649 ===
train loss:1.711320766568537
train loss:1.5786321671021764
train loss:1.7497772232645865
=== epoch:243, train acc:0.4766666666666667, test acc:0.3676 ===
train loss:1.7591329159202203
train loss:1.6084195054453954
train loss:1.5096650884245368
=== epoch:244, train acc:0.48, test acc:0.3724 ===
train loss:1.7121261197629665
train loss:1.5985904788095016
train loss:1.5411111356793752
=== epoch:245, train acc:0.48333333333333334, test acc:0.3731 ===
train loss:1.5982216585334155
train loss:1.638834051902801
train loss:1.5946055503395176
=== epoch:246, train acc:0.4866666666666667, test acc:0.3757 ===
train loss:1.656636585720448
train loss:1.4612906229616553
train loss:1.5941509626244075
=== epoch:247, train acc:0.4866666666666667, test acc:0.3796 ===
train loss:1.7164016810653757
train loss:1.5346580638881253
train loss:1.542538901818693
=== epoch:248, train acc:0.49, test acc:0.3832 ===
train loss:1.7308203984158939
train loss:1.5922302426665516
train loss:1.6108121535789053
=== epoch:249, train acc:0.49, test acc:0.3843 ===
train loss:1.5550386136578294
train loss:1.5563264600387745
train loss:1.614005513399188
=== epoch:250, train acc:0.4866666666666667, test acc:0.3842 ===
train loss:1.6500828782037262
train loss:1.607933999439878
train loss:1.6568878532643685
=== epoch:251, train acc:0.49333333333333335, test acc:0.3875 ===
train loss:1.4758006171269265
train loss:1.6532829044147137
train loss:1.5876132603249373
=== epoch:252, train acc:0.49333333333333335, test acc:0.3909 ===
train loss:1.5790953511321293
train loss:1.5211766697296012
train loss:1.6003585039092116
=== epoch:253, train acc:0.4866666666666667, test acc:0.3866 ===
train loss:1.5632813699473649
train loss:1.6778924478513166
train loss:1.7046327661625922
=== epoch:254, train acc:0.5066666666666667, test acc:0.3985 ===
train loss:1.6072893775061263
train loss:1.5160192765962208
train loss:1.6472567994911358
=== epoch:255, train acc:0.5066666666666667, test acc:0.401 ===
train loss:1.824939695036039
train loss:1.486832916459613
train loss:1.6273239580549872
=== epoch:256, train acc:0.52, test acc:0.4062 ===
train loss:1.4874601388255393
train loss:1.467083064091311
train loss:1.5727689542098664
=== epoch:257, train acc:0.53, test acc:0.4135 ===
train loss:1.6014578918452758
train loss:1.6171917806471432
train loss:1.6139329028629745
=== epoch:258, train acc:0.5333333333333333, test acc:0.4142 ===
train loss:1.6156921897466183
train loss:1.5796825560037462
train loss:1.6303723682756828
=== epoch:259, train acc:0.5366666666666666, test acc:0.416 ===
train loss:1.67597727636647
train loss:1.6359530460000717
train loss:1.5909065298419358
=== epoch:260, train acc:0.5333333333333333, test acc:0.4128 ===
train loss:1.476790153724701
train loss:1.645917837760857
train loss:1.5046795811581344
=== epoch:261, train acc:0.5366666666666666, test acc:0.4136 ===
train loss:1.592658864567856
train loss:1.6036165890026342
train loss:1.48338089704377
=== epoch:262, train acc:0.5466666666666666, test acc:0.4151 ===
train loss:1.4431031663487912
train loss:1.5213506892554178
train loss:1.5697001108058286
=== epoch:263, train acc:0.5566666666666666, test acc:0.4177 ===
train loss:1.5214170317352547
train loss:1.5180087638544617
train loss:1.5513941088676841
=== epoch:264, train acc:0.55, test acc:0.4188 ===
train loss:1.5683995482511086
train loss:1.4676937019399676
train loss:1.4580990824226263
=== epoch:265, train acc:0.5433333333333333, test acc:0.4186 ===
train loss:1.707480586407072
train loss:1.7280677973945473
train loss:1.5943649244397458
=== epoch:266, train acc:0.5433333333333333, test acc:0.4192 ===
train loss:1.6544583690371593
train loss:1.583485112615594
train loss:1.530855314452777
=== epoch:267, train acc:0.5533333333333333, test acc:0.421 ===
train loss:1.4163990687864885
train loss:1.5623199244026293
train loss:1.5682494418913913
=== epoch:268, train acc:0.5666666666666667, test acc:0.4257 ===
train loss:1.5866676552735208
train loss:1.5222086761796292
train loss:1.552707033154662
=== epoch:269, train acc:0.5666666666666667, test acc:0.4246 ===
train loss:1.4304363682995516
train loss:1.3942265034198058
train loss:1.509513509608932
=== epoch:270, train acc:0.5733333333333334, test acc:0.4274 ===
train loss:1.5285919174023705
train loss:1.523626471058269
train loss:1.5459679077521553
=== epoch:271, train acc:0.58, test acc:0.4319 ===
train loss:1.2754820882863398
train loss:1.3306849494085853
train loss:1.487829704384883
=== epoch:272, train acc:0.5733333333333334, test acc:0.4298 ===
train loss:1.693594217374419
train loss:1.4789983761448753
train loss:1.5293101160774887
=== epoch:273, train acc:0.5766666666666667, test acc:0.4322 ===
train loss:1.4397633088679538
train loss:1.688308959908451
train loss:1.591509956330321
=== epoch:274, train acc:0.59, test acc:0.4366 ===
train loss:1.5196009987955374
train loss:1.6142814885675005
train loss:1.5086929363310508
=== epoch:275, train acc:0.59, test acc:0.4406 ===
train loss:1.5089217384314362
train loss:1.607481330837734
train loss:1.4307427865183475
=== epoch:276, train acc:0.5933333333333334, test acc:0.4429 ===
train loss:1.449484243903928
train loss:1.6394668791191593
train loss:1.5061246469661724
=== epoch:277, train acc:0.59, test acc:0.4438 ===
train loss:1.5177055329009266
train loss:1.4645210190011435
train loss:1.5788320483727964
=== epoch:278, train acc:0.5933333333333334, test acc:0.4446 ===
train loss:1.3248500542673856
train loss:1.425712461833036
train loss:1.5769464795472186
=== epoch:279, train acc:0.59, test acc:0.4465 ===
train loss:1.5212173941834117
train loss:1.4893327404747831
train loss:1.4836456868756966
=== epoch:280, train acc:0.5966666666666667, test acc:0.4459 ===
train loss:1.5759914496885892
train loss:1.3897710796790788
train loss:1.4847861713034656
=== epoch:281, train acc:0.59, test acc:0.4459 ===
train loss:1.5329187589934907
train loss:1.3924491700739612
train loss:1.4864436046449185
=== epoch:282, train acc:0.5933333333333334, test acc:0.4482 ===
train loss:1.4984844900409855
train loss:1.3860508188393887
train loss:1.4957577775488926
=== epoch:283, train acc:0.6, test acc:0.4473 ===
train loss:1.5785863676120204
train loss:1.450373119556028
train loss:1.3911709595556598
=== epoch:284, train acc:0.6, test acc:0.4484 ===
train loss:1.4162460238454186
train loss:1.5943493677742087
train loss:1.2839307441393368
=== epoch:285, train acc:0.5933333333333334, test acc:0.4489 ===
train loss:1.5771857148568125
train loss:1.404284254426774
train loss:1.4446479155959886
=== epoch:286, train acc:0.5966666666666667, test acc:0.4476 ===
train loss:1.3910898457538057
train loss:1.5132214887411823
train loss:1.4751150145621412
=== epoch:287, train acc:0.59, test acc:0.4489 ===
train loss:1.4766364963477931
train loss:1.4797548246781274
train loss:1.420681103522677
=== epoch:288, train acc:0.5933333333333334, test acc:0.4511 ===
train loss:1.428367776657395
train loss:1.359248896336314
train loss:1.372411799737209
=== epoch:289, train acc:0.5933333333333334, test acc:0.4493 ===
train loss:1.5034113818168715
train loss:1.3577210499430956
train loss:1.2964329128208476
=== epoch:290, train acc:0.5966666666666667, test acc:0.45 ===
train loss:1.504386786709066
train loss:1.5729882332621126
train loss:1.2274979096454768
=== epoch:291, train acc:0.5933333333333334, test acc:0.4521 ===
train loss:1.308004413101983
train loss:1.489396218796289
train loss:1.3022177695228365
=== epoch:292, train acc:0.6033333333333334, test acc:0.4559 ===
train loss:1.5313962977249078
train loss:1.3349677964717706
train loss:1.3605829672214755
=== epoch:293, train acc:0.6033333333333334, test acc:0.458 ===
train loss:1.409897312064241
train loss:1.324359046307616
train loss:1.5679920469725541
=== epoch:294, train acc:0.6, test acc:0.4576 ===
train loss:1.3823909338766933
train loss:1.4190104963643273
train loss:1.4806251658954224
=== epoch:295, train acc:0.5966666666666667, test acc:0.4599 ===
train loss:1.4289183837655555
train loss:1.3465830333848827
train loss:1.4598031711166772
=== epoch:296, train acc:0.6, test acc:0.4615 ===
train loss:1.4444288546243653
train loss:1.4765143250458672
train loss:1.41917198141865
=== epoch:297, train acc:0.5966666666666667, test acc:0.4616 ===
train loss:1.379963562422937
train loss:1.4314152309404529
train loss:1.519393150293773
=== epoch:298, train acc:0.6, test acc:0.463 ===
train loss:1.3352178599115019
train loss:1.37437421278872
train loss:1.3801700621916182
=== epoch:299, train acc:0.6066666666666667, test acc:0.4664 ===
train loss:1.3744326244886877
train loss:1.4929308777758425
train loss:1.3930432943610944
=== epoch:300, train acc:0.61, test acc:0.468 ===
train loss:1.3487368002494748
train loss:1.2965541557717892
train loss:1.2238049784281597
=== epoch:301, train acc:0.5933333333333334, test acc:0.4681 ===
train loss:1.4355372170789529
train loss:1.4458507216463057
=============== Final Test Accuracy ===============
test acc:0.4699
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>그림과 같이 드롭아웃을 적용하니 훈련 데이터와 시험 데이터에 대한 정확도 차이가 줄었다.</li>
<li>또, 훈련 데이터에 대한 정확도가 100%에 도달하지도 않게 되었다<ul>
<li>즉, 표현력을 높이면서도 오버 피팅을 억제할 수 있다. </li>
<li>원래는 train의 정확도가 1에 육박하였으며 test와의 정확도 차이도 벌어져 있었다. </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>기계학습에서의 앙상블 학습은 드롭아웃과 밀접하다. <ul>
<li>드롭아웃이 학습 때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석할 수 있기 때문이다. </li>
<li>그리고 추론 때는 뉴런의 출력에 삭제한 비율을 곱함으로써 앙상블 학습에서 여러 모델의 평균을 내는 것과 같은 효과를 얻는 것이다. </li>
<li>즉 드롭아웃은 앙상블 학습과 같은 효과를 하나의 네트워크로 구현했다고 생각할 수 있다. </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>적절한 하이퍼파라미터 값 찾기<ul>
<li>하이퍼파라미터? 각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시의 학습률과 가중치 감소 등이다. </li>
<li>하이퍼 파라미터의 값을 최대한 효율적으로 탐색하는 방법을 설명한다. </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>검증 데이터 <ul>
<li>앞으로 하이퍼파라미터를 다양한 값으로 설정하고 검증할 텐데 여기서 주의할 점은 하이퍼파라미터의 성능을 평가할 때는 시험 데이터를 사용해서 안 된다.<ul>
<li>시험 데이터를 사용하여 하이퍼파라미터를 조정하면 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문이다.</li>
<li>즉, 하이퍼파라미터 값의 좋음을 시험 데이터로 확인하게 되므로 하이퍼파라미터의 값이 시험 데이터에만 적합하도록 조정되어 버린다. </li>
<li>그렇게 되면 다른 데이터에는 적응하지 못하니 범용 성능이 떨어지는 모델이 될지도 모른다. </li>
<li>따라서 하이퍼파라미터를 조정할 때는 하이퍼파라미터 전용 확인 데이터가 필요하다. 이때 검증 데이터라고 부른다. </li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>훈련 데이터</strong> : 매개변수 학습</li>
<li><strong>검증 데이터</strong> : 하이퍼파라미터 성능 평가</li>
<li><strong>시험 데이터</strong> : 신경망의 범용 성능 평가</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>데이터 셋이 훈련 데이터와 시험 데이터로만 분리되어 있다면 검증 데이터를 직접 분리해주어야 한다. </li>
<li>훈련 데이터 중 20% 정도를 검증 데이터로 먼저 분리하자</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shuffle_dataset</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">permutation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">permutation</span><span class="p">,:]</span> <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="n">x</span><span class="p">[</span><span class="n">permutation</span><span class="p">,:,:,:]</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="n">permutation</span><span class="p">]</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">t_train</span><span class="p">),(</span><span class="n">x_test</span><span class="p">,</span><span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">load_mnist</span><span class="p">()</span>

<span class="c1"># 훈련 데이터를 뒤섞는다.</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span> <span class="o">=</span> <span class="n">shuffle_dataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>

<span class="c1"># 20%를 검증 데이터로 분할</span>
<span class="n">validation_rate</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
<span class="n">validation_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">validation_rate</span><span class="p">)</span>

<span class="n">x_val</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:</span><span class="n">validation_num</span><span class="p">]</span>
<span class="n">t_val</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[:</span><span class="n">validation_num</span><span class="p">]</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">validation_num</span><span class="p">:]</span>
<span class="n">t_train</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">validation_num</span><span class="p">:]</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>해당 코드를 통해 훈련 데이터를 분리하기 전에 입력 데이터와 정답 레이블을 뒤섞는다. 데이터 셋 안의 데이터가 치우쳐 있을지도 모르기 때문이다. </li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>하이퍼파라미터 최적화<ul>
<li>하이퍼파라미터를 최적화할 때의 핵심은 하이퍼파라미터의 최적값이 존재하는 범위를 조금씩 줄여간다는 것이다. </li>
<li>범위를 조금씩 줄이려면, 우선 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼파라미터 값을 골라낸 후, 그 값으로 정확도를 평가한다. 정확도를 잘 살피면서 이 작업을 여러 번 반복하며 하이퍼파라미터의 최적 값의 범위를 좁혀가는 것이다. </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>정리<ul>
<li>이번 장에서는 신경망 학습에 중요한 기술 몇 가지를 소개했다. 매개변수 갱신 방법과 가중치의 초깃값을 설정하는 방법, 또 배치 정규화와 드롭아웃 등 현대적인 신경망에서 빼놓을 수 없는 기술들이다.<ul>
<li>매개변수 갱신 방법 : 확률적 경사 하강법(SGD), 모멘텀, AdaGrad,Adam등이 있다. </li>
<li>가중치 초깃값을 정하는 방법은 올바른 학습을 하는 데 매우 중요하다. </li>
<li>가충치의 초깃값으로는 Xavier 초깃값과 He 초깃값이 효과적이다.</li>
<li>배치 정규화를 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값의 영향을 덜 받을 수 있다.</li>
<li>오버피팅을 억제하는 정규화 기술로는 가중치 감소와 드롭아웃이 있다. </li>
<li>하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 점차 좁히면서 하는 것이 효과적이다. </li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>합성곱 신경망(Convolutional Neural Network, CNN)<ul>
<li>이미지 인식과 음성 인식 등 다양한 곳에서 사용되는데 특히, 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 다 CNN을 기초로 한다. </li>
<li>지금까지 본 신경망과 같이 레고 블록처럼 계층을 조합하여 만들 수 있다. </li>
<li>다만, <code>합성곱 계층</code>과 <code>풀링 계층</code>이 새롭게 등장한다.</li>
<li>지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었다. 이를 완전연결, 전결합이라고 하며, 완전히 연결된 계층을 Affine 계층이라는 이름으로 구현했다.</li>
<li>완전 연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층 혹은 Sigmoid 계층이 이어진다. 그렇다면 CNN의 구조는 어떻게 다를까<ul>
<li>CNN에서는 새로운 합성곱 계층과 풀링 계층이 추가된다.</li>
<li>합성곱 계층 - ReLU - 풀링계층 흐름으로 연결된다. 풀링계층은 생략가능하다.</li>
<li>주의* 출력에 가까운 층은 지금까지의 Affine-ReLU 구성을 사용할 수 있다는 것이다. 또 마지막 출력 계층에선 Affine - Softmax 조합을 그대로 사용한다. </li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>CNN에서는 패딩, 스트라이드 등 CNN 고유의 용어가 등장한다. 또, 각 계층 사이에는 3차원 데이터같이 입체적인 데이터가 흐른다는 점에서 완전연결 신경망과 다르다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>완전 연결 계층의 문제점<ul>
<li>완전연결 계층에서는 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있었다.</li>
<li>그렇다면 문제점은 무엇인가? 데이터의 형상이 무시된다는 것이다. <ul>
<li>만약 입력 데이터가 이미지인 경우를 예로 들면, 이미지는 통상 세로 , 가로, 채널(색상)로 구성된 3차원 데이터이다.</li>
<li>그러나 완전연결 계층에 입력할 때는 3차원 데이터를 평평한 1차원 데이터로 평탄화해줘야 한다. </li>
<li>완전연결 계층은 형상을 무시하고, 모든 입력 데이터를 동등한 뉴런 즉 같은 차원의 뉴런으로 취급하여 형상에 담긴 정보를 살릴 수 없다. </li>
<li>한편, 합성곱 계층은 형상을 유지한다. 이미지도 3차원 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차원 데이터로 전달한다. 그래서 CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 가능성이 있는 것이다. </li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>CNN에서는 합성곱 계층의 입출력 데이터를 특징 맵(feature map)이라고도 한다. </li>
<li>합성곱 계층의 입력 데이터를 입력 특징 맵, 출력 데이터를 출력 특징 맵이라고 하는 식이다. <ul>
<li>이 책에서는 입출력 데이터와 특징 맵을 같은 의미로 사용한다. </li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>합성곱 계층에서의 합성곱 연산을 처리하자. </li>
<li>합성곱 연산은 이미지 처리에서 말하는 필터 연산에 해당한다. 필터를 커널이라 칭하기도 한다. </li>
<li>합성곱 연산의 자세한 과정은 교재 231p를 참고한다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수가 그동안의 가중치에 해당한다. 그리고 CNN에도 편향이 존재한다. </li>
<li>편향은 필터를 적용한 후의 데이터에 더해진다. 그리고 편향은 항상 하나만 스칼라로서 존재한다. 그 하나의 값을 필터를 적용한 모든 원소에 더하는 것이다.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>패딩<ul>
<li>합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(예컨대 0)으로 채우기도 한다. 이를 패딩이라한다. </li>
<li>그렇게되면 출력 데이터의 형상도 달라질 것이다. <ul>
<li>즉 패딩은 주로 출력 크기를 조정할 목적으로 사용한다. 예를 들어(4,4) 입력데이터에 (3,3) 필터를 적용하면 출력은 (2,2)가 되어 입력보다 2만큼 줄어든게 된다. 이는 합성곱 연산을 몇 번이나 되풀이하는 심층 신경망에서는 문제가 될 수 있다. </li>
<li>합성곱 연산을 거칠 때마다 크기가 작아지면 어느 시점에서는 출력 크기가 1이 되어버린다. 더 이상은 합성곱 연산을 적용할 수 없다는 뜻이다. 이러한 사태를 막기 위해 패딩을 적용한다. </li>
<li>이를 이용하여 입력 데이터의 공간적 크기를 고정한 채로 다음 계층에 전달할 수 있다. </li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>스트라이드<ul>
<li>필터를 적용하는 위치의 간격을 스트라이드라고 한다. </li>
<li>스트라이드를 2로 하면 필터를 적용하는 윈도우가 두 칸씩 이동한다. <ul>
<li>그렇다면, 스트라이드를 키우면 출력의 크기는 작아진다. 한편, 패딩을 크게하면 출력 크기가 커졌었다. </li>
<li>이러한 관계를 수식화한 234p를 참고해보자</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/INTROdl/2022/02/18/dl.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/INTROdl/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/INTROdl/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/INTROdl/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/rhkrehtjd" target="_blank" title="rhkrehtjd"><svg class="svg-icon grey"><use xlink:href="/INTROdl/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
