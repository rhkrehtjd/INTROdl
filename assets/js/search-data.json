{
  
    
        "post0": {
            "title": "클래스로 신경망 구현, 오차역 전파법",
            "content": "학습 알고리즘 구현하기 신경망 학습의 순서를 확인해보자 전제 : 신경망에는 적응 가능한 가중치와 편향이 이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라고 한다. 신경망 학습은 다음과 같이 4단계로 수행된다. | 1) 미니배치 : 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것이 목표이다. | 2) 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실함수의 값을 가장 작게 하는 방향을 제시한다. | 3) 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다. | 4) 반복 : 1~3단계를 반복한다. | . | 이것이 신경망 학습이 이루어지는 순서이다. 이는 경사 하강법으로 매개변수를 갱신하는 방법이며, 이때 데이터를 미니배치로 무작위로 선정하기 때문에 확률적 경사 하강법이라고 부른다. (이하 SGD) 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법이라 의미이다. | . | 실제로 손글씨 숫자를 학습하는 신경망을 구현해보자 | 여기에서는 2층 신경망(은닉층이 1개인 네트워크)을 대상으로 MNIST 데이터셋을 사용하여 학습을 수행한다. | 처음에는 2층 신경망을 하나의 클래스로 구현하는 것부처 시작한다. | 이 클래스의 이름은 TwoLayerNet이다. | . import sys, os sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정 from common.functions import * from common.gradient import numerical_gradient class TwoLayerNet: def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01): # 가중치 초기화 self.params = {} self.params[&#39;W1&#39;] = weight_init_std * np.random.randn(input_size, hidden_size) self.params[&#39;b1&#39;] = np.zeros(hidden_size) self.params[&#39;W2&#39;] = weight_init_std * np.random.randn(hidden_size, output_size) self.params[&#39;b2&#39;] = np.zeros(output_size) def predict(self, x): W1, W2 = self.params[&#39;W1&#39;], self.params[&#39;W2&#39;] b1, b2 = self.params[&#39;b1&#39;], self.params[&#39;b2&#39;] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 y = softmax(a2) return y # x : 입력 데이터, t : 정답 레이블 def loss(self, x, t): y = self.predict(x) return cross_entropy_error(y, t) def accuracy(self, x, t): y = self.predict(x) y = np.argmax(y, axis=1) t = np.argmax(t, axis=1) accuracy = np.sum(y == t) / float(x.shape[0]) return accuracy # x : 입력 데이터, t : 정답 레이블 def numerical_gradient(self, x, t): loss_W = lambda W: self.loss(x, t) grads = {} grads[&#39;W1&#39;] = numerical_gradient(loss_W, self.params[&#39;W1&#39;]) grads[&#39;b1&#39;] = numerical_gradient(loss_W, self.params[&#39;b1&#39;]) grads[&#39;W2&#39;] = numerical_gradient(loss_W, self.params[&#39;W2&#39;]) grads[&#39;b2&#39;] = numerical_gradient(loss_W, self.params[&#39;b2&#39;]) return grads . 앞에서 다룬 신경망의 순전파 처리 구현과 공통되는 부분이 많고, 새로운 내용은 딱히 없다. | 우선 이 클래스가 사용하는 변수와 메서드를 정리해보자 중요해보이는 것 일부만 작성하였으며 그 외의 것은 139p를 참고하자 params : 신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수) | grads : 기울기 보관하는 딕셔너리 변수 (numerical_gradient() 메서드의 반환 값) | . | TwoLayerNet 클래스는 딕셔너리인 params와 grads를 인스턴스 변수로 갖는다. | 자세한 내용은 해당 교재를 참고하자. | 예를 하나 살펴보자 | . | . net = TwoLayerNet(input_size = 784, hidden_size= 100, output_size= 10) print(net.params[&#39;W1&#39;].shape) print(net.params[&#39;b1&#39;].shape) print(net.params[&#39;W2&#39;].shape) print(net.params[&#39;b2&#39;].shape) . (784, 100) (100,) (100, 10) (10,) . 이와 같이 params 변수에는 이 신경망에 필요한 매개변수가 모두 저장된다. 그리고 params 변수에 저장된 가중치 매개변수가 예측 처리(순방향 처리)에서 사용된다. 참고로 예측 처리는 다음과 같이 실행할 수 있다. | . x = np.random.rand(100,784) # 더미 입력 데이터 100장 분량 y = net.predict(x) . grads 변수에는 params 변수에 대응하는 각 매개변수의 기울기가 저장된다. 예를 들어 다음과 같이 numericla_gradient() 메서드를 사용해 기울기를 계산하면 grads 변수에 기울기 정보가 저장된다. | . x = np.random.rand(100,784) # 더미 입력 데이터 (100장 분량) t = np.random.rand(100,10) # 더미 정답 레이블 (100장 분량) grads = net.numerical_gradient(x,t) print(grads[&#39;W1&#39;].shape) print(grads[&#39;b1&#39;].shape) print(grads[&#39;W2&#39;].shape) print(grads[&#39;b2&#39;].shape) . (784, 100) (100,) (100, 10) (10,) . 이어서 TwoLayerNet 메서드를 살펴보자 우선 init : 메서드는 클래스를 초기화한다. (이 초기화 메서드는 TwoLayerNet을 생성할 때 불리는 메서드이다.) | 추가 : 신경망 학습은 시간이 오래 걸리니, 시간을 절약하려면 같은 결과를 훨씬 빠르게 얻을 수 있는 오차역전파법으로 각 매개변수의 손실 함수에 대한 기울기를 계산할 수 있다. 이는 다음장에서 학습할 것이다. | . | . 미니배치 학습 구현하기 신경망 학습 구현에는 앞에서 설명한 미니배치 학습을 활용한다. 미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고, 그 미니배치에 대해서 경사법으로 매개변수를 갱신한다. | . | . import numpy as np import matplotlib.pyplot as plt from dataset.mnist import load_mnist from two_layer_net import TwoLayerNet # 데이터 읽기 (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10) # 하이퍼파라미터 iters_num = 10000 # 반복 횟수를 적절히 설정한다. train_size = x_train.shape[0] batch_size = 100 # 미니배치 크기 learning_rate = 0.1 train_loss_list = [] for i in range(iters_num): # 미니배치 획득 batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 기울기 계산 #grad = network.numerical_gradient(x_batch, t_batch) grad = network.gradient(x_batch, t_batch) # 매개변수 갱신 for key in (&#39;W1&#39;, &#39;b1&#39;, &#39;W2&#39;, &#39;b2&#39;): network.params[key] -= learning_rate * grad[key] # 학습 경과 기록 loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) . 여기서는 미니배치 크기를 100으로 했다. 즉, 매번 60000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려낸다. 그리고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다. 경사법에 의한 갱신 횟수(반복 횟수)를 10000번으로 설정하고 갱신할 때마다 훈련 데이터에 대한 손실함수를 계산하고 그 값을 배열에 추가한다. | 학습 횟수가 늘어가면서 손실 함수의 값이 줄어들고 이는 학습이 잘 이루어지고 있다는 뜻으로 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미한다. 신경망이 학습하고 있는 것이다. 다시 말해 데이터를 반복해서 학습함으로써 최적 가중치 매개변수로 서서히 다가가고 있는 것이다. | . 하지만 정확히는 훈련 데이터의 미니배치에 대한 손실 함수의 값이다. 훈련 데이터의 손실 함수 값이 작아지는 것은 잘 학습하고 있다는 방증이지만 이 결과만으로는 다른 데이터셋에서도 비슷한 실력을 발휘할지는 확실하지 않다. | 신경망 학습에서는 훈련 데이터 외의 데이터를 올바르게 인식하는지를 확인해여 한다. 다른 말로 오버피팅을 일으키지 않는지 확인해야 한다. 오비피팅 되었다는 것은 예를 들어 훈련 데이터 포함된 이미지만 제대로 구분하고 그렇지 않은 이미지는 식별할 수 없다는 뜻이다. | 범용적인 능력의 평가를 위해, 훈련 데이터에 포함되지 않은 데이터를 사용해 평가해봐야 한다. 이를 위해 다음 구현에서는 학습 도중 정기적으로 훈련 데이터와 시험 데이터를 대상으로 정확도를 기록한다. 여기에서는 1에폭별로 훈련 데이터와 시험 데이터에 대한 정확도를 기록한다. 에폭은 하나의 단위이다. 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다. 예컨대 훈련 데이터 10000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 소진한게 된다. 이 경우 100회가 1에폭이 된다. | . | . | . from dataset.mnist import load_mnist from two_layer_net import TwoLayerNet # 데이터 읽기 (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10) # 하이퍼파라미터 iters_num = 10000 # 반복 횟수를 적절히 설정한다. train_size = x_train.shape[0] batch_size = 100 # 미니배치 크기 learning_rate = 0.1 train_loss_list = [] train_acc_list = [] test_acc_list = [] # 1에폭당 반복 수 iter_per_epoch = max(train_size / batch_size, 1) for i in range(iters_num): # 미니배치 획득 batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] # 기울기 계산 #grad = network.numerical_gradient(x_batch, t_batch) grad = network.gradient(x_batch, t_batch) # 매개변수 갱신 for key in (&#39;W1&#39;, &#39;b1&#39;, &#39;W2&#39;, &#39;b2&#39;): network.params[key] -= learning_rate * grad[key] # 학습 경과 기록 loss = network.loss(x_batch, t_batch) train_loss_list.append(loss) # 1에폭당 정확도 계산 if i % iter_per_epoch == 0: train_acc = network.accuracy(x_train, t_train) test_acc = network.accuracy(x_test, t_test) train_acc_list.append(train_acc) test_acc_list.append(test_acc) print(&quot;train acc, test acc | &quot; + str(train_acc) + &quot;, &quot; + str(test_acc)) . train acc, test acc | 0.09915, 0.1009 train acc, test acc | 0.7825666666666666, 0.7853 train acc, test acc | 0.8771333333333333, 0.8807 train acc, test acc | 0.8981166666666667, 0.9014 train acc, test acc | 0.9082833333333333, 0.9104 train acc, test acc | 0.9147, 0.9171 train acc, test acc | 0.9193, 0.9215 train acc, test acc | 0.92455, 0.9265 train acc, test acc | 0.9277166666666666, 0.9291 train acc, test acc | 0.9313833333333333, 0.9322 train acc, test acc | 0.93445, 0.9348 train acc, test acc | 0.9368333333333333, 0.9367 train acc, test acc | 0.939, 0.9385 train acc, test acc | 0.9403666666666667, 0.9408 train acc, test acc | 0.9431666666666667, 0.942 train acc, test acc | 0.9448, 0.9435 train acc, test acc | 0.94635, 0.9451 . 이 예에서는 1에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고 그 결과를 기록한다. | 정확도를 1에폭마다 계산하는 이유는 for문 앞에서 매번 계산하기에는 시간이 오래 걸리고 또 그렇게까지 자주 기록할 필요도 없기 때문이다. | 앞의 코드로 얻은 결과를 그래프로 그려보자 | . markers = {&#39;train&#39;: &#39;o&#39;, &#39;test&#39;: &#39;s&#39;} x = np.arange(len(train_acc_list)) plt.plot(x, train_acc_list, label=&#39;train acc&#39;) plt.plot(x, test_acc_list, label=&#39;test acc&#39;, linestyle=&#39;--&#39;) plt.xlabel(&quot;epochs&quot;) plt.ylabel(&quot;accuracy&quot;) plt.ylim(0, 1.0) plt.legend(loc=&#39;lower right&#39;) plt.show() . 훈련 데이터에 대한 정확도를 실선으로 시험 데이터 에대한 정확도를 점선으로 그렸다. 보다시피 에폭이 진행될수록, 즉 학습이 진행될수록 훈련데이터와 시험데이터를 사용하고 평가한 정확도가 모두 좋아지고 있다. 또 두 정확도에는 차이가 없음을 알 수 있다. 다시 말해 이번 학습에서는 오버피팅이 일어나지 않았음을 알 수 있다. | . 만약 오버피팅이 일어난다면? 훈련이란 훈련 데이터에 대한 정확도를 높이는 방향으로 학습이 이루어지니 그 정확도는 에폭을 반복할 수록 높아진다. 반면 훈련 데이터에 지나치게 적응하면, 즉 오버피팅되면 훈련 데이터와는 다른 데이터를 보면 잘못된 판단을 하기 시작한다. 어느 순간부터 시험 데이터에 대한 정확도가 점차 떨어지기 시작한다는 뜻이다. 이 순간이 오버피팅이 시작되는 순간이다. 여기서 중요한 insight! $ to$ 이 순간을 포착해 학습을 중단하면 오버피팅을 효과적으로 예방할 수 있을 것이다. 이 기법을 조기 종료라 하며, 가중치 감소, 드롭 아웃과 함께 대표적인 오버피팅 예방법이다. | . | . 결론 기계학습에서 사용하는 데이터 셋은 훈련 데이터와 시험 데이터로 나눠 사용한다. | 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다. | 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다. | 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다. | 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다. | 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다. | 수치 미분을 이요한 계산에는 시간이 걸리지만, 그 구현은 간단하다. 한편, 다음 장에서 구현하는 다소 복잡한 오차역 전파법은 기울기를 고속으로 구할 수 있다. | . | . . 오차역 전파법 가중치 매개변수의 기울기 정확히는 가중치 매개변수에 대한 손실 함수의 기울기를 효율적으로 계산하는 오차역 전파법을 배워보자 | 오차역 전파법 이해하기 오차역 전파법을 수식으로도 이해할 수 있겠지만, 이번 장에서는 계산 그래프를 사용해서 시작적으로 이해해보자 | . | . | . 계산 그래프 계산 과정을 그래프로 나타낸 것이다. | 복수의 노드와 에지로 표현된다. 노드 사이의 직선을 에지라고 한다. | 간단한 문제부터 해결해보자 | . | . 문제 1) 현빈 군은 슈퍼에서 1개에 100원인 사과를 2개 샀다. 이때 지불 금액을 구하세요. 단, 소비세가 10% 부과된다. 계산 그래프는 계산 과정을 노드와 화살표(에지)로 표현한다. | 원안에 연산 내용을 적고 계산 결과를 화살표 위에 적어 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해지게 한다. | 여기서는 원 대신 괄호로 대체한다. | (사과) $ to$ 100 $ to$ (x2) $ to$ 200 $ to$ (x1.1) $ to$ 220 | 처음에 사과의 100원이 x2 노드로 흐르고 200원이 되어 다음 노드(x1.1)로 전달된다. 이제 200원이 x1.1 노드를 거쳐 220원이 된다. | 따라서 이 계산 그래프에 따르면 최종 답은 220원이 된다. | 여기에서는 x2와 x1.1을 각각 하나의 연산으로 취급해 원 안에 표기했지만, 곱셈이 x만을 연산으로 생각할 수도 있다. 이렇게하면 다음과 같이 2와 1.1은 각각 사과의 개수와 소비세 변수가 되어 원밖에 표기하게 된다. | 사과 $ to$ 100 $ to$ (x) $ to$ 200 $ to$ (x) $ to$ 220 $ to$ | 이제 첫 번째 노드에 2가 대입되고 다음 노드에 1.1이 대입된다. | . | . 지금까지 살펴본 계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행된다. 1) 계산 그래프를 구성한다. | 2) 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다. | 여기서 2번째 &#39;계산을 왼쪽에서 오른쪽으로 진행&#39;하는 단계를 순전파라고 한다. | 순전파의 반대 &#39;역전파&#39;도 존재할 것이다. 역전파는 이후에 미분을 계산할 때 중요한 역할을 한다. | . | . 계산 그래프의 특징 : 국소적(자신과 직접 관계된 작은 범위)계산을 전파함으로써 최종 결과를 얻는다. 국소적 계산은 결국 전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다는 점이다. | 국소적 계산? 가령 슈퍼마켓에서 사과 2개를 포함한 여러 식품을 구입하는 경우를 생각해보자. 여러 식품을 구입하여 총 금액이 3000원이 되었다 여기에서 ㅎ개심은 각 노드에서의 계산은 국소적 계산이라는 점이다. 가령 사과와 그 외의 물품 값을 더하는 계산 3000+200에서 3000이라는 숫자가 어떻게 계산되었느냐와는 상관없이 단지 두 숫자(3000과 200)를 더하면 된다는 뜻이다. 각 노드는 자신과 관련한 계산외에는 아무것도 신경 쓸 게 없다는 것이다. | 이처럼 계산 그래프는 국소적 계산에 집중한다. 전체 계산이 제아무리 복잡하더라도 각 단계에서 하는 일은 해당 노드의 국소적 계산이다. 국소적 계산은 단순하지만, 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 해낼 수 있다. | 또한 계산 그래프는 중간 계산 결과를 모두 보관할 수 있다. | 또한 역절파를 통해 미분을 효율적으로 계산할 수 있다. 위에서 설명한 문제1)을 살펴보자 | 가령 사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지를 알고 싶다고 해보자. | 이는 사과 과격에 대한 지불 금액의 미분을 구하는 문제에 해당된다. 기호로 나타낸다면 x는 사과 값을, L을 지불 금액이라고 했을때 $ frac{ partial L}{ partial x}$을 구하는 것이다. | 위 미분 값은 사과 값이 아주 조금 올랐을 때 지불 금액이 얼마나 증가하느냐를 표시한 것이다. | 즉, 사과 과격에 대한 지불 금액의 미분 같은 값은 계산 그래프에서 역전파를 하면 구할 수 있다. | 역전파가 어떻게 이루어지느냐는 뒤에서 추가 설명하겠다. | . | . | . | . 이처럼 계산 그래프의 이점은 순전파와 역전파를 활용해서 각 변수의 미분을 효율적으로 구할 수 있다는 것이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/10/dl.html",
            "relUrl": "/2022/02/10/dl.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "수치 미분",
            "content": "경사법에서는 기울기 값을 기준으로 나아갈 방향을 정한다. | 함수를 미분하는 계산을 구현해보자 | . def numerical_diff(f,x): h = 1e-50 # 매우 작은 수, 소수점 아래 0이 50개 return (f(x+h) - f(x))/h . 위 함수에서 개선해야 할 점 1) 1e-50은 반올림 오차 문제를 일으킨다. 작은 값이 생략되어 최종 계산 결과에 오차가 생기게 하는 것이다. 예를 들면 | . | . import numpy as np np.float32(1e-50) . 0.0 . 미세한 값 h를 10-4 정도로 이용해보자. 10-4 정도의 값을 사용하면 좋은 결과를 얻는다고 알려져 있다. | 두 번째 개선 : 함수 f의 차분(임의의 두 점에서의 함수 값들의 차이)과 관련한 것이다. 위 함수에서는 x+h와 x사이의 함수f의 차분을 계산하고 있지만, 애당초 이 계산에는 오차가 있다는 사실에 주의해야 한다. | 진정한 미분은 x 위치의 함수의 기울기(이를 접선이라고 함)에 해당하지만, 이번 구현에서의 미분은 (x+h)와 x사이의 기울기에 해당한다 | 그래서 진정한 미분(진정한 접선)과 이번 구현의 값은 엄밀히는 일치하지 않는다. 이 차이는 h를 무한히 0으로 좁히는 것이 불가능해 생기는 한계이다. | . | . 수치 미분에는 오차가 포함된다. 이 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수f의 차분을 계산하는 방법을 쓰기도 한다. | 해당 차분은 x를 중심으로 그 전후의 차분을 계산한다는 의미에서 중심 차분 혹은 중앙 차분이라고 한다. (x와 x+h의 차분은 전방 차분이라고 한다.) | . | 두 개선점을 적용해 수치 미분을 구현해보자 | . def numerical_diff(f,x): h = 1e-4 #0.0001 # 개선 1 return (f(x+h) - f(x-h)) / (2*h) # 2h = x+h-(x-h) =2h # 개선 2 . . 앞 절의 수치 미분을 사용하여 간단한 함수를 미분해보자 | . def function_1(x): return 0.01*x**2 + 0.1*x . import matplotlib.pyplot as plt x = np.arange(0.0,20.0,0.1) y = function_1(x) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;f(x)&#39;) plt.plot(x,y) plt.show() . 이제 x는 5일 때와 10일 때의 함수의 미분을 계산해보자 | . print(numerical_diff(function_1,5)) print(numerical_diff(function_1,10)) # 함수의 기울기에 해당한다. . 0.1999999999990898 0.2999999999986347 . . def tangent_line(f, x): d = numerical_diff(f, x) print(d) y = f(x) - d*x return lambda t: d*t + y fig, (ax1,ax2)= plt.subplots(1,2) x = np.arange(0.0, 20.0, 0.1) y = function_1(x) tf = tangent_line(function_1, 5) y2 = tf(x) ax1.plot(x, y) ax1.plot(x, y2) ax1.set_xlabel(&#39;x&#39;) ax1.set_ylabel(&#39;f(x)&#39;) tf = tangent_line(function_1, 10) y2 = tf(x) ax2.plot(x, y) ax2.plot(x, y2) ax2.set_xlabel(&#39;x&#39;) ax2.set_ylabel(&#39;f(x)&#39;) plt.show() . 0.1999999999990898 0.2999999999986347 . . 편미분 | . def function_2(x): return x[0]**2+x[1]**2 # 또는 return np.sum(x**2) . 앞의 예와 달리 변수가 2개이다 미분해보자 변수가 2개다 | 어느 변수에 대한 미분이냐가 중요하다 | 이와 같이 변수가 여럿인 함수에 대한 미분을 편미분이라고 한다. | $ frac{ partial f}{ partial x_0}$ | $ frac{ partial f}{ partial x_1}$ | . | . | . x_0 = 3, x_1 = 4 일 때, x_0에 대한 편미분을 구해보자 | . def function_tmp1(x0): return x0*x0 + 4**2 numerical_diff(function_tmp1,3) . 6.00000000000378 . def function_tmp2(x1): return 3**2 + x1*x1 numerical_diff(function_tmp2,4) . 7.999999999999119 . 변수가 하나인 함수를 정의하고, 그 함수를 미분하는 형태로 구현하여 풀었다. 예를 들어 문제 1에서는 x1=4로 고정된 새로운 함수를 정의하고 변수가 x0 하나뿐인 함수에 대해 이전에 정의해두었던 수치 미분 함수를 적용하였따. | 이처럼 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다. | 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다. | 앞의 예에서는 목표 변수를 제외한 나머지를 특정 값에 고정하기 위해서 새로운 함수를 정의했다. | 그리고 그 새로이 정의한 함수에 대해 그동안 사용한 수치 미분 함수를 적용하여 편미분을 구한 것이다. | . | . . 기울기 위에선 x0와 x1에 대한 편미분 각각을 구했다. 이번엔 편미분을 묶어서 | ($ frac{ partial f}{ partial x_0}$, $ frac{ partial f}{ partial x_1}$) | 을 계산해본다고 해보자. 이때 ($ frac{ partial f}{ partial x_0}$, $ frac{ partial f}{ partial x_1}$)처럼 모든 변수의 편미분을 벡터로 정리한 것을 기울기라고 한다. | 구현해보자 | . | . def numerical_gradient(f,x): h = 1e-4 grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성 for idx in range(x.size): tmp_val = x[idx] # f(x+h) 계산 x[idx] = tmp_val + h fxh1 = f(x) # f(x-h) 계산 x[idx] = tmp_val - h fxh2 = f(x) grad[idx] = (fxh1 - fxh2) / (2*h) x[idx] = tmp_val # 값 복원 return grad . 좀 복잡해보이지만 동작 방식은 변수가 하나일 때의 수치 미분과 거의 동일하다 | . numerical_gradient(function_2,np.array([3.0,4.0])) . array([6., 8.]) . numerical_gradient(function_2,np.array([0.0,2.0])) . array([0., 4.]) . numerical_gradient(function_2,np.array([3.0,0.0])) . array([6., 0.]) . 아직은 잘 모르겠지만 numerical_gradient함수에 인수로 넘파이 배열 입력해줄 때, 소수형태가 아닌 int형태로 입력해주면 값이 현저히 달라짐 | 그런데 이 기울기가 의미하는 건 뭘까, 그림으로 그려서 이해해보자 기울기의 결과에 마이너스를 붙인 벡터를 그린것이다. | . | . 기울기 그림을 살펴보면 방향을 가진 벡터로 그려진다. | 기욹는 함수의 가장 낮은 장소(최솟값)를 가리키는 것 같다. | 또 가장 낮은 곳(최솟값)에서 멀어질수록 화살표의 크기가 커짐을 알 수 있다. | 위 그림에서 기울기는 가장 낮은 장소를 가리킨다. 실제는 반드시 그렇다고는 할 수 없다. | 사실 기울기는 각 지점에서 낮아지는 방향을 가리킨다. #### 더 정확히 말하자면 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향이다 | (※ 매우중요) | . | . . 경사법(경사 하강법) 기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수(가중치와 편향)를 학습 시에 찾아야 한다. 여기에서 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값이다. 이런 상황에서 기울기를 잘 이용해 함수의 최소값(또는 가능한 한 작은 값)을 찾으려는 것이 경사법이다. | 여기에서 주의할 점은 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 기울기라는 것이다. 그렇지만 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 즉 그쪽이 정말로 나아갈 방향인지는 보장할 수 없다. 실제로 복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없는 경우가 대부분이다. | . | 기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 그 방향으로 가야 함수의 값을 줄일 수 있다. | 그래서 최솟값이 되는 장소를 찾는 문제, 아니면 가능한 한 작은 값이 되는 장소를 찾는 문제에서는 기울기 정보를 단서로 나아갈 방향을 정해야 한다. | . | . 경사법 : 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그런 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 그 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 줄이는 것이 경사법이다. 경사법은 기계학습을 최적화하는 데 흔히 쓰는 방법이다. 특히 신경망에서는 경사법을 많이 사용한다. | . 경사법을 수식으로 나타내보자 $x_0 = x_0 - eta frac{ partial f}{ partial x_0}$ | $x_1 = x_1 - eta frac{ partial f}{ partial x_1}$ | . | $ eta$는 갱신하는 양이다. 신경망 학습에서는 학습률이라고 한다. 한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하느냐를 정하는 것이 학습률이다. | 위 식은 1회에 해당하는 갱신이고, 이 단계를 반복한다. | 위 단계를 반복하며 서서히 함수의 값을 줄이는 것이다. | 또한 학습률은 0.01 혹은 0.001 등 미리 특정한 값으로 정해두어야 하는데, 일반적으로 이 값이 너무 크거나 작으면 좋은 장소를 찾아갈 수 없다. 신경망 학습에서는 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인하면서 진행한다. | . . 경사 하강법을 구현해보자 | . def gradient_descent(f,init_x,lr=0.01,step_num=100): x = init_x for i in range(step_num): grad = numerical_gradient(f,x) x-= lr*grad return x . f = 최적화하려는 함수 | init_x = 초깃값 | lr = 학습률 | step_num = 경사법에 따른 반복횟수 | . def function_2(x): return x[0]**2 +x[1]**2 init_x = np.array([-3.0,4.0]) gradient_descent(function_2,init_x=init_x,lr =0.1, step_num=100) . array([-6.11110793e-10, 8.14814391e-10]) . 초깃값을 (-3.0,4.0)으로 설정한 후 경사법을 사용해 최솟값 탐색을 시작한다. | 최종 결과는 거의 (0,0)에 가깝다. 실제로 진정한 최솟값을 (0,0)이므로 경사법으로 거의 정확한 결과를 얻은 것이다. | . . 학습률을 달리하여 해보자 | . init_x = np.array([-3.0,4.0]) gradient_descent(function_2,init_x=init_x,lr=10.0,step_num=100) . array([-2.58983747e+13, -1.29524862e+12]) . 너무 크게 했더니 큰 값으로 발산해버렸다 | . init_x = np.array([-3.0,4.0]) gradient_descent(function_2,init_x=init_x,lr=1e-10,step_num=100) . array([-2.99999994, 3.99999992]) . 너무 작게 했더니 거의 갱신하지 않고 끝났다 | . $ to$ 따라서 학습률을 적절히 설정하는 일은 중요하다 . 학습률과 같은 매개 변수를 하이퍼 파라미터라고 한다. 이는 가중치와 편향같은 신경망의 매개변수와는 성질이 다른 매개변수이다. 신경망의 가중치 매개변수는 훈련 데이터와 학습 알고리즘에 의해서 자동으로 획득되는 매개변수인 반면, 학습률 같은 하이퍼 파라미터는 사람이 직접 설정해야 하는 매개변수인 것이다. 일반적으로는 이 하이퍼파라미터들은 여러 후보 값 중에서 시험을 통해 가장 잘 학습하는 값을 찾는 과정을 거쳐야 한다. | . . 신경망에서의 기울기 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다. | . | . import sys, os sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정 import numpy as np from common.functions import softmax, cross_entropy_error from common.gradient import numerical_gradient class simpleNet: def __init__(self): self.W = np.random.randn(2,3) # 정규분포로 초기화 def predict(self, x): return np.dot(x, self.W) def loss(self, x, t): z = self.predict(x) y = softmax(z) loss = cross_entropy_error(y, t) return loss . simpleNet 클래스는 형상이 2x3인 가중치 매개변수 하나를 인스턴스 변수로 갖는다. | 메서드는 2개인데, 하나는 예측을 수행하는 predict이고 다른 하나는 손실 함수의 값을 구하는 loss이다. 여기서 x는 입력데이터, t는 정답 레이블이다. | . net = simpleNet() # 가중치 매개변수 print(net.W) x = np.array([0.6, 0.9]) p = net.predict(x) print(p) # 최댓값의 인덱스 print(np.argmax(p)) # 정답 레이블 t = np.array([0, 0, 1]) net.loss(x,t) . [[ 0.48129772 0.4062162 -0.31963495] [ 0.11586695 1.06101565 -0.79045592]] [ 0.39305889 1.19864381 -0.90319129] 1 . 2.5523095142122116 . 이어서 기울기를 구해보자 | . def f(W): return net.loss(x,t) dW = numerical_gradient(f, net.W) print(dW) . [[ 0.17086397 0.38239445 -0.55325842] [ 0.25629596 0.57359168 -0.82988764]] . 람다를 사용해 아래와 같이 구현할 수 있겠다 | . f = lambda w: net.loss(x, t) dW = numerical_gradient(f, net.W) print(dW) . [[ 0.17086397 0.38239445 -0.55325842] [ 0.25629596 0.57359168 -0.82988764]] . 신경망의 기울기를 구한 다음에는 경사법에 따라 가중치 매개변수를 갱신하기만 하면 된다. 다름 절에서는 2층 신경망을 대상으로 학습 과정 전체를 구현해보자 | 135p참고하기 | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/09/dl-Copy1.html",
            "relUrl": "/2022/02/09/dl-Copy1.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "교차 엔트로피 오차",
            "content": "데이터에서 학습한다 : 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다. | 이번 장에서는 신경망 학습(데이터로부터 매개변수의 값을 정하는 방법)에 대해 알아보고 파이썬으로 MNIST 데이터셋의 손글씨 숫자를 학습하는 코드를 구현해보자 | . . 데이터 주도 학습 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지녔다. 구체적 예 &gt; 가령 이미지에서 &#39;5&#39;라는 숫자를 인식하는 프로그램을 구현한다고 해보자. 주어진 데이터를 잘 활용해서 해결해야 한다. 이런 방법의 하나로서 이미지에서 특징을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법이 있다. 여기서 말하는 특징은 입력 데이터(입력 이미지)에서 본질적인 데이터(중요 데이터)를 정확하게 추출할 수 있도록 설계된 변환기를 가리킨다. 이미지의 특징은 보통 벡터로 기술한다. 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM,KNN등으로 학습할 수 있다. | 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 사람이 설계한다. | 즉, 특징과 기계학습을 활용한 접근에도 문제에 따라서는 사람이 적절한 특징을 생각해내야 하는 것이다. | 반면 신경망 방식은 사람이 개입하지 않는 블록 하나로 이루어진다. | 신경망은 이미지를 있는 그대로 학습한다. 신경망은 이미지에 포함된 중요한 특징까지도 기계가 스스로 학습한다. 따라서 종단간 기계학습이라고 딥러닝을 부르기도 한다. | . | . | . 신경망도 하나의 지표를 기준으로 최적의 매개변수 값을 탐색한다. 신경망 학습에서 사용하는 지표는 손실 함수라고도 한다. 이 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 오차제곱합과 교차 엔트로피 오차를 사용한다. | 오차 제곱합은 각 원소의 출력(추정 값)과 정답 레이블(참 값)의 차를 제곱한 후, 그 총합을 구한다. 파이썬으로 구현해보자 | . import numpy as np def sum_squares_error(y,t): return 0.5 * np.sum((y-t)**2) . 여기서 인수 y와 t는 넘파이 배열이다. | . t = [0,0,1,0,0,0,0,0,0,0] y = [0.1,0.05,0.6,0,0.05,0.1,0,0.1,0,0] sum_squares_error(np.array(y), np.array(t)) . 0.09750000000000003 . sum(y) # 해당 예는 정답도 2이고 예측도 2일 확률이 0.6으로 가장 높다고 했음 . 1.0 . y = [0.1,0.05,0.1,0,0.05,0.1,0,0.6,0,0] sum_squares_error(np.array(y),np.array(t)) . 0.5975 . sum(y) # 해당 예는 정답은 똑갑이 2이지만 예측은 6일 확률이 0.6으로 가장 높다고 했음 . 1.0 . 즉, 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있다. | 오차제곱합 기준으로는 첫 번째 추정 결과가 (오차가 더 작으니) 정답에 더 가까울 것으로 판단할 수 있다. | . . 교차 엔트로피 오차 실질적으로 정답일 때의 추정의 자연로그를 계산하는 식이 된다. | 즉, 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정하게 된다. | 정답에 해당하는 출력이 커질수록 0에 다가가다가, 그 출력이 1일 때, 0이 된다. | 반대로 정답일 때의 출력이 작아질수록 오차는 커진다. | . | . import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&#39;ignore&#39;) x=np.arange(0,1,0.01) plt.plot(x,np.log(x)) plt.title(&#39;y=log(x)&#39;) . Text(0.5, 1.0, &#39;y=log(x)&#39;) . def cross_entropy_error(y,t): delta = 1e-7 # np.log 함수에 0을 입력하면 마이너스 무한대를 뜻하는 -inf가 되어 더 이상 계산할 수 없기에 아주 작은 값인 delta를 더해줌. return -np.sum( t*np.log(y+delta)) . t = [0,0,1,0,0,0,0,0,0,0] y = [0.1,0.05,0.6,0,0.05,0.1,0,0.1,0,0] cross_entropy_error(np.array(y),np.array(t)) . 0.510825457099338 . y = [0.1,0.05,0.1,0,0.05,0.6,0,0.1,0,0] cross_entropy_error(np.array(y),np.array(t)) . 2.302584092994546 . 첫 번째 예는 정답일 때의 출력이 0.6인 경우로 이때의 교차 엔트로피 오차는 약 0.51이다. | 다음은 정답일 때의 출력이 0.1인 경우로 이 때의 교차 엔트로피 오차는 무려 2.3이다. | 즉, 결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단한 것으로, 앞서 오차제곱합의 판단과 일치한다. | . . 지금까지는 데이터 하나에 대한 손실 함수만 생각했으니, 이제 훈련 데이터 모두에 대한 손실함수의 합을 구하는 방법에 대해 생각해보자 | 빅데이터 수준의 수백만, 수천만개의 수준에서는 데이터 일부를 추려 전체의 근사치로 이용할 수 있다. 이 일부를 미니배치라고 한다. 가령 60000장의 훈련 데이터 중에서 100장을 무작위로 뽑아 그 100장 만을 사용하여 학습하는 것이다. 이러한 학습을 미니배치 학습이라고 한다. | 미니배치 학습을 구현하는 코드를 작성해보자 | . | . import sys, os sys.path.append(os.pardir) from dataset.mnist import load_mnist (x_train,t_train),(x_test, t_test) = load_mnist(normalize=True, one_hot_label=True) print(x_train.shape) print(t_train.shape) . (60000, 784) (60000, 10) . 이 훈련 데이터에서 무작위로 10장만 빼내려면 어떻게 하면 될까. | . train_size = x_train.shape[0] batch_size = 10 batch_mask = np.random.choice(train_size, batch_size) x_batch = x_train[batch_mask] t_batch = t_train[batch_mask] . np.random.choice(60000,10) . 의미 : 0이상 60000 미만의 수 중에서 무작위로 10개 골라낸다 | . 이제 무작위로 선택한 이 인덱스를 사용해 미니배치를 뽑아내기만 하면 된다. 손실함수도 이 미니배치로 계산한다. | . . 미니배치 같은 배치 데이터를 지원하는 교차 엔트로피 오차는 어떻게 구현할까 | 아래 셀은 데이터가 하나인 경우와 데이터가 배치로 묶여 입력된 경우 모두를 처리할 수 있도록 구현한 것이다. | . def cross_entropy_error(y,t): # y는 신경망의 출력, t는 정답레이블 if y.dim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum( t * np.log(y + 1e-7)) / batch_size . y(신경망의 출력)가 1차원이라면, 즉 데이터 하나당 교차 엔트로피 오차를 구하는 경우는 reshape함수로 데이터의 형상을 바꿔준다. | 정답 레이블이 원-핫 인코딩이 아니라 &#39;2&#39;나 &#39;7&#39;등의 숫자 레이블로 주어졌을 때의 교차 엔트로피 오차는 다음과 같이 구현할 수 있다. | . def cross_entropy_error(y,t): # y는 신경망의 출력, t는 정답레이블 if y.dim == 1: t = t.reshape(1, t.size) y = y.reshape(1, y.size) batch_size = y.shape[0] return -np.sum( np.log(y[np.arange(batach_size),t] + 1e-7)) / batch_size . 이 구현에서는 원-핫 인코딩일 때 t가 0인 원소는 교차 엔트로피 오차도 0이므로, 그 계산은 무시해도 좋다는 것이 핵심이다. | 다시 말하면 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있다. | . .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/08/dl.html",
            "relUrl": "/2022/02/08/dl.html",
            "date": " • Feb 8, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "소프트맥스, MNIST Data Set",
            "content": "소프트맥스 함수 분류에서 사용한다. | 소프트맥스의 출력은 모든 입력 신호로부터 화살표를 받는다. | 소프트맥스의 함수의 분모에서 볼 수 있듯, 촐력층의 각 뉴런이 모든 입력 신호에서 영향을 받기 때문이다. | 이상의 소프트맥스 함수를 구현해보자 | . | . import numpy as np a = np.array([0.3,2.9,4]) exp_a = np.exp(a) print(exp_a) sum_exp_a = np.sum(exp_a) print(sum_exp_a) y = exp_a / sum_exp_a print(y) . [ 1.34985881 18.17414537 54.59815003] 74.1221542101633 [0.01821127 0.24519181 0.73659691] . C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy _distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs: C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll warnings.warn(&#34;loaded more than 1 DLL from .libs:&#34; . 위 논리 흐름을 파이썬 함수로 정의하자 | . def softmax(a): exp_a = np.exp(a) sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y . 소프트맥스 함수 구현시 주의할 점 위에서 정의한 softmax() 함수의 코드는 컴퓨터로 계산할 때 결함이 있다. 바로 오버플로 문제이다. 소프트맥스 함수는 지수 함수를 사용하는데, 지수함수란 것이 쉽게 아주 큰 값을 내지만 컴퓨터는 다룰 수 있는 범위가 한정되어 있어 수치가 커짐에 따라 결과 수치가 불안정해질 수 있다. | 해결책 : 오버플로를 막기 위해 입력 신호 중 최댓값을 이용하여 지수 함수에서 빼준다. | 구현해보자 | . | . | . import warnings warnings.filterwarnings(&#39;ignore&#39;) a = np.array([1010,1000,990]) print(np.exp(a) / np.sum(np.exp(a))) c = np.max(a) np.exp(a-c) / np.sum(np.exp(a-c)) . [nan nan nan] . array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09]) . 위에서 볼 수 있듯, 아무런 조치 없이 그냥 계산하면 nan이 반환된다. | 하지만 입력 신호 중 최댓값을 빼주면 올바르게 계싼할 수 있다. | 이를 바탕으로 소프트맥스 함수를 다시 구현해보자 | . def softmax(a): c = np.max(a) exp_a = np.exp(a - c) # 이게 오버플로 대책이다. sum_exp_a = np.sum(exp_a) y = exp_a / sum_exp_a return y . 소프트맥스 함수의 특징 | . a = np.array([0.3,2.9,4]) y = softmax(a) print(y) print(np.sum(y)) . [0.01821127 0.24519181 0.73659691] 1.0 . 즉, 소프트맥스 함수의 출력은 0에서 1사이의 실수이다. (그럴 수 밖에 없는 것이 소프트맥스 함수를 살펴보면 1을 넘을 수 없다) | 출력의 총합은 1이며 소프트맥스 함수의 중요한 성질이 된다. 이 성질 덕분에 소프트맥스 함수의 출력을 확률로 정의할 수 있다. | 즉, 소프트맥스 함수를 이용함으로써 문제를 확률적(통계적)으로 대응할 수 있게 되는것이다. | . | . . 출력층의 뉴런 수 정하기 출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야 한다. | 분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적이다. 예를 들어 입력 이미지를 숫자 0부터 9 중 하나로 분류하는 문제라면 출력층의 뉴런을 10개로 설정한다. | . | . | . 손글씨 숫자 인식 이번 절에서는 이미 학습된 매개변수(가중치, 편향(임계값))를 사용하여 학습 과정은 생략하고 추론 과정만 구현한다. | 이 추론 과정을 신경망의 순전파(forward propagation)라고도 한다. 머신러닝과 마찬가지로 신경망도 두 단계를 거쳐 문제를 해결한다. 먼저 훈련 데이터 즉, 학습 데이터를 사용해 가중치 매개변수를 학습하고 추론 단계에서는 앞서 학습한 매개변수를 사용하여 입력 데이터를 분류한다. | . | . | . MNIST data set 손글씨 숫자 이미지 집합 | MNIST의 이미지 데이터는 28 $ times$ 28 크기의 회색조 이미지이며 각 픽셀은 0에서 255까지의 값을 취한다. 각 이미지에는 또한 7,2,1과 같이 그 이미지가 실제 의미하는 숫자가 레이블로 붙어 있다. | . | . import sys,os sys.path.append(os.pardir) # 부모 디렉토리의 파일을 가져올 수 있도록 설정 from dataset.mnist import load_mnist (x_train, t_train),(x_test, t_test) = load_mnist(flatten = True, normalize = False) # 각 데이터 형상 출력해보자 print(x_train.shape) print(t_train.shape) print(x_test.shape) print(t_test.shape) . (60000, 784) (60000,) (10000, 784) (10000,) . 첫 번째 인수인 normalize는 입력 이미지의 픽셀값을 0~1 사이의 값으로 정규화할지를 정한다. | flatten은 입력 이미지를 평탄하게, 즉 1차원 배열로 만들지를 정한다. False로 설정하면 입력 이미지를 1 x 28 x 28의 3차원 배열로, True로 설정하면 784개의 원소로 이루어진 1차원 배열로 저장한다. | . | . . 참고) 파이썬에는 pickle이라는 편리한 기능이 있다. 이는 프로그램 실행 중에 특정 객체를 파일로 저장하는 기능이다. 저장해준 pickle 파일을 로드하면 실행 당시의 객체를 즉시 복원할 수 있다. MNIST 데이터셋을 읽는 load_mnist() 함수에서도 (2 번째 이후의 읽기 시에) pickle을 이용한다. pickle 덕분에 MNIST 데이터를 순식간에 준비할 수 있는 것이다. | . | . . MNIST 이미지를 화면으로 불러보도록 하자. | . import sys, os sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 설정 import numpy as np from dataset.mnist import load_mnist from PIL import Image def img_show(img): pil_img = Image.fromarray(np.uint8(img)) pil_img.show() (x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False) img = x_train[0] label = t_train[0] print(label) # 5 print(img.shape) # (784,) img = img.reshape(28, 28) # 형상을 원래 이미지의 크기로 변형 print(img.shape) # (28, 28) img_show(img) . 5 (784,) (28, 28) . 주의 flatten=True로 설정해 읽어 들인 이미지는 1차원 넘파이 배열로 저장돼 있다. 그래서 이미지를 표시할 땐 원래 형상인 28 x 28 크기로 다시 변형해야 한다. reshape() 메서드에 원하는 형상을 인수로 지정하면 넘파이 배열의 형상을 바꿀 수 있다. | 또한 넘파이로 저장된 이미지 데이터를 PIL용 데이터 객체로 변환해야 하며, 이 변환은 Image.fromarray()가 수행한다. | . | . . 신경망의 추론 처리 드디어 이 MNIST 데이터셋을 가지고 추론을 수행하는 신경망을 구현해보자 | 이 신경망은 입력층 뉴런을 784(28x28)개, 출력층 뉴련을 10개로 구성한다. | 한편, 은닉층은 총 두 개로, 첫 번째 은닉층에는 50개의 뉴런을 두 번째 은닉층에는 100개의 뉴런을 배치한다. 여기서 50과 100은 임의로 정한 값이다. | . | . import sys, os sys.path.append(os.pardir) # 부모 디렉터리의 파일을 가져올 수 있도록 import numpy as np import pickle from dataset.mnist import load_mnist from common.functions import sigmoid, softmax def get_data(): (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False) return x_test, t_test def init_network(): with open(&quot;sample_weight.pkl&quot;, &#39;rb&#39;) as f: network = pickle.load(f) return network def predict(network, x): w1, w2, w3 = network[&#39;W1&#39;], network[&#39;W2&#39;], network[&#39;W3&#39;] b1, b2, b3 = network[&#39;b1&#39;], network[&#39;b2&#39;], network[&#39;b3&#39;] a1 = np.dot(x, w1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, w2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, w3) + b3 y = softmax(a3) return y . init_network()에서는 pickle 파일인 sample_weight.pkl에 저장된 학습된 가중치 매개변수를 읽는다. 이 파일에는 가중치와 편향 매개변수가 딕셔너리 변수로 저장되어 있다. | 이제 이 세 함수를 이용해 신경망에 의한 추론을 수행해보고, 정확도(분류가 얼마나 올바른가)도 평가해보자 | . x, t = get_data() network = init_network() batch_size = 100 # 배치 크기 accuracy_cnt = 0 for i in range(0, len(x), batch_size): x_batch = x[i:i+batch_size] y_batch = predict(network, x_batch) p = np.argmax(y_batch, axis=1) accuracy_cnt += np.sum(p == t[i:i+batch_size]) print(&quot;Accuracy:&quot; + str(float(accuracy_cnt) / len(x))) . Accuracy:0.9352 . 또한 이 예에서는 load_mnist 함수의 인수인 normalize를 True로 설정했다. 이처럼 데이터를 특정 범위로 변환하는 처리를 정규화라 하고, 신경망의 입력 데이터에 특정 변환을 가하는 것을 전처리라 한다. 여기에서는 입력 이미지 데이터에 대한 전처리 작업으로 정규화를 수행한 셈이다. | . . 배치처리 입력 데이터와 가중치 매개변수의 형상에 주의하여 조금 전 구현을 다시 살펴보자 | 우선 앞서 구현한 신경망 각 층의 가중치 형상을 출력해보자 | . | . x,_ = get_data() network = init_network() W1,W2,W3 = network[&#39;W1&#39;], network[&#39;W2&#39;], network[&#39;W3&#39;] print(x.shape) print(x[0].shape) print(W1.shape) #1 print(W2.shape) #2 print(W3.shape) #3 . (10000, 784) (784,) (784, 50) (50, 100) (100, 10) . 이 결과에서 다차원 배열의 대응하는 차원의 수가 일치함을 확인할 수 있다. (교재 102p를 참고해보자) | . | 이는 이미지 데이터 1장만 입력했을 때의 데이터 처리 흐름이다. 그렇다면 이미지 여러 장을 한 번에 입력하는 경우를 생각해보자 가령 이미지 100개를 묶어 predict() 함수에 한 번에 넘기는 경우가 있다고 해보자. | x의 형상을 100x784fh qkRNjtj 100장 분량의 데이터를 하나의 입력 데이터로 표현하면 될 것이다. | 이때 입력 데이터의 형상은 100x784, 출력 데이터의 형상은 100x10이 된다. | 이는 100장 분량의 입력 데이터의 결과가 한 번에 출력됨을 나타낸다. | 가령 x[0]과 y[0]에는 0번째 이미지와 그 추론 결과가 저장되는 식이다. | 이처럼 하나로 묶은 입력 데이터를 배치(batch)라고 한다. 즉, 묶음을 의미하며 이미지가 지폐처럼 다발로 묶여 있다고 생각하면 된다. | . | . | . 배치 처리를 구현해보자 | . x, t = get_data() network = init_network() batch_size = 100 # 배치 크기 accuracy_cnt = 0 for i in range(0, len(x), batch_size): x_batch = x[i:i+batch_size] y_batch = predict(network, x_batch) p = np.argmax(y_batch, axis=1) # 100x10이라는 다차원에서 0번째 차원말고 1번째 차원에 접근하겠다. #?????????????? accuracy_cnt += np.sum(p == t[i:i+batch_size]) print(&quot;Accuracy:&quot; + str(float(accuracy_cnt) / len(x))) . Accuracy:0.9352 . . x = np.array([[3,2,1],[4,5,6]]) y = np.argmax(x,axis=1) print(y) . [0 2] . . 마지막으로 배치 단위로 분류한 결과를 실제 답과 비교한다. 이를 위해 ==연사자를 사용한다. | . y = np.array([1,2,1,0]) t = np.array([1,2,0,0]) print(y==t) np.sum(y==t) . [ True True False True] . 3 . 이런 식으로 구현하면 된다. | . . Conclusion . 이번 장에서는 신경망의 순전파를 살펴봤다. 이번 장에서 설명한 신경망은 각 층의 뉴런들이 다음 층의 뉴런으로 신호를 전달한다는 점에서 앞 장의 퍼셉트론과 같다. 하지만 다름 뉴런으로 갈 때, 신호를 변화시키는 활성화 함수에 큰 차이가 있었다. 신경망에서는 매끄럽게 변화하는 시그모이드 함수를, 퍼셉트론에서는 갑자기 변화하는 계단 함수를 활성화 함수로 사용했다. 이 차이가 신경망 학습에 중요하다. 다음 장에서 더욱 알아보도록 하자. .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/07/dl.html",
            "relUrl": "/2022/02/07/dl.html",
            "date": " • Feb 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "신경망",
            "content": "신경망을 통해 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 이제부터 살펴볼 신경망의 중요한 성질이다. | 신경망의 개요를 알아보고, 신경망이 입력 데이터가 무엇인지 식별하는 처리 과정을 자세히 알아보자. | . 퍼셉트론에서 신경망으로 . 입력층, 은닉층, 출력층으로 구성된다. | 이 중 은닉층의 뉴런은 사람 눈에는 보이지 않는다. | 뉴런이 연결되는 방식은 앞 장의 퍼셉트론과 달라진 것이 없다. | 차이점에 대해 알아보자 | . 활성화 함수 . 입력 신호의 총합을 출력 신호로 변환하는 함수를 일컫는다. | 활성화라는 이름이 말해주듯 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다. | 임계값을 경계로 출력이 바뀌는데 이런 함수를 계단 함수라고 한다. | 그래서 퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다라고 할 수 있다. | 즉 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 계단 함수를 채용하고 있다. | 그렇다면 계단 함수 이외의 함수를 사용하면 어떻게 될까. 사실 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠이다. | 신경망에서 이용하는 활성화 함수를 알아보자 | . 신경망에서 자주 이용하는 활성화 함수인 시그모이드 함수를 알아보자 $ frac{1}{1+exp(-x)}$ | . | . 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다. | 앞 장에서본 퍼셉트론과 앞으로 볼 신경망의 주된 차이는 이 활성화 함수이다. | 계단 함수를 구현해보자 | . import numpy as np def step_function(x): if x&gt;0: return 1 else : return 0 . 인수 x는 실수만 받아들인다. 즉 step_function(3.0) 같은 것은 가능하지만 step_function(np.array([1,2]))는 안 된다. | 가능하도록 해보자 | . def step_function(x): y = x &gt; 0 return y.astype(np.int) . 아래 셀을 통해 위 함수를 이해해보자 | . x = np.array([-1,1,2]) y = x&gt;0 y . array([False, True, True]) . 이렇게 넘파이 배열에 부등호 연산을 수행하면 배열의 원소 각각에 부등호 연산을 수행한 bool 배열이 새로 생성되고 y라는 변수에 저장된다. | 그런데 우리가 원하는 계단 함수는 0이나 1의 int형을 출력하는 함수이기 때문에 아래 셀을 통해 변환하여 반환해주자 | . import warnings warnings.filterwarnings(&quot;ignore&quot;) y = y.astype(np.int) y . array([0, 1, 1]) . 이제 앞에서 정의한 계단 함수를 그래프로 그려보자 | . import matplotlib.pyplot as plt def step_function(x): return np.array(x&gt;0 , dtype=np.int) x = np.arange(-5,5,0.1) y = step_function(x) plt.plot(x,y) plt.ylim(-0.1,1.1) plt.show() . 시그모이드 함수를 구현해보자 | . def sigmoid(x): return 1/(1 + np.exp(-x)) x = np.array([-1,1,2]) sigmoid(x) . array([0.26894142, 0.73105858, 0.88079708]) . 시그모이드 함수를 그래프로 그려보자 | . x = np.arange(-5, 5, 0.1) y = sigmoid(x) plt.plot(x,y) plt.ylim(-0.1,1.1) plt.title(&#39;Sigmoid Function&#39;) plt.show() . 잡담 : 시그모이드란 &#39;S&#39;자 모양이라는 뜻이다. 계단 함수처럼 그 모양을 따 이름을 지은 것이다. &#39;S자 모양 함수&#39;라고도 부를 수 있다. | . . fig, (ax1,ax2) = plt.subplots(1,2) x = np.arange(-5,5,0.1) y = step_function(x) ax1.plot(x,y) ax1.set_title(&#39;Step Function&#39;) x = np.arange(-5, 5, 0.1) y = sigmoid(x) ax2.plot(x,y) ax2.set_title(&#39;Sigmoid Function&#39;) plt.show() . 비교 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변한다. 한편 계단 함수는 0을 경계로 출력이 갑자기 바뀌어버린다. 시그모이드 함수의 이 매끈함이 신경망 학습에서 아주 중요한 역할을 한다. | 다시 말해 퍼셉트론에서는 뉴런 사이에 0 혹은 1이 흘렀다면 신경망에서는 연속적인 실수가 흐른다. | 공통점 : 둘 다 입력이 작을 때의 출력은 0에 가깝고 혹은 0이고 입력이 커지면 출력이 1에 가까워지는 혹은 1이 되는 구조이다. | 즉, 계단 함수와 시그모이드 함수는 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다. | 그리고 입력이 아무리 작거나 커도 출력은 0에서 1 사이라는 것도 둘의 공통점이다. | 또한 둘다 비선형 함수이다. | . | . 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문이다. | . . 시그모이드 함수는 신경망 분야에서 오래전부터 이용해왔으나 최근에는 ReUL(Rectified Linear Unit) 렐루 함수를 주로 이용한다. | 렐루 함수는 입력이 0을 넘으면 그 입력을 그대로 출력하고 0 이하이면 0을 출력하는 함수이다. | . def relu(x): return np.maximum(0,x) x = np.arange(-6 ,6 ,0.1) y = relu(x) plt.plot(x,y) plt.ylim(-1,7) plt.show() . ReLU 함수도 활성화 함수로 이용할 수 있다. | . . 넘파이의 다차원 배열에 대해 간략히 알아보자 | . b = np.array([[1,2],[3,4],[5,6]]) b . array([[1, 2], [3, 4], [5, 6]]) . print(np.ndim(b)) print(np.shape(b)) . 2 (3, 2) . $3 times 2$ 배열이다. | 처음 차원에는 원소가 3개 다음 차원에는 원소가 2개 들어있다는 의미이다. | 이때 처음 차원은 0번째 차원, 다음 차원은 1번째 차원에 대응한다. 2차원 배열은 행렬이라고 부르고 가로방향을 행, 세로방향을 열이라고 부른다. | . | . np.dot() . 은 입력이 1차원 배열이면 벡터를, 2차원 배열이면 행렬 곱을 계산한다. . 주의할 것은 np.dot(A,B)와 np.dot(B,A)는 다른 값이 될 수 있다.(우연의 일치가 발생할 순 있다.) . 행렬의 곱은 *과는 다르다 | . . a = np.array([2,3]) # 입력 값 b = np.array([[1,2,3],[4,5,6]]) # 가중치 # 편향과 활성화 함수를 생략하고 가중치만 갖는 간단한 신경망이다. np.dot(a,b) . array([14, 19, 24]) . . def identity_function(x): return x def init_network(): network = {} # dict 형태로 이용할 것 network[&#39;W1&#39;] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]]) network[&#39;b1&#39;] = np.array([0.1,0.2,0.3]) network[&#39;W2&#39;] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]]) network[&#39;b2&#39;] = np.array([0.1,0.2]) network[&#39;W3&#39;] = np.array([[0.1,0.3],[0.2,0.4]]) network[&#39;b3&#39;] = np.array([0.1,0.2]) return network def forward(network, x): W1,W2,W3 = network[&#39;W1&#39;],network[&#39;W2&#39;],network[&#39;W3&#39;] b1,b2,b3 = network[&#39;b1&#39;],network[&#39;b2&#39;],network[&#39;b3&#39;] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) # 1층 a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) # 2층 a3 = np.dot(z2,W3) + b3 y = identity_function(a3) # 3층 return y network = init_network() x = np.array([1,0.5]) y = forward(network, x) print(y) . [0.31682708 0.69627909] . network . {&#39;W1&#39;: array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]), &#39;b1&#39;: array([0.1, 0.2, 0.3]), &#39;W2&#39;: array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]), &#39;b2&#39;: array([0.1, 0.2]), &#39;W3&#39;: array([[0.1, 0.3], [0.2, 0.4]]), &#39;b3&#39;: array([0.1, 0.2])} .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/05/dl.html",
            "relUrl": "/2022/02/05/dl.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "퍼셉트론",
            "content": "퍼셉트론을 구현해보자 AND 게이트 구현 | . | . def AND(x1, x2): w1,w2,theta = 0.5,0.5,0.7 tmp = x1*w1+x2*w2 if tmp&lt;=theta: return 0 elif tmp&gt;theta: return 1 print(AND(0,0)) print(AND(1,0)) print(AND(0,1)) print(AND(1,1)) . 0 0 0 1 . 물론 앞에서 구현한 AND 게이트는 직관적이고 알기 쉽지만, 앞으로를 생각해서 다른 방식으로 수정해보자. 편향이라는 개념을 도입하여, 퍼셉트론은 입력신호에 가중치를 곱한 값과 편향을 더하여, 그 값이 0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력한다. | . | . import numpy as np x = np.array([0,1]) # 입력 w = np.array([0.5,0.5]) # 가중치 b = -0.7 # 편향 print(w*x) print(np.sum(w*x)) print(np.sum(w*x)+b) . [0. 0.5] 0.5 -0.19999999999999996 . . 가중치와 편향을 도입한 AND 게이트는 다음과 같이 구현할 수 있다. | . def AND(x1,x2): x = np.array([x1,x2]) w = np.array([0.5,0.5]) b = -0.7 tmp = np.sum(x*w) + b if tmp&lt;=0 : return 0 elif tmp &gt;0 : return 1 . w1,w2는 각 입력 신호가 결과에 주는 영향력을 조절하는 매개변수이고, 편향을 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수이다. | . def NAND(x1,x2): x = np.array([x1,x2]) w = np.array([-0.5,-0.5]) b = 0.7 tmp = np.sum(x*w) + b if tmp&lt;=0 : return 0 elif tmp &gt;0 : return 1 def OR(x1,x2): x = np.array([x1,x2]) w = np.array([0.5,0.5]) b = -0.2 tmp = np.sum(x*w) + b if tmp&lt;=0 : return 0 elif tmp &gt;0 : return 1 . AND, NAND, OR 모두 같은 구조의 퍼셉트론이고 차이는 가중치 매개변수의 값뿐이다. 실제로 파이썬으로 작성한 NAND와 OR 게이트의 코드에서도 AND와 다른 곳은 가중치와 편향 값을 설정하는 부분뿐이다. | . . XOR 게이트 배타적 논리합이라는 논리회로이다. 한쪽이 1일때만 1을 출력. OR게이트와 달리 둘 다 1일때는 출력하지 않는다. | 지금까지 본 퍼셉트론의 구조로는 이 XOR 게이트를 구현할 수 없다. | . | . import matplotlib.pyplot as plt plt.scatter([0,0,1,1],[0,1,0,1]) plt.xlim(0,2) plt.ylim(0,2) . (0.0, 2.0) . (0,0)(1,1),(1,0)(0,1)이 두 묶음을 각각 나눌 수 있는 직선은 있을 수 없다. | 하지만 직선이라는 제약을 없앤다면 가능하다 | 퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가 있다. 곡선은 표현할 수 없다. | 퍼셉트론으로는 XOR 게이트를 표현할 수 없지만 층을 쌓아 다층 퍼셉트론을 통해 구현할 수 있다. | . . AND, OR NAND 게이트를 조합하여 XOR게이트를 만들 수 있다. | 단층 퍼셉트론으로는 XOR 게이트를 표현할 수 없다. 단층 퍼셉트론으로는 비선형 영역을 분리할 수 없다. | 퍼셉트론을 조합하여 즉, 층을 쌓아서 XOR 게이트를 구현하는 것이다. | . def XOR(x1,x2): s1 = NAND(x1,x2) s2 = OR(x1,x2) y = AND(s1,s2) return y . print(XOR(0,0)) print(XOR(0,1)) print(XOR(1,0)) print(XOR(1,1)) . 0 1 1 0 . 이처럼 퍼셉트론은 층을 쌓아 더 다양한 것을 표현할 수 있다. | . . Conclusion . 퍼셉트론은 입출력을 갖춘 알고리즘이다. 입력을 주면 정해진 규칙에 따른 값을 출력한다. | 퍼셉트론에서는 가중치와 편향을 매개변수로 설정한다. | 퍼셉트론으로 AND,OR 게이트 등의 논리 회로를 표현할 수 있다. | XOR 게이트는 단층 퍼셉트론으로는 표현할 수 없다. | 2층 퍼셉트론을 이용하면 XOR 게이트를 표현할 수 있다. | 단층 퍼셉트론은 직선형 영역만 표현할 수 있고, 다층 퍼셉트론은 비선형 영역도 표현할 수 있다. | 다층 퍼셉트론은 (이론상) 컴퓨터도 표현할 수 있다. | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/04/dl.html",
            "relUrl": "/2022/02/04/dl.html",
            "date": " • Feb 4, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "2022/01/29/SAT",
            "content": "Softmax Classfication Logistic Regression의 연장선에 있다고 볼 수 있다. | . | . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # 재현성을 위하여 torch.manual_seed(1) . &lt;torch._C.Generator at 0x208d7a60b70&gt; . Discrete Probability Distribution 이산적인 확률 분포를 말한다. 확률 분포에는 연속적인 확률 분포와 이산적인 확률 분포가 있다. | 가령 우리가 주사위를 던져서 주사위가 6개중 하나의 숫자가 나오게 되는데 이러한 경우 혹은 가위바위보 이런 경우를 이산적인 확률 분포라고 한다. | 주사위의 PMF(Probability Mass Function) 확률질량함수는 아래와 같다. Uniform Distribution | | . | 가위 바위 보도 동일하게 정의할 수 있을 것이다. Uniform Distribution | | . | 일반적으로 알고 있는 확률분포함수pdf에서는 | | 위와 같이 생길 수 있는데 면적이 확률을 의미한다. 어떤 지점 자체는 확률을 의미하지 않는다. 알 수도 없다. | discrete에서는 지점의 확률을 구할 수 있다. | 그래서 이러한 이산적인 확률 분포를 바탕으로 머신러닝을 수행한다고 생각해보자 | 가위바위보 상황에서 이전이 상대방이 낸 것을 바탕으로 해서 다음에 상대방이 낼 것을 예측할 수 있을 것이다. 완전히 랜덤은 아닐 것이고 사람마다 일정한 어떤 패턴이 있을 것이다. 즉 확률 분포함수가 있다는 것이다. 예전에 철수가 가위를 냈을 떄 다음에 주먹을 낼 확률은 얼마? 이런 것이다. 이렇게 정의할 수 있을 것이다. | P(주먹 | 가위) = ? 혹은 P(가위 | 가위) 혹은 P(보 | 가위), 이 세가지를 알 수 있다면 상대방이 가위를 냈을 떄 다음에 무엇을 낼 지 예측할 수 잇을 것이다. | 분명히 이런 패턴의 확률 분포가 있을 것이고 이 확률 분포를 근사해야한다. | . | softmax라는 함수가 있는데 말 그대로 max값을 뽑아주는데 soft하게 뽑아준다. | softmax | | 기존에 max를 뽑는다면.. 아래를 살펴보자 | . | . z = torch.FloatTensor([1,2,3]) . 위와 같이 주어졌을 떄, argmax 값은 max=(0,0,1)이 됐을 것이다. | 그런데 softmax는 가볍게 뽑아준다. 부드럽게 뽑아준다. 즉, 위 처럼 (0,0,1) 대신에 합쳐서 1이 되는,, 비율에 따라서 나타내 줄 수 있겠다. | 확률 값으로 볼 수도 있겠다. 아래를 살펴보자 | . hypothesis = F.softmax(z, dim=0) print(hypothesis) . tensor([0.0900, 0.2447, 0.6652]) . 각각의 이 세 값들은 위 사진 수식에 따라서 활용이 될 것이다. | 예를 들어 첫 번쨰 값인 0.09같은 경우에는 | | 이와 같이 구해졌을 것이다. | . hypothesis.sum() . tensor(1.) . softmax값은 1이 된다. | 우리는 이를 이용해 철수가 가위를 냈을 때 다음에 어떤 것을 낼지 확률 분포를 근사할 수 있겠다, | . . Cross Entropy 이러한 두개의 확률 분포가 주어졌을 때 두 확률 분포가 얼마나 비슷한지를 나타내는 수치라고 볼 수 있다 | 수식을 살펴보자 | | 좀 더 직관적으로 나타내보자 | | 예를 들어서 맨 왼쪽에 있는 분포가 P 가운데가 Q_1 맨 오른쪽에 있는 분포가 Q_2라고 했을 때, P에서 샘플링한다는 (사진상에서 E밑에있는 P가 P에서 샘플링한다는 것을 의미함) 것은 P에 해당되는 density대로 샘플링이 되겠다. | 예를 들어 사진상에서 빨간동그라미에서 점이 뽑혔다면, 그 위에서 Q_1과 Q_2에 해당될 것이다. | 즉, 우리는 이 log를 취하고 -를 붙여줬기 때문에, Q_1의 값이 Q_2의 값보다 훨씬 크게 될 것이다. | | 따라서 만약에 철수가 가위를 냈을 때 다음에 무엇을 낼지에 대한 확률 분포함수가 있을 것인데 그것을 P라고 했을 떄, 우리는 이 cross entropy를 구해서 cross entropy 이것을 최소화하도록 하면 Q_2에서 Q_1으로 그리고 P로 다가갈 수 있을 것이다. | 그래서 우리가 가지고 있는 모델의 확률 분포함수는 점점 P에 근사하게 될 것이다. | 따라서 cross entropy를 최소화하는 것이 중요하다. | . | . corss entropy loss 손실함수를 계산해보자 cross entropy는 아래와 같이 수식으로 계산할 수 있겠다. | | . | . z = torch.rand(3 ,5 ,requires_grad = True) hypothesis = F.softmax(z,dim=1) # 두 번째 행에 대해서 softmax를 수행하라. print(hypothesis) . tensor([[0.2441, 0.1429, 0.2298, 0.2344, 0.1487], [0.1665, 0.2504, 0.2309, 0.1707, 0.1815], [0.2733, 0.1576, 0.2292, 0.2147, 0.1252]], grad_fn=&lt;SoftmaxBackward0&gt;) . 이것이 사실은 예측값 즉 yhat이 될 것이다. | 정답이 뭔지 알아보자(지금은 따로 정답이 없기 때문에 랜덤으로 정답을 생성해보자) | . y = torch.randint(5, (3,)).long() print(y) . tensor([1, 0, 0]) . 우리는 각각의 샘플에 대해서 정답 인덱스를 구했다고 볼 수 있다. | 첫 번째 행에서 1인덱스 두 번째 행에서 0인덱스 세 번째 행에서 0인덱스를 의미한다. | . one hot vector로 나타내보자 | . y_one_hot = torch.zeros_like(hypothesis) y_one_hot.scatter_(1, y.unsqueeze(1),1) # tensor([1,0,0]) . tensor([[0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.]]) . cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean() print(cost) . tensor(1.6784, grad_fn=&lt;MeanBackward0&gt;) . sim(dim=1)은 3 x 1이 남을 것이다. | . . Cross-entropy Loss with torch.nn.functional | . torch.log(F.softmax(z, dim=1)) # low lovel . 을 생략하고 아래와 같이 이용할 수 있겠다 . | . F.log_softmax(z, dim=1) . tensor([[-1.4101, -1.9457, -1.4703, -1.4507, -1.9056], [-1.7925, -1.3848, -1.4657, -1.7679, -1.7066], [-1.2971, -1.8475, -1.4731, -1.5385, -2.0782]], grad_fn=&lt;LogSoftmaxBackward0&gt;) . 한 번 더 간편하게 이용할 수 있다. | NLL (Negative Log Likelihood) | . F.nll_loss(F.log_softmax(z,dim=1),y) . tensor(1.6784, grad_fn=&lt;NllLossBackward0&gt;) . 위 값과 동일함을 알 수 있다. | 더 단순하게 수행할 수 잇다. | . F.cross_entropy(z,y) . tensor(1.6784, grad_fn=&lt;NllLossBackward0&gt;) . 값이 동일함을 알 수 있다. | 필요에 따라서 원하는 값을 사용하면 된다. | . . Training with Low-level Cross Entripy Loss | 손실함수를 통해서 직접 최적화, 학습을 해보자 | . x_train = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7]] y_train = [2,2,2,1,1,1,0,0] x_train = torch.FloatTensor(x_train) y_train = torch.FloatTensor(y_train) . 이와 같이 학스부data가 주어져있다고 해보자. | x_train이 m x 4 라고 해보고 그렇다면 y_train도 m개가 있을 것이다. 즉 4차원의 벡터를 받아서 어떤 클래스인지 예측하도록 하고싶다. | 여기서 y_train은 one-hot벡터로 나타냈을 때, 1이 있는 위치에서의 인덱스 값일 것이다. | 코드를 작성해보자 | . # 모델 초기화 W = torch.zeros((4,3), requires_grad = True) b = torch. zeros(1, requires_grad = True) # optimizer 설정 optimizer = optim.SGD([W,b], lr=0.1) nb_epochs = 1000 for epoch in range(nb_epochs +1): # cost 계산(1) hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) y_one_hot = torch.zeros_like(hypothesis) y_one_hot.scatter_(1, y_train.unsqueeze(1),1) cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번 마다 로그 출력 if epoch7 % 100 == 0 : print(&#39;Epoch {:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) . 좀 더 쉽게 구현을 해보자 | Training with F.cross_entropy | . # 모델 초기화 W = torch.zeros((4,3), requires_grad = True) b = torch. zeros(1, requires_grad = True) # optimizer 설정 optimizer = optim.SGD([W,b], lr=0.1) nb_epochs = 1000 for epoch in range(nb_epochs +1): # cost 계산(2) z = x_train.matmul(W)+b cost = F.cross_entropy(z, y_train) # scatter를 사용할 필요가 없어짐 # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번 마다 로그 출력 if epoch7 % 100 == 0 : print(&#39;Epoch {:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) . 좀 더 실전에 가깝게 쉽게 구현해보자 | High level Implementation with nn.Module | . class SoftmaxClassifierModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(4,3) def forward(self, x) : return self.linear(x) model = SoftmaxClassifierModel() . # optimizer 설정 optimizer = optim.SGD(model.parameters(), lr=0.1) nb_epochs = 1000 for epoch in range(nb_epochs +1): # H(x) 계산 prediction = model(x_train) # cost 계산 cost = F.cross_entropy(prediction, y_train) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번 마다 로그 출력 if epoch7 % 100 == 0 : print(&#39;Epoch {:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/29/intro.html",
            "relUrl": "/2022/01/29/intro.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "2022/01/28/FRI",
            "content": "회귀 분석 회귀 분석은 데이터의 특성에 따라 선형 회귀와 비선형 회귀로 나눌 수 있다. | 데이터 분포에 따라 선형적 특성을 보이면 선형 회귀를 사용하고 데이터가 비선형적 특성을 보이면 비선형 회귀를 사용한다. | 이러한 방법을 사용하는 회귀분석의 목적은 컴퓨터가 보유한 데이터로부터 데이터의 특성을 스스로 학습하고 앞으로 데이터를 추정 및 예측하는 것이다. Linear Regression vs Logistic Regression | 두 가지 모두 회귀 분석의 종류이다. 즉, output의 형태만 다를 뿐이다. | 먼저 선형,비선형 회귀는 연속형 data를 input으로 하여 연속형 output을 추정하는 알고리즘이다. | 반면 Logistic Regression은 연속형 data를 input으로하여 이산형 output을 추정하는 알고리즘이다. | 즉 넣어주는 값의 형태는 동일하지만 도출되는 output의 형태가 다르다는 것이 두 알고리즘의 차이점이다.ref | | 왼쪽 그림은 초록색 또는 검은색의 연속적인 data를 쉽게 표현하는 선형,비선형 그래프를 추정하는 것이 목적이다. | 반면 오른쪽 그림은 0또는 1이라는 값으로 고양이 여부, 즉 binary한 값을 도출하는 알고리즘이다. | 여기서 input이 이미지로 이루어진 것처럼 보이지만 나중에 이미지는 컴퓨터가 계산할 수 있는 실수값으로 이루어진 어떤 tensor(다차원의 행렬로 보면 될 것 같다)값으로 변환되기 때문에 숫자로 이루어진 연속형 data로 보면 된다. | 이렇게 Logistic Regression (Binary Classification에 이용)입력된 data의 특징을 통해 0또는 1이라는 output의 값을 추정하는 것을 목표로 하는 알고리즘. 이를 위해 sigmoid라는 활성화 함수를 사용하게 된다. | | 간단히, 특정 범주에 속할 것인지에 대한 확률을 예측하는 것이기 때문에 Output이 반드기 0~1사이에 있어야하므로 sigmoid라는 활성화함수(스칼라값을 입력하면 스칼라값을 출력하는 함수)를 사용한다고 알아두자 | 예를 들어 고양이라고 생각하는 1에 해당하는 확률 값이 sigmoid를 통해 0.78이 나왔다면 컴퓨터는 0.78의 확률로 고양이라고 생각한다는 것이다. 마지막으로 Logistic Regression 알고리즘을 통해 추정된 yhat의 값이 정말 옳은 값인지 잘못된 예측인지 알아야 할 것이다. | 이러한 오차를 계산하기 위해 Cross Entropy가 사용됨 | | 이 방법을 통해 추정된 yaht의 값과 참값이 얼마나 다른지를 수학적으로 계산하여 하나의 숫자값으로 오차를 표현해준다. | 따라서 이 오차를 0으로 만드는 것이 Logistic Regression의 최종 목표이다. | . | . | 정리하자면 다음과 같다. | 오차를 줄이는 방향으로 알고리즘이 학습되어야 할 것이다. | . | . | . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # 재현성을 위하여 torch.manual_seed(1) # 사용될 data x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]] y_data = [[0],[0],[0],[1],[1],[1]] x_train = torch.FloatTensor(x_data) y_train = torch.FloatTensor(y_data) print(x_train.shape) print(y_train.shape) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy _distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs: C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll warnings.warn(&#34;loaded more than 1 DLL from .libs:&#34; . torch.Size([6, 2]) torch.Size([6, 1]) . Computing the Hypothesis | . print(&#39;e^1 equals: &#39;,torch.exp(torch.FloatTensor([1]))) . e^1 equals: tensor([2.7183]) . | . W = torch.zeros((2,1), requires_grad = True) # gradient를 배우겠다. b = torch.zeros(1, requires_grad = True) # gradient를 배우겠다. # 지금은 일단 0으로 줬음 hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b))) # x_train.matmul(W) = torch.matmul(x,W) print(hypothesis) print(hypothesis.shape) . tensor([[0.5000], [0.5000], [0.5000], [0.5000], [0.5000], [0.5000]], grad_fn=&lt;MulBackward0&gt;) torch.Size([6, 1]) . print(&#39;1/(1+e^{-1}) = &#39; , torch.sigmoid(torch.FloatTensor([1]))) . 1/(1+e^{-1}) = tensor([0.7311]) . 사실은 위 셀처럼 제공되어지고 있으니, 아래와 같이 간단히 써볼 수 있겠다. | . hypothesis = torch.sigmoid(x_train.matmul(W) + b) print(hypothesis) print(hypothesis.shape) . tensor([[0.5000], [0.5000], [0.5000], [0.5000], [0.5000], [0.5000]], grad_fn=&lt;SigmoidBackward0&gt;) torch.Size([6, 1]) . 값이 동일하게 나옴을 알 수 있다. | . . Computing the Cost Function | 위 사진에서 H(x)는 P(x = 1; W)로 표현할 수 있겠다. | . | 우리는 hypothesis와 y_train의 오차를 알고싶다. | . print(hypothesis) print(y_train) . tensor([[0.5000], [0.5000], [0.5000], [0.5000], [0.5000], [0.5000]], grad_fn=&lt;SigmoidBackward0&gt;) tensor([[0.], [0.], [0.], [1.], [1.], [1.]]) . 일단 한 개의 요소에 대해서만 계산해보자 | . -(y_train[0] * torch.log(hypothesis[0]) + (1 - y_train[0]) * torch.log(1 - hypothesis[0])) . tensor([0.6931], grad_fn=&lt;NegBackward0&gt;) . 아래 사진을 참고해보자 | | . 전체 sample에 대해 표현해보자 | . losses = -(y_train*torch.log(hypothesis) + (1-y_train)*torch.log(1-hypothesis)) print(losses) . tensor([[0.6931], [0.6931], [0.6931], [0.6931], [0.6931], [0.6931]], grad_fn=&lt;NegBackward0&gt;) . cost = losses.mean() print(cost) . tensor(0.6931, grad_fn=&lt;MeanBackward0&gt;) . 이 모든 것을 한 번에 해결해보자 | . F.binary_cross_entropy(hypothesis, y_train) . tensor(0.6931, grad_fn=&lt;BinaryCrossEntropyBackward0&gt;) . 동일한 값이 나옴을 알 수 있다. | . . Whole Training Procedure | . W = torch.zeros((2,1), requires_grad=True) b = torch.zeros(1, requires_grad=True) # optimizer 설정 optimizer = optim.SGD([W,b], lr=1) # SGD를 이용하여 learning rate=1인 상태로 W,b를 학습 nb_epochs = 1000 for epoch in range(nb_epochs + 1) : # cost 계산 hypothesis = torch.sigmoid(x_train.matmul(W) + b) cost = F.binary_cross_entropy(hypothesis, y_train) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번마다 로그 출력 if epoch % 100 == 0 : print(&#39;Epoch{:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) . Epoch 0/1000 Cost : 0.693147 Epoch 100/1000 Cost : 0.134722 Epoch 200/1000 Cost : 0.080643 Epoch 300/1000 Cost : 0.057900 Epoch 400/1000 Cost : 0.045300 Epoch 500/1000 Cost : 0.037261 Epoch 600/1000 Cost : 0.031672 Epoch 700/1000 Cost : 0.027556 Epoch 800/1000 Cost : 0.024394 Epoch 900/1000 Cost : 0.021888 Epoch1000/1000 Cost : 0.019852 . cost값이 점점 줄어듦을 알 수 있다. | . . Evlauation | . hypothesis = torch.sigmoid(x_train.matmul(W) +b) print(hypothesis[:5]) . tensor([[2.7648e-04], [3.1608e-02], [3.8977e-02], [9.5622e-01], [9.9823e-01]], grad_fn=&lt;SliceBackward0&gt;) . 사실은 x_train이 아니라 x_test인 게 더 정확하다. | 간단히 해석해보면 순서대로 x=1일 확률을 의미한다. | . prediction = hypothesis &gt;= torch.FloatTensor([0.5]) print(prediction[:5]) # 예측 print(y_train[:5]) # 실제 정답 . tensor([[False], [False], [False], [ True], [ True]]) tensor([[0.], [0.], [0.], [1.], [1.]]) . 잘 예측했음을 알 수 있다. | . correct_prediction = prediction.float() == y_train print(correct_prediction) . tensor([[True], [True], [True], [True], [True], [True]]) . 5개에 대해선 동일함을 알 수 있다. | . . Higher Implementation with Class | . class BinaryClassifier(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(8,1) self.sigmoid = nn.Sigmoid() def forward(self, x): return self.sigmoid(self.linear(x)) . model = BinaryClassifier() . | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/28/intro.html",
            "relUrl": "/2022/01/28/intro.html",
            "date": " • Jan 28, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "2022/01/27/THU",
            "content": "Simple Linear Regression 하나의 정보로부터 하나의 결론을 짓는 모델이었다. 다음과 같다. | $H(x) = W(x) + b$ | 하지만 대부분의 경우 좀 더 복잡한 예측을 하기 위해선 다양한 정보가 필요하다. | 이를 위해 Multivariate Linear Regression을 알아보자 | . | Multivariate Linear Regression 복수의 정보가 존재할 때 어떻게 하나의 추측값을 계산할 수 있는지? | 예를 들어, 만약 쪽지시험들의 성적이 73,80,75점일때, 이 학생의 기말고사 점수가 몇점일지 예측해보자 | . | . import torch x_train = torch.FloatTensor([[73,80,75], [93,88,93], [89,91,90], [96,98,100], [73,66,70]]) y_train = torch.FloatTensor([[125],[185],[180],[196],[142]]) . Hypothesis Function은 인공신경망의 구조를 나타내는데 이번에도 동일하게 $H(x) = W(x) + b$를 이용하여 표현한다. 하지만 simple linear regression에서는 x에 하나의 정보밖에 없어서 x를 1x1 vector로 표현했다면 이번에는 x에 3개의 정보가 있으므로 아래와 같이 나타낼 수 있다. | | 즉 x를 3x1 vector로 나타낸다. | 그렇다면 이 Hypothesis function을 PyTorch에서 어떻게 계산할 수 있을까 | 단순한 hypothesis 정의를 이용하여 아래와 같이 작성할 수 있을 것이다. | | 하지만 x에 3개의 정보가 있을 때 이렇게 나열하는 게 가능할 수 있겠지만 더 많은 양의 정보를 x가 가지고 있다면 hypothesis를 계산하는 이 한줄은 점점 길어질 것이다. | 따라서 우리는 PyTorch에서 제공해주는 matmul이라는 함수를 사용하면 된다. | 코드는 아래와 같다.hypothesis = x_train.matmul(W) + b . | Multivariate Linear Regression의 Cost function은 Simple Linear Regression과 마찬가지로 MSE를 사용하며 계산 방식역시 동일하다. | 또한 Multivariate Linear Regression의 학습방식 또한 Gradient Descent with torch.optim로서 동일하다. | | 이제 완성된 코드를 한 번 작성해보자 | . import torch x_train = torch.FloatTensor([[73,80,75], [93,88,93], [89,91,90], [96,98,100], [73,66,70]]) y_train = torch.FloatTensor([[125],[185],[180],[196],[142]]) # 모델 초기화 W = torch.zeros((3,1), requires_grad=True) b = torch.zeros(1, requires_grad=True) # optimizer 설정 import torch.optim as optim optimizer = optim.SGD([W,b], lr = 1e-5) . nb_epochs = 20 for epoch in range(nb_epochs+1): # H(x) 계산 hypothesis = x_train.matmul(W) + b # cost 계산 cost = torch.mean((hypothesis - y_train)**2) # cost로 H(x)계산 optimizer.zero_grad() cost.backward() optimizer.step() print(&#39;Epoch {:4d}/{} hypothesis: {} Cost {:.6f}&#39;.format(epoch, nb_epochs, hypothesis.squeeze().detach(),cost.item())) . Epoch 0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost 28166.000000 Epoch 1/20 hypothesis: tensor([65.3835, 78.5927, 77.4353, 84.3257, 59.9476]) Cost 8919.982422 Epoch 2/20 hypothesis: tensor([101.9877, 122.5950, 120.7879, 131.5363, 93.5114]) Cost 2887.348145 Epoch 3/20 hypothesis: tensor([122.4795, 147.2313, 145.0590, 157.9675, 112.3039]) Cost 996.406433 Epoch 4/20 hypothesis: tensor([133.9505, 161.0253, 158.6470, 172.7651, 122.8264]) Cost 403.667480 Epoch 5/20 hypothesis: tensor([140.3712, 168.7492, 166.2539, 181.0495, 128.7189]) Cost 217.844818 Epoch 6/20 hypothesis: tensor([143.9643, 173.0745, 170.5123, 185.6873, 132.0192]) Cost 159.569305 Epoch 7/20 hypothesis: tensor([145.9744, 175.4972, 172.8959, 188.2836, 133.8682]) Cost 141.273163 Epoch 8/20 hypothesis: tensor([147.0982, 176.8546, 174.2299, 189.7369, 134.9047]) Cost 135.508194 Epoch 9/20 hypothesis: tensor([147.7258, 177.6156, 174.9762, 190.5503, 135.4863]) Cost 133.671143 Epoch 10/20 hypothesis: tensor([148.0756, 178.0428, 175.3936, 191.0054, 135.8133]) Cost 133.065277 Epoch 11/20 hypothesis: tensor([148.2699, 178.2830, 175.6268, 191.2600, 135.9977]) Cost 132.845520 Epoch 12/20 hypothesis: tensor([148.3771, 178.4185, 175.7569, 191.4022, 136.1022]) Cost 132.746689 Epoch 13/20 hypothesis: tensor([148.4356, 178.4955, 175.8292, 191.4816, 136.1620]) Cost 132.685730 Epoch 14/20 hypothesis: tensor([148.4667, 178.5396, 175.8692, 191.5258, 136.1968]) Cost 132.636658 Epoch 15/20 hypothesis: tensor([148.4826, 178.5654, 175.8911, 191.5503, 136.2177]) Cost 132.591431 Epoch 16/20 hypothesis: tensor([148.4900, 178.5809, 175.9028, 191.5637, 136.2306]) Cost 132.547455 Epoch 17/20 hypothesis: tensor([148.4925, 178.5906, 175.9090, 191.5710, 136.2392]) Cost 132.503815 Epoch 18/20 hypothesis: tensor([148.4924, 178.5971, 175.9119, 191.5748, 136.2453]) Cost 132.460098 Epoch 19/20 hypothesis: tensor([148.4908, 178.6018, 175.9130, 191.5766, 136.2500]) Cost 132.416672 Epoch 20/20 hypothesis: tensor([148.4883, 178.6055, 175.9132, 191.5774, 136.2540]) Cost 132.373291 . cost가 점점 작아짐을 알 수 있다. | H(x)도 점점 y에 가까워짐을 알 수 있다. | Learning rate에 따라 발산할 수도 있다. | . . W와 b를 일일히 작성해주는 것은 번거로운 일이 될 수 있다. | 따라서 PyTorch에서 제공해주는 nn.Module을 상속해서 모델을 생성하자 | . # 아래는 Full code이다. x_train = torch.FloatTensor([[73,80,75], [93,88,93], [89,91,90], [96,98,100], [73,66,70]]) y_train = torch.FloatTensor([[125],[185],[180],[196],[142]]) import torch.nn as nn class MultivariateLinearRegressionModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3,1) def forward(self,x): return self.linear(x) # 모델 초기화 model = MultivariateLinearRegressionModel() # optimizer 설정 optimizer = optim.SGD([W,b],lr=1e-5) import torch.nn.functional as F # cost 계산 cost = F.mse_loss(prediction, y_train) nb_epochs = 20 for epoch in range(nb_epochs+1): # H(x) 계산 hypothesis = model(x_train) # cost 계산 cost = F.mse_loss(prediction, y_train) # cost로 H(x)계산 optimizer.zero_grad() cost.backward() optimizer.step() print(&#39;Epoch {:4d}/{} hypothesis: {} Cost {:.6f}&#39;.format(epoch, nb_epochs, hypothesis.squeeze().detach(),cost.item())) . nn.linear (3,1) 입력 차원 = 3 | 출력 차원 = 1 | . | Hypothesis 계산은 forward에서 진행된다. | Gradient 계산은 Pytorch가 알아서 진행해준다. backward() | . . 또한 Pytorch에서는 다양한 cost function을 제공한다. | . import torch.nn.functional as F # cost 계산 cost = F.mse_loss(prediction, y_train) . 이렇게 사용하면 쉽게 다른 loss와 교체 가능하다. (l1_loss, smooth_l1_loss 등 ...) | 그런데 여기서 prediction이 정의되지 않았다고 함... | . 지금까지 적은 양의 데이터를 가지고 학습했다. | 하지만 딥러닝은 많은 양의 데이터와 함께할 때 빛을 발한다. | PyTorch에서는 많은 양의 데이터를 어떻게 다룰까? | . . 복잡한 머신러닝 모델을 학습하려면 엄청난 양의 데이터가 필요하다. | 대부분 dataset은 적어도 수십만 개의 데이터를 제공한다. | 엄청난 양의 데이터를 한 번에 학습시킬 수 없다 너무 느리다 | 하드웨어적으로 불가능하다. | . | 그렇다면 일부분의 데이터로만 학습하면 어떨까? 이렇게 해서 나온 아이디어가 Minibatch Gradient Descent이다 | 전체 데이터를 균일하게 나눠서 학습하자 | 아래 사진을 참고해보자 | | 보다 작은 단위인 Minibatch로 나누어서 Minibatch하나하나 학습하는 것이다. | 각 Minibatch에 있는 cost만 계산한 후에 Gradient descent 할 수 있기 때문에 컴퓨터에 무리가 덜 가게 된다. | 한 번에 Gradient descent를 하지 않기 때문에 업데이트를 좀 더 빠르게 할 수 있다 | 그렇지만 모델의 cost를 계산할 때 전체 데이터를 사용하지 않기 때문에 잘못된 방향으로 업데이트를 할 수도 있다. | 기존 Gradient descent처럼 매끄럽게 cost가 줄어들지 않고 좀 더 거칠게 줄어들게 된다. (각각 좌우의 그래프이다.) | | 이제 실제 dataset을 minibatch로 쪼개는 데에 사용되는 PyTorch Dataset과 module에 대해 알아보자 | . | . | . torch.utils.data.Dataset 상속 | __len__() 이 dataset의 총 data 수 | . | __getitem__() 어떠한 인덱스 idx르르 받았을 때, 그에 상응하는 입출력 데이터 반환 | . | . 이렇게 dataset을 만들었다면, PyTorch에서 module로 제공해주는 DataLoader를 사용할 수 있다. | | instance를 만드려면 두 개의 parameter를 지정해주어야 한다. | batch_size = 2 각 minibatch의 크기 | 통상적으로 2의 제곱수로 설정한다. | . | shuffle = True Epoch마다 dataset을 섞어서 데이터가 학습되는 순서를 바꾼다. | 이 옵션을 설정함으로써 우리의 모델이 dataset의 순서를 외우지 못하게 방지할 수 있으므로 권장하는 옵션이다. | . | . Full Code with Dataset and DataLoader | | enumerate(dataloader) minibatch 인덱스와 데이터를 받음 | . | len(dataloader) 한 epoch당 minibatch 개수 | . | . 지금까지 하나 또는 여러 개의 입력으로부터 어떤 숫자 하나를 예측하는 모델을 만들었다. | 다음시간엔 이렇게 숫자 하나를 예측하는 모델이 아니라 어떤 입력을 받았을 때 그것을 분류하는 모델에 대해서 알아보자 | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/27/intro.html",
            "relUrl": "/2022/01/27/intro.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "2022/01/26/WED",
            "content": "PyTorch를 이용해서 Linear Regression을 작성해보자 . Data definition 공부한 시간과 점수와의 상관관계에 대해 알아보자 | | 학습 시간이 1,2,3일때를 Training dataset이라 한다. | 학습이 끝난 후 이 모델이 얼마나 잘 작동하는지 판별하기 위해 사용하는 data를 Test dataset이라 한다. | 이때 4시간 공부했을 때 점수는 몇점일까? | 본격적으로 코딩해보자 | . | . 모델을 학습시키기 위한 data는 torch.tensor를 이용한다. | 이때 입력과 출력은 각기 다른 dataset에 입력해준다. | . import torch x_train = torch.FloatTensor([[1],[2],[3]]) # 모델을 학습시키기 위한 입력 dataset y_train = torch.FloatTensor([[2],[4],[6]]) # 모델을 학습시키기 위한 출력 dataset . 입출력은 x,y로 구분한다. | 우리의 모델인 Hypothesis를 구현해보자 | Linear Regression은 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일이다. | 이러한 직선 $y$는 $Wx + b$로 나타낼 수 있다. | 이때 $W$는 Weight $b$는 Bias이다. | 그렇다면 이러한 Hypothesis를 정의하려면 W와 b를 먼저 정의해야 한다. | 일단 Weight와 Bias를 0으로 초기화해보자, 즉 어떤 입력을 받더라도 0을 예측할 것이다. | 이 W와 b를 학습시키는 게 우리의 목적이므로 requires_grad=True라고 입력해줌으로써 학습할 것이라고 명시해주는 것이다. | . W = torch.zeros(1, requires_grad = True) # Hypothesis 초기화 b = torch.zeros(1, requires_grad = True) # Hypothesis 초기화 hypothesis = x_train * W + b . 이렇게 W와 b를 정의하면 hypothesis는 $Wx + b$ 이렇게 간단한 곱과 합으로 나타낼 수 있다. | 이제 문제와 모델 정의가 모두 끝났으니 학습을 시작해보자 | 학습을 하려면 일단 우리의 모델이 얼마나 정답과 근사한지 알아야 한다. | . | 우리의 예측값과 실제 training dataset의 y값의 차이를 제곱해서 평균을 내는 것이다. | 위 수식을 PyTorch로 구현해보자 | . cost = torch.mean((hypothesis - y_train)**2) . MSE를 구했다. | 이 계산된 loss를 이용해서 모델을 개선시킬 차례이다. | . import torch.optim as optim optimizer = optim.SGD([W,b], lr = 0.01) # Optimizer를 정의해준다. # 우리의 모델에서 학습시킬 변수는 Weight와 Bias이기 때문에 이 2개를 list로 # 만들어서 입력해준뒤 적당한 learning rate도 입력해준다. optimizer.zero_grad() # gradient를 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선 . 이제 학습을 원하는만큼 for문을 반복해준다. | . nb_epochs = 1000 for epoch in range(1, nb_epochs + 1): hypothesis = x_train * W + b cost = torch.mean((hypothesis - y_train)**2) optimizer.zero_grad() # gradient를 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선 . 이렇게 반복적으로 학습하면 W와 b가 각각 하나의 최적의 수로 수렴하게 된다. | . W . tensor([1.9709], requires_grad=True) . b . tensor([0.0663], requires_grad=True) . W*4+b . tensor([7.9497], grad_fn=&lt;AddBackward0&gt;) . 8에 근접함을 알 수 있다. | . . 그렇다면 이렇게 cost를 줄이는 Gradient Descent에 대해 알아보자 | | Hypothesis 함수는 인공신경망의 구조로 나타내는데 주어진 input x에 대해 어떤 output y를 예측할지 알려주며 위와 같이 H(x)로 나타낸다. | b가 삭제된 더 간단한 모델에 대해 알아보자. 즉, W만 학습하면 된다. | . 위 train dataset은 입력과 출력이 동일하므로 H(x)=x가 정확한 모델일 것이다. 즉 W=1이 가장 좋은 숫자이다. | 반대로 W가 어떤 1이 아닌 값에서 시작할 때 학습의 목표는 W를 1로 수렴시키는 것이다. | W가 1에 가까울수록 더 정확한 모델이 되는 것이다. | 그렇다면 어떤 모델이 주어졌을 때 그것이 좋고 나쁨을 어떻게 평가할 수 있을까? 여기서 우리는 cost function을 정의할 수 있겠다. | cost function은 모델의 예측값이 실제 데이터와 얼마나 다른지를 나타내는 값으로 잘 학습된 모델일수록 낮은 cost를 가질 것이다. | 우리의 예제에서는 w가 1일 때 모든 데이터가 정확히 들어맞기 때문에 이때 cost가 0이다 | 그리고 w가 1에서 멀어질수록 예측값과 실제 데이터가 달라지므로 cost가 높아질 것이다. | | . | linear regression에서 사용되는 cost function은 MSE(예측값과 실제값의 차이를 제곱한 것의 평균)이다. | 우리가 원하는 cost function을 최소화하는 것이다. | 그러기 위해선 일단 위의 w-cost 그래프를 보자 | 기울기가 음수일 때는 W가 더 커져야 하고 기울기가 양수일 땐 W가 더 작아져야 한다. | 또한 기울기가 가파를수록 cost가 큰 것이니 w를 많이 바꿔야 하고 기울기가 완만할 땐 기울기가 가파를 때에 비해 w를 조금만 바꾸면 된다. | 우리는 이 기울기를 gradient라고 한다. | 이 gradient를 계산하기 위해선 미분을 해야하는데 cost function은 결국 w에 대한 2차 함수이기 때문에 간단한 미분 방정식을 통해 계산할 수 있다. | | 아까 설명했듯 gradient 즉 기울기가 양수일 때는 w를 줄이고 기울기가 음수일 때는 w를 늘려야 한다. | | 이러한 방법이 Gradient Descent이다. | . | . x_train = torch.FloatTensor([[1],[2],[3]]) y_train = torch.FloatTensor([[1],[2],[3]]) # 모델 초기화 W = torch.zeros(1) # learning rate 설정 lr = 0.1 nb_epochs = 10 for epoch in range(nb_epochs + 1): # H(x) 계산 hypothesis = x_train * W # cost gradient 계산 cost = torch.mean((hypothesis - y_train)**2) gradient = torch.sum((W * x_train - y_train)*x_train) print(&#39;Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}&#39;.format(epoch, nb_epochs,W.item(), cost.item())) # cost gradient로 H(x) 개선 W-=lr*gradient . Epoch 0/10 W: 0.000, Cost: 4.666667 Epoch 1/10 W: 1.400, Cost: 0.746666 Epoch 2/10 W: 0.840, Cost: 0.119467 Epoch 3/10 W: 1.064, Cost: 0.019115 Epoch 4/10 W: 0.974, Cost: 0.003058 Epoch 5/10 W: 1.010, Cost: 0.000489 Epoch 6/10 W: 0.996, Cost: 0.000078 Epoch 7/10 W: 1.002, Cost: 0.000013 Epoch 8/10 W: 0.999, Cost: 0.000002 Epoch 9/10 W: 1.000, Cost: 0.000000 Epoch 10/10 W: 1.000, Cost: 0.000000 . Epoch이 증가할수록 W가 1에 수렴하고 Cost역시 줄어듦을 알 수 있다. | gradient descesnt를 더 편리하게 할 수 있도록 torch.optim을 이용할 수 있다. | . import torch.optim as optim optimizer = optim.SGD([W,b], lr = 0.01) # Optimizer를 정의해준다. # 우리의 모델에서 학습시킬 변수는 Weight와 Bias이기 때문에 이 2개를 list로 # 만들어서 입력해준뒤 적당한 learning rate도 입력해준다. optimizer.zero_grad() # gradient를 0으로 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선, 즉 실행한다. . | 이와 같이 이용할 수 있겠다. | . 지금까지 우리는 하나의 정보로부터 하나의 정보를 추측하는 모델을 만들었다. 예를 들어 한 학생읭 수업 참여도를 알 때 점수를 추측할 수 있는 모델 같은 게 있을 수 있다. 하지만 좋은 추측을 하기 위해선 정보 양이 많이 필요하다. | 예를 들어 한 학생의 점수를 추측할 때, 쪽지시험 하나의 성적보다 그 수업 때 진행된 모든 쪽지시험의 점수를 알면 좋듯 많은 정보는 추측의 질을 높여준다. | . | . | . 다음시간엔 여러개의 정보로부터 모델을 만들어보자 | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/26/intro.html",
            "relUrl": "/2022/01/26/intro.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "2022/01/13/THU",
            "content": "View(Reshape) . import numpy as np import torch t=np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]]) ft=torch.FloatTensor(t) print(ft.shape) . torch.Size([2, 2, 3]) . t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) . print(ft.view([-1,3])) # view사용해서 모양 바꾸자 print(ft.view([-1,3]).shape) . tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]]) torch.Size([4, 3]) . 두개의 차원으로 변경할 건데 앞 차원은 모르겠고 뒷 차원은 세개의 element를 가질래 | . print(ft.view([-1,1,3])) print(ft.view([-1,1,3]).shape) . tensor([[[ 0., 1., 2.]], [[ 3., 4., 5.]], [[ 6., 7., 8.]], [[ 9., 10., 11.]]]) torch.Size([4, 1, 3]) . 세개의 차원으로 변경할 건데 첫번째 차원은 아직 모르겠고 두번째 차원과 세번째 차원은 각각 1과3으로 변경해줘 | . Squeeze . ft=torch.FloatTensor([[0],[1],[2]]) print(ft) print(ft.shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . print(ft.squeeze()) print(ft.squeeze().shape) . tensor([0., 1., 2.]) torch.Size([3]) . 차원이 줄어들었음 | 1만 압축해주는 것 같음 | . print(ft.squeeze(dim=0)) print(ft.squeeze(dim=1)) # 해당 차원이 1일 때만! . tensor([[0.], [1.], [2.]]) tensor([0., 1., 2.]) . Unsqueeze . ft = torch.Tensor([0,1,2]) print(ft.shape) . torch.Size([3]) . 첫번째 차원에 1을 넣자 | . print(ft.unsqueeze(0)) print(ft.unsqueeze(0).shape) . tensor([[0., 1., 2.]]) torch.Size([1, 3]) . print(ft.view(1,-1)) print(ft.view(1,-1).shape) . tensor([[0., 1., 2.]]) torch.Size([1, 3]) . 두번째 차원에 1을 넣자 | . print(ft.unsqueeze(1)) print(ft.unsqueeze(1).shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . -1=(dim=1) | . print(ft.unsqueeze(-1)) print(ft.unsqueeze(-1).shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . Type Casting . lt = torch.LongTensor([1,2,3,4]) print(lt) . tensor([1, 2, 3, 4]) . print(lt.float()) . tensor([1., 2., 3., 4.]) . bool형일 때 | . bt=torch.ByteTensor([True,True,True]) print(bt) . tensor([1, 1, 1], dtype=torch.uint8) . print(bt.long()) print(bt.float()) . tensor([1, 1, 1]) tensor([1., 1., 1.]) . Concatenate . x=torch.FloatTensor([[1,2],[3,4]]) y=torch.FloatTensor([[5,6],[7,8]]) . print(torch.cat([x,y], dim=0)) print(torch.cat([x,y], dim=1)) . tensor([[1., 2.], [3., 4.], [5., 6.], [7., 8.]]) tensor([[1., 2., 5., 6.], [3., 4., 7., 8.]]) . Stacking . x=torch.FloatTensor([1,4]) y=torch.FloatTensor([2,5]) z=torch.FloatTensor([3,6]) . print(torch.stack([x,y,z])) print(torch.stack([x,y,z],dim=1)) . tensor([[1., 4.], [2., 5.], [3., 6.]]) tensor([[1., 2., 3.], [4., 5., 6.]]) . print(torch.cat([x.unsqueeze(0),y.unsqueeze(0),z.unsqueeze(0)],dim=0)) . tensor([[1., 4.], [2., 5.], [3., 6.]]) . Ones and Zeros . x=torch.FloatTensor([[0,1,2],[2,1,0]]) print(torch.ones_like(x)) print(torch.zeros_like(x)) . tensor([[1., 1., 1.], [1., 1., 1.]]) tensor([[0., 0., 0.], [0., 0., 0.]]) . In-place Operation . x=torch.FloatTensor([[1,2],[3,4]]) . print(x.mul(2)) print(x) print(x.mul_(2)) print(x) . tensor([[2., 4.], [6., 8.]]) tensor([[1., 2.], [3., 4.]]) tensor([[2., 4.], [6., 8.]]) tensor([[2., 4.], [6., 8.]]) . 즉 언더바를 통해 inplace=True 효과! | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/13/intro.html",
            "relUrl": "/2022/01/13/intro.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "2022/01/12/WED",
            "content": "import torch . t=torch.FloatTensor([0.,1.,2.,3.,4.,5.,6.]) print(t) . tensor([0., 1., 2., 3., 4., 5., 6.]) . print(t.dim()) print(t.shape) print(t.size()) print(t[0],t[1],t[-1]) . 1 torch.Size([7]) torch.Size([7]) tensor(0.) tensor(1.) tensor(6.) . 그 외에도 slicing도 ndarray처럼 사용 가능 | 2차원 3차원도 ndarray처럼 사용 가능 | . Broadcasting . m1 = torch.FloatTensor([[3,3]]) m2 = torch.FloatTensor([[2,2]]) print(m1+m2) . tensor([[5., 5.]]) . 이렇게 같은 크기뿐만 아니라 Broadcasting을 통해 | . m1 = torch.FloatTensor([[1,2]]) m2 = torch.FloatTensor([[3]]) print(m1+m2) . tensor([[4., 5.]]) . 이렇게 크기가 동일하지 않아도 pytorch가 자동으로 동일한 size로 변환하여 연산 수행 가능 | . m1 = torch.FloatTensor([[1,2]]) m2 = torch.FloatTensor([[3],[4]]) print(m1+m2) . tensor([[4., 5.], [5., 6.]]) . print(m1.shape) . torch.Size([1, 2]) . m1 . tensor([[1., 2.]]) . print(m2.shape) . torch.Size([2, 1]) . m2 . tensor([[3.], [4.]]) . 이렇게 행렬 모양이 다르더라도 m1과 m2를 각각 2x2행렬로 변경하여 덧셈 수행하는 기능도 가능하다 | 이런 Broadcasting 기능은 자동으로 수행되기 때문에 잘못 사용하지 않게 유의하자 | . m1 = torch.FloatTensor([[1,2],[3,4]]) m2 = torch.FloatTensor([[3],[4]]) print(m1*m2) print(m1.mul(m2)) . tensor([[ 3., 6.], [12., 16.]]) tensor([[ 3., 6.], [12., 16.]]) . 이때 m2가 Broadcasting되면서 우리가 일반적으로 알고 있는 행렬 연산 수행과는 상이하게 진행됨 | 우리가 알고 있는 행렬 연산을 하기 위해서는 | . m1 = torch.FloatTensor([[1,2],[3,4]]) m2 = torch.FloatTensor([[3],[4]]) print(m1.matmul(m2)) . tensor([[11.], [25.]]) . 이렇게 해야한다. | . t = torch.FloatTensor([1,2]) print(t.mean()) . tensor(1.5000) . t = torch.FloatTensor([[1,2],[3,4]]) print(t.mean(dim=0)) # dim=0을 없애겠다. (2x2) -&gt; (1x2) print(t.mean(dim=1)) # dim=1을 없애겠다. (2x2) -&gt; (2x1) print(t.mean(dim=-1)) . tensor([2., 3.]) tensor([1.5000, 3.5000]) tensor([1.5000, 3.5000]) . t = torch.FloatTensor([[1,2],[3,4]]) print(t) . tensor([[1., 2.], [3., 4.]]) . print(t.sum()) print(t.sum(dim=0)) print(t.sum(dim=1)) print(t.sum(dim=-1)) . tensor(10.) tensor([4., 6.]) tensor([3., 7.]) tensor([3., 7.]) . t = torch.FloatTensor([[3,2],[1,4]]) print(t) . tensor([[3., 2.], [1., 4.]]) . print(t.max()) . tensor(4.) . print(t.max(dim=0)) # dim=0 제외하고 각 열에서의 max와 그 때의 index값을 알려줌 print(t.max(dim=0)[0]) # max값 print(t.max(dim=0)[1]) # max값의 index 즉, argmax . torch.return_types.max( values=tensor([3., 4.]), indices=tensor([0, 1])) tensor([3., 4.]) tensor([0, 1]) . print(t.max(dim=1)) print(t.max(dim=1)[0]) print(t.max(dim=1)[1]) . torch.return_types.max( values=tensor([3., 4.]), indices=tensor([0, 1])) tensor([3., 4.]) tensor([0, 1]) .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/12/intro.html",
            "relUrl": "/2022/01/12/intro.html",
            "date": " • Jan 12, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rhkrehtjd.github.io/INTROdl/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rhkrehtjd.github.io/INTROdl/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}