{
  
    
        "post0": {
            "title": "2022/01/26/WED",
            "content": "PyTorch를 이용해서 Linear Regression을 작성해보자 . Data definition 공부한 시간과 점수와의 상관관계에 대해 알아보자 | | 학습 시간이 1,2,3일때를 Training dataset이라 한다. | 학습이 끝난 후 이 모델이 얼마나 잘 작동하는지 판별하기 위해 사용하는 data를 Test dataset이라 한다. | 이때 4시간 공부했을 때 점수는 몇점일까? | 본격적으로 코딩해보자 | . | . 모델을 학습시키기 위한 data는 torch.tensor를 이용한다. | 이때 입력과 출력은 각기 다른 dataset에 입력해준다. | . import torch x_train = torch.FloatTensor([[1],[2],[3]]) # 모델을 학습시키기 위한 입력 dataset y_train = torch.FloatTensor([[2],[4],[6]]) # 모델을 학습시키기 위한 출력 dataset . 입출력은 x,y로 구분한다. | 우리의 모델인 Hypothesis를 구현해보자 | Linear Regression은 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일이다. | 이러한 직선 $y$는 $Wx + b$로 나타낼 수 있다. | 이때 $W$는 Weight $b$는 Bias이다. | 그렇다면 이러한 Hypothesis를 정의하려면 W와 b를 먼저 정의해야 한다. | 일단 Weight와 Bias를 0으로 초기화해보자, 즉 어떤 입력을 받더라도 0을 예측할 것이다. | 이 W와 b를 학습시키는 게 우리의 목적이므로 requires_grad=True라고 입력해줌으로써 학습할 것이라고 명시해주는 것이다. | . W = torch.zeros(1, requires_grad = True) # Hypothesis 초기화 b = torch.zeros(1, requires_grad = True) # Hypothesis 초기화 hypothesis = x_train * W + b . 이렇게 W와 b를 정의하면 hypothesis는 $Wx + b$ 이렇게 간단한 곱과 합으로 나타낼 수 있다. | 이제 문제와 모델 정의가 모두 끝났으니 학습을 시작해보자 | 학습을 하려면 일단 우리의 모델이 얼마나 정답과 근사한지 알아야 한다. | . | 우리의 예측값과 실제 training dataset의 y값의 차이를 제곱해서 평균을 내는 것이다. | 위 수식을 PyTorch로 구현해보자 | . cost = torch.mean((hypothesis - y_train)**2) . MSE를 구했다. | 이 계산된 loss를 이용해서 모델을 개선시킬 차례이다. | . import torch.optim as optim optimizer = optim.SGD([W,b], lr = 0.01) # Optimizer를 정의해준다. # 우리의 모델에서 학습시킬 변수는 Weight와 Bias이기 때문에 이 2개를 list로 # 만들어서 입력해준뒤 적당한 learning rate도 입력해준다. optimizer.zero_grad() # gradient를 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선 . 이제 학습을 원하는만큼 for문을 반복해준다. | . nb_epochs = 1000 for epoch in range(1, nb_epochs + 1): hypothesis = x_train * W + b cost = torch.mean((hypothesis - y_train)**2) optimizer.zero_grad() # gradient를 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선 . 이렇게 반복적으로 학습하면 W와 b가 각각 하나의 최적의 수로 수렴하게 된다. | . W . tensor([1.9709], requires_grad=True) . b . tensor([0.0663], requires_grad=True) . W*4+b . tensor([7.9497], grad_fn=&lt;AddBackward0&gt;) . 8에 근접함을 알 수 있다. | . . 그렇다면 이렇게 cost를 줄이는 Gradient Descent에 대해 알아보자 | | Hypothesis 함수는 인공신경망의 구조로 나타내는데 주어진 input x에 대해 어떤 output y를 예측할지 알려주며 위와 같이 H(x)로 나타낸다. | b가 삭제된 더 간단한 모델에 대해 알아보자. 즉, W만 학습하면 된다. | . 위 train dataset은 입력과 출력이 동일하므로 H(x)=x가 정확한 모델일 것이다. 즉 W=1이 가장 좋은 숫자이다. | 반대로 W가 어떤 1이 아닌 값에서 시작할 때 학습의 목표는 W를 1로 수렴시키는 것이다. | W가 1에 가까울수록 더 정확한 모델이 되는 것이다. | 그렇다면 어떤 모델이 주어졌을 때 그것이 좋고 나쁨을 어떻게 평가할 수 있을까? 여기서 우리는 cost function을 정의할 수 있겠다. | cost function은 모델의 예측값이 실제 데이터와 얼마나 다른지를 나타내는 값으로 잘 학습된 모델일수록 낮은 cost를 가질 것이다. | 우리의 예제에서는 w가 1일 때 모든 데이터가 정확히 들어맞기 때문에 이때 cost가 0이다 | 그리고 w가 1에서 멀어질수록 예측값과 실제 데이터가 달라지므로 cost가 높아질 것이다. | | . | linear regression에서 사용되는 cost function은 MSE(예측값과 실제값의 차이를 제곱한 것의 평균)이다. | 우리가 원하는 cost function을 최소화하는 것이다. | 그러기 위해선 일단 위의 w-cost 그래프를 보자 | 기울기가 음수일 때는 W가 더 커져야 하고 기울기가 양수일 땐 W가 더 작아져야 한다. | 또한 기울기가 가파를수록 cost가 큰 것이니 w를 많이 바꿔야 하고 기울기가 완만할 땐 기울기가 가파를 때에 비해 w를 조금만 바꾸면 된다. | 우리는 이 기울기를 gradient라고 한다. | 이 gradient를 계산하기 위해선 미분을 해야하는데 cost function은 결국 w에 대한 2차 함수이기 때문에 간단한 미분 방정식을 통해 계산할 수 있다. | | 아까 설명했듯 gradient 즉 기울기가 양수일 때는 w를 줄이고 기울기가 음수일 때는 w를 늘려야 한다. | | 이러한 방법이 Gradient Descent이다. | . | . x_train = torch.FloatTensor([[1],[2],[3]]) y_train = torch.FloatTensor([[1],[2],[3]]) # 모델 초기화 W = torch.zeros(1) # learning rate 설정 lr = 0.1 nb_epochs = 10 for epoch in range(nb_epochs + 1): # H(x) 계산 hypothesis = x_train * W # cost gradient 계산 cost = torch.mean((hypothesis - y_train)**2) gradient = torch.sum((W * x_train - y_train)*x_train) print(&#39;Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}&#39;.format(epoch, nb_epochs,W.item(), cost.item())) # cost gradient로 H(x) 개선 W-=lr*gradient . Epoch 0/10 W: 0.000, Cost: 4.666667 Epoch 1/10 W: 1.400, Cost: 0.746666 Epoch 2/10 W: 0.840, Cost: 0.119467 Epoch 3/10 W: 1.064, Cost: 0.019115 Epoch 4/10 W: 0.974, Cost: 0.003058 Epoch 5/10 W: 1.010, Cost: 0.000489 Epoch 6/10 W: 0.996, Cost: 0.000078 Epoch 7/10 W: 1.002, Cost: 0.000013 Epoch 8/10 W: 0.999, Cost: 0.000002 Epoch 9/10 W: 1.000, Cost: 0.000000 Epoch 10/10 W: 1.000, Cost: 0.000000 . Epoch이 증가할수록 W가 1에 수렴하고 Cost역시 줄어듦을 알 수 있다. | gradient descesnt를 더 편리하게 할 수 있도록 torch.optim을 이용할 수 있다. | . import torch.optim as optim optimizer = optim.SGD([W,b], lr = 0.01) # Optimizer를 정의해준다. # 우리의 모델에서 학습시킬 변수는 Weight와 Bias이기 때문에 이 2개를 list로 # 만들어서 입력해준뒤 적당한 learning rate도 입력해준다. optimizer.zero_grad() # gradient를 0으로 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선, 즉 실행한다. . | 이와 같이 이용할 수 있겠다. | . 지금까지 우리는 하나의 정보로부터 하나의 정보를 추측하는 모델을 만들었다. 예를 들어 한 학생읭 수업 참여도를 알 때 점수를 추측할 수 있는 모델 같은 게 있을 수 있다. 하지만 좋은 추측을 하기 위해선 정보 양이 많이 필요하다. | 예를 들어 한 학생의 점수를 추측할 때, 쪽지시험 하나의 성적보다 그 수업 때 진행된 모든 쪽지시험의 점수를 알면 좋듯 많은 정보는 추측의 질을 높여준다. | . | . | . 다음시간엔 여러개의 정보로부터 모델을 만들어보자 | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/26/intro.html",
            "relUrl": "/2022/01/26/intro.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "2022/01/13/THU",
            "content": "View(Reshape) . import numpy as np import torch t=np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]]) ft=torch.FloatTensor(t) print(ft.shape) . torch.Size([2, 2, 3]) . t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) . print(ft.view([-1,3])) # view사용해서 모양 바꾸자 print(ft.view([-1,3]).shape) . tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]]) torch.Size([4, 3]) . 두개의 차원으로 변경할 건데 앞 차원은 모르겠고 뒷 차원은 세개의 element를 가질래 | . print(ft.view([-1,1,3])) print(ft.view([-1,1,3]).shape) . tensor([[[ 0., 1., 2.]], [[ 3., 4., 5.]], [[ 6., 7., 8.]], [[ 9., 10., 11.]]]) torch.Size([4, 1, 3]) . 세개의 차원으로 변경할 건데 첫번째 차원은 아직 모르겠고 두번째 차원과 세번째 차원은 각각 1과3으로 변경해줘 | . Squeeze . ft=torch.FloatTensor([[0],[1],[2]]) print(ft) print(ft.shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . print(ft.squeeze()) print(ft.squeeze().shape) . tensor([0., 1., 2.]) torch.Size([3]) . 차원이 줄어들었음 | 1만 압축해주는 것 같음 | . print(ft.squeeze(dim=0)) print(ft.squeeze(dim=1)) # 해당 차원이 1일 때만! . tensor([[0.], [1.], [2.]]) tensor([0., 1., 2.]) . Unsqueeze . ft = torch.Tensor([0,1,2]) print(ft.shape) . torch.Size([3]) . 첫번째 차원에 1을 넣자 | . print(ft.unsqueeze(0)) print(ft.unsqueeze(0).shape) . tensor([[0., 1., 2.]]) torch.Size([1, 3]) . print(ft.view(1,-1)) print(ft.view(1,-1).shape) . tensor([[0., 1., 2.]]) torch.Size([1, 3]) . 두번째 차원에 1을 넣자 | . print(ft.unsqueeze(1)) print(ft.unsqueeze(1).shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . -1=(dim=1) | . print(ft.unsqueeze(-1)) print(ft.unsqueeze(-1).shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . Type Casting . lt = torch.LongTensor([1,2,3,4]) print(lt) . tensor([1, 2, 3, 4]) . print(lt.float()) . tensor([1., 2., 3., 4.]) . bool형일 때 | . bt=torch.ByteTensor([True,True,True]) print(bt) . tensor([1, 1, 1], dtype=torch.uint8) . print(bt.long()) print(bt.float()) . tensor([1, 1, 1]) tensor([1., 1., 1.]) . Concatenate . x=torch.FloatTensor([[1,2],[3,4]]) y=torch.FloatTensor([[5,6],[7,8]]) . print(torch.cat([x,y], dim=0)) print(torch.cat([x,y], dim=1)) . tensor([[1., 2.], [3., 4.], [5., 6.], [7., 8.]]) tensor([[1., 2., 5., 6.], [3., 4., 7., 8.]]) . Stacking . x=torch.FloatTensor([1,4]) y=torch.FloatTensor([2,5]) z=torch.FloatTensor([3,6]) . print(torch.stack([x,y,z])) print(torch.stack([x,y,z],dim=1)) . tensor([[1., 4.], [2., 5.], [3., 6.]]) tensor([[1., 2., 3.], [4., 5., 6.]]) . print(torch.cat([x.unsqueeze(0),y.unsqueeze(0),z.unsqueeze(0)],dim=0)) . tensor([[1., 4.], [2., 5.], [3., 6.]]) . Ones and Zeros . x=torch.FloatTensor([[0,1,2],[2,1,0]]) print(torch.ones_like(x)) print(torch.zeros_like(x)) . tensor([[1., 1., 1.], [1., 1., 1.]]) tensor([[0., 0., 0.], [0., 0., 0.]]) . In-place Operation . x=torch.FloatTensor([[1,2],[3,4]]) . print(x.mul(2)) print(x) print(x.mul_(2)) print(x) . tensor([[2., 4.], [6., 8.]]) tensor([[1., 2.], [3., 4.]]) tensor([[2., 4.], [6., 8.]]) tensor([[2., 4.], [6., 8.]]) . 즉 언더바를 통해 inplace=True 효과! | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/13/intro.html",
            "relUrl": "/2022/01/13/intro.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2022/01/12/WED",
            "content": "import torch . t=torch.FloatTensor([0.,1.,2.,3.,4.,5.,6.]) print(t) . tensor([0., 1., 2., 3., 4., 5., 6.]) . print(t.dim()) print(t.shape) print(t.size()) print(t[0],t[1],t[-1]) . 1 torch.Size([7]) torch.Size([7]) tensor(0.) tensor(1.) tensor(6.) . 그 외에도 slicing도 ndarray처럼 사용 가능 | 2차원 3차원도 ndarray처럼 사용 가능 | . Broadcasting . m1 = torch.FloatTensor([[3,3]]) m2 = torch.FloatTensor([[2,2]]) print(m1+m2) . tensor([[5., 5.]]) . 이렇게 같은 크기뿐만 아니라 Broadcasting을 통해 | . m1 = torch.FloatTensor([[1,2]]) m2 = torch.FloatTensor([[3]]) print(m1+m2) . tensor([[4., 5.]]) . 이렇게 크기가 동일하지 않아도 pytorch가 자동으로 동일한 size로 변환하여 연산 수행 가능 | . m1 = torch.FloatTensor([[1,2]]) m2 = torch.FloatTensor([[3],[4]]) print(m1+m2) . tensor([[4., 5.], [5., 6.]]) . print(m1.shape) . torch.Size([1, 2]) . m1 . tensor([[1., 2.]]) . print(m2.shape) . torch.Size([2, 1]) . m2 . tensor([[3.], [4.]]) . 이렇게 행렬 모양이 다르더라도 m1과 m2를 각각 2x2행렬로 변경하여 덧셈 수행하는 기능도 가능하다 | 이런 Broadcasting 기능은 자동으로 수행되기 때문에 잘못 사용하지 않게 유의하자 | . m1 = torch.FloatTensor([[1,2],[3,4]]) m2 = torch.FloatTensor([[3],[4]]) print(m1*m2) print(m1.mul(m2)) . tensor([[ 3., 6.], [12., 16.]]) tensor([[ 3., 6.], [12., 16.]]) . 이때 m2가 Broadcasting되면서 우리가 일반적으로 알고 있는 행렬 연산 수행과는 상이하게 진행됨 | 우리가 알고 있는 행렬 연산을 하기 위해서는 | . m1 = torch.FloatTensor([[1,2],[3,4]]) m2 = torch.FloatTensor([[3],[4]]) print(m1.matmul(m2)) . tensor([[11.], [25.]]) . 이렇게 해야한다. | . t = torch.FloatTensor([1,2]) print(t.mean()) . tensor(1.5000) . t = torch.FloatTensor([[1,2],[3,4]]) print(t.mean(dim=0)) # dim=0을 없애겠다. (2x2) -&gt; (1x2) print(t.mean(dim=1)) # dim=1을 없애겠다. (2x2) -&gt; (2x1) print(t.mean(dim=-1)) . tensor([2., 3.]) tensor([1.5000, 3.5000]) tensor([1.5000, 3.5000]) . t = torch.FloatTensor([[1,2],[3,4]]) print(t) . tensor([[1., 2.], [3., 4.]]) . print(t.sum()) print(t.sum(dim=0)) print(t.sum(dim=1)) print(t.sum(dim=-1)) . tensor(10.) tensor([4., 6.]) tensor([3., 7.]) tensor([3., 7.]) . t = torch.FloatTensor([[3,2],[1,4]]) print(t) . tensor([[3., 2.], [1., 4.]]) . print(t.max()) . tensor(4.) . print(t.max(dim=0)) # dim=0 제외하고 각 열에서의 max와 그 때의 index값을 알려줌 print(t.max(dim=0)[0]) # max값 print(t.max(dim=0)[1]) # max값의 index 즉, argmax . torch.return_types.max( values=tensor([3., 4.]), indices=tensor([0, 1])) tensor([3., 4.]) tensor([0, 1]) . print(t.max(dim=1)) print(t.max(dim=1)[0]) print(t.max(dim=1)[1]) . torch.return_types.max( values=tensor([3., 4.]), indices=tensor([0, 1])) tensor([3., 4.]) tensor([0, 1]) .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/12/intro.html",
            "relUrl": "/2022/01/12/intro.html",
            "date": " • Jan 12, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rhkrehtjd.github.io/INTROdl/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rhkrehtjd.github.io/INTROdl/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}