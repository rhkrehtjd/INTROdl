{
  
    
        "post0": {
            "title": "신경망",
            "content": "신경망을 통해 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 이제부터 살펴볼 신경망의 중요한 성질이다. | 신경망의 개요를 알아보고, 신경망이 입력 데이터가 무엇인지 식별하는 처리 과정을 자세히 알아보자. | . 퍼셉트론에서 신경망으로 . 입력층, 은닉층, 출력층으로 구성된다. | 이 중 은닉층의 뉴런은 사람 눈에는 보이지 않는다. | 뉴런이 연결되는 방식은 앞 장의 퍼셉트론과 달라진 것이 없다. | 차이점에 대해 알아보자 | . 활성화 함수 . 입력 신호의 총합을 출력 신호로 변환하는 함수를 일컫는다. | 활성화라는 이름이 말해주듯 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다. | 임계값을 경계로 출력이 바뀌는데 이런 함수를 계단 함수라고 한다. | 그래서 퍼셉트론에서는 활성화 함수로 계단 함수를 이용한다라고 할 수 있다. | 즉 활성화 함수로 쓸 수 있는 여러 후보 중에서 퍼셉트론은 계단 함수를 채용하고 있다. | 그렇다면 계단 함수 이외의 함수를 사용하면 어떻게 될까. 사실 활성화 함수를 계단 함수에서 다른 함수로 변경하는 것이 신경망의 세계로 나아가는 열쇠이다. | 신경망에서 이용하는 활성화 함수를 알아보자 | . 신경망에서 자주 이용하는 활성화 함수인 시그모이드 함수를 알아보자 $ frac{1}{1+exp(-x)}$ | . | . 신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다. | 앞 장에서본 퍼셉트론과 앞으로 볼 신경망의 주된 차이는 이 활성화 함수이다. | 계단 함수를 구현해보자 | . import numpy as np def step_function(x): if x&gt;0: return 1 else : return 0 . 인수 x는 실수만 받아들인다. 즉 step_function(3.0) 같은 것은 가능하지만 step_function(np.array([1,2]))는 안 된다. | 가능하도록 해보자 | . def step_function(x): y = x &gt; 0 return y.astype(np.int) . 아래 셀을 통해 위 함수를 이해해보자 | . x = np.array([-1,1,2]) y = x&gt;0 y . array([False, True, True]) . 이렇게 넘파이 배열에 부등호 연산을 수행하면 배열의 원소 각각에 부등호 연산을 수행한 bool 배열이 새로 생성되고 y라는 변수에 저장된다. | 그런데 우리가 원하는 계단 함수는 0이나 1의 int형을 출력하는 함수이기 때문에 아래 셀을 통해 변환하여 반환해주자 | . import warnings warnings.filterwarnings(&quot;ignore&quot;) y = y.astype(np.int) y . array([0, 1, 1]) . 이제 앞에서 정의한 계단 함수를 그래프로 그려보자 | . import matplotlib.pyplot as plt def step_function(x): return np.array(x&gt;0 , dtype=np.int) x = np.arange(-5,5,0.1) y = step_function(x) plt.plot(x,y) plt.ylim(-0.1,1.1) plt.show() . 시그모이드 함수를 구현해보자 | . def sigmoid(x): return 1/(1 + np.exp(-x)) x = np.array([-1,1,2]) sigmoid(x) . array([0.26894142, 0.73105858, 0.88079708]) . 시그모이드 함수를 그래프로 그려보자 | . x = np.arange(-5, 5, 0.1) y = sigmoid(x) plt.plot(x,y) plt.ylim(-0.1,1.1) plt.title(&#39;Sigmoid Function&#39;) plt.show() . 잡담 : 시그모이드란 &#39;S&#39;자 모양이라는 뜻이다. 계단 함수처럼 그 모양을 따 이름을 지은 것이다. &#39;S자 모양 함수&#39;라고도 부를 수 있다. | . . fig, (ax1,ax2) = plt.subplots(1,2) x = np.arange(-5,5,0.1) y = step_function(x) ax1.plot(x,y) ax1.set_title(&#39;Step Function&#39;) x = np.arange(-5, 5, 0.1) y = sigmoid(x) ax2.plot(x,y) ax2.set_title(&#39;Sigmoid Function&#39;) plt.show() . 비교 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변한다. 한편 계단 함수는 0을 경계로 출력이 갑자기 바뀌어버린다. 시그모이드 함수의 이 매끈함이 신경망 학습에서 아주 중요한 역할을 한다. | 다시 말해 퍼셉트론에서는 뉴런 사이에 0 혹은 1이 흘렀다면 신경망에서는 연속적인 실수가 흐른다. | 공통점 : 둘 다 입력이 작을 때의 출력은 0에 가깝고 혹은 0이고 입력이 커지면 출력이 1에 가까워지는 혹은 1이 되는 구조이다. | 즉, 계단 함수와 시그모이드 함수는 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다. | 그리고 입력이 아무리 작거나 커도 출력은 0에서 1 사이라는 것도 둘의 공통점이다. | 또한 둘다 비선형 함수이다. | . | . 신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다. 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문이다. | . . 시그모이드 함수는 신경망 분야에서 오래전부터 이용해왔으나 최근에는 ReUL(Rectified Linear Unit) 렐루 함수를 주로 이용한다. | 렐루 함수는 입력이 0을 넘으면 그 입력을 그대로 출력하고 0 이하이면 0을 출력하는 함수이다. | . def relu(x): return np.maximum(0,x) x = np.arange(-6 ,6 ,0.1) y = relu(x) plt.plot(x,y) plt.ylim(-1,7) plt.show() . ReLU 함수도 활성화 함수로 이용할 수 있다. | . . 넘파이의 다차원 배열에 대해 간략히 알아보자 | . b = np.array([[1,2],[3,4],[5,6]]) b . array([[1, 2], [3, 4], [5, 6]]) . print(np.ndim(b)) print(np.shape(b)) . 2 (3, 2) . $3 times 2$ 배열이다. | 처음 차원에는 원소가 3개 다음 차원에는 원소가 2개 들어있다는 의미이다. | 이때 처음 차원은 0번째 차원, 다음 차원은 1번째 차원에 대응한다. 2차원 배열은 행렬이라고 부르고 가로방향을 행, 세로방향을 열이라고 부른다. | . | . np.dot() . 은 입력이 1차원 배열이면 벡터를, 2차원 배열이면 행렬 곱을 계산한다. . 주의할 것은 np.dot(A,B)와 np.dot(B,A)는 다른 값이 될 수 있다.(우연의 일치가 발생할 순 있다.) . 행렬의 곱은 *과는 다르다 | . . a = np.array([2,3]) # 입력 값 b = np.array([[1,2,3],[4,5,6]]) # 가중치 # 편향과 활성화 함수를 생략하고 가중치만 갖는 간단한 신경망이다. np.dot(a,b) . array([14, 19, 24]) . . def identity_function(x): return x def init_network(): network = {} # dict 형태로 이용할 것 network[&#39;W1&#39;] = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]]) network[&#39;b1&#39;] = np.array([0.1,0.2,0.3]) network[&#39;W2&#39;] = np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]]) network[&#39;b2&#39;] = np.array([0.1,0.2]) network[&#39;W3&#39;] = np.array([[0.1,0.3],[0.2,0.4]]) network[&#39;b3&#39;] = np.array([0.1,0.2]) return network def forward(network, x): W1,W2,W3 = network[&#39;W1&#39;],network[&#39;W2&#39;],network[&#39;W3&#39;] b1,b2,b3 = network[&#39;b1&#39;],network[&#39;b2&#39;],network[&#39;b3&#39;] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) # 1층 a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) # 2층 a3 = np.dot(z2,W3) + b3 y = identity_function(a3) # 3층 return y network = init_network() x = np.array([1,0.5]) y = forward(network, x) print(y) . [0.31682708 0.69627909] . network . {&#39;W1&#39;: array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]), &#39;b1&#39;: array([0.1, 0.2, 0.3]), &#39;W2&#39;: array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]), &#39;b2&#39;: array([0.1, 0.2]), &#39;W3&#39;: array([[0.1, 0.3], [0.2, 0.4]]), &#39;b3&#39;: array([0.1, 0.2])} .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/05/dl.html",
            "relUrl": "/2022/02/05/dl.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "퍼셉트론",
            "content": "퍼셉트론을 구현해보자 AND 게이트 구현 | . | . def AND(x1, x2): w1,w2,theta = 0.5,0.5,0.7 tmp = x1*w1+x2*w2 if tmp&lt;=theta: return 0 elif tmp&gt;theta: return 1 print(AND(0,0)) print(AND(1,0)) print(AND(0,1)) print(AND(1,1)) . 0 0 0 1 . 물론 앞에서 구현한 AND 게이트는 직관적이고 알기 쉽지만, 앞으로를 생각해서 다른 방식으로 수정해보자. 편향이라는 개념을 도입하여, 퍼셉트론은 입력신호에 가중치를 곱한 값과 편향을 더하여, 그 값이 0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력한다. | . | . import numpy as np x = np.array([0,1]) # 입력 w = np.array([0.5,0.5]) # 가중치 b = -0.7 # 편향 print(w*x) print(np.sum(w*x)) print(np.sum(w*x)+b) . [0. 0.5] 0.5 -0.19999999999999996 . . 가중치와 편향을 도입한 AND 게이트는 다음과 같이 구현할 수 있다. | . def AND(x1,x2): x = np.array([x1,x2]) w = np.array([0.5,0.5]) b = -0.7 tmp = np.sum(x*w) + b if tmp&lt;=0 : return 0 elif tmp &gt;0 : return 1 . w1,w2는 각 입력 신호가 결과에 주는 영향력을 조절하는 매개변수이고, 편향을 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수이다. | . def NAND(x1,x2): x = np.array([x1,x2]) w = np.array([-0.5,-0.5]) b = 0.7 tmp = np.sum(x*w) + b if tmp&lt;=0 : return 0 elif tmp &gt;0 : return 1 def OR(x1,x2): x = np.array([x1,x2]) w = np.array([0.5,0.5]) b = -0.2 tmp = np.sum(x*w) + b if tmp&lt;=0 : return 0 elif tmp &gt;0 : return 1 . AND, NAND, OR 모두 같은 구조의 퍼셉트론이고 차이는 가중치 매개변수의 값뿐이다. 실제로 파이썬으로 작성한 NAND와 OR 게이트의 코드에서도 AND와 다른 곳은 가중치와 편향 값을 설정하는 부분뿐이다. | . . XOR 게이트 배타적 논리합이라는 논리회로이다. 한쪽이 1일때만 1을 출력. OR게이트와 달리 둘 다 1일때는 출력하지 않는다. | 지금까지 본 퍼셉트론의 구조로는 이 XOR 게이트를 구현할 수 없다. | . | . import matplotlib.pyplot as plt plt.scatter([0,0,1,1],[0,1,0,1]) plt.xlim(0,2) plt.ylim(0,2) . (0.0, 2.0) . (0,0)(1,1),(1,0)(0,1)이 두 묶음을 각각 나눌 수 있는 직선은 있을 수 없다. | 하지만 직선이라는 제약을 없앤다면 가능하다 | 퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가 있다. 곡선은 표현할 수 없다. | 퍼셉트론으로는 XOR 게이트를 표현할 수 없지만 층을 쌓아 다층 퍼셉트론을 통해 구현할 수 있다. | . . AND, OR NAND 게이트를 조합하여 XOR게이트를 만들 수 있다. | 단층 퍼셉트론으로는 XOR 게이트를 표현할 수 없다. 단층 퍼셉트론으로는 비선형 영역을 분리할 수 없다. | 퍼셉트론을 조합하여 즉, 층을 쌓아서 XOR 게이트를 구현하는 것이다. | . def XOR(x1,x2): s1 = NAND(x1,x2) s2 = OR(x1,x2) y = AND(s1,s2) return y . print(XOR(0,0)) print(XOR(0,1)) print(XOR(1,0)) print(XOR(1,1)) . 0 1 1 0 . 이처럼 퍼셉트론은 층을 쌓아 더 다양한 것을 표현할 수 있다. | . . Conclusion . 퍼셉트론은 입출력을 갖춘 알고리즘이다. 입력을 주면 정해진 규칙에 따른 값을 출력한다. | 퍼셉트론에서는 가중치와 편향을 매개변수로 설정한다. | 퍼셉트론으로 AND,OR 게이트 등의 논리 회로를 표현할 수 있다. | XOR 게이트는 단층 퍼셉트론으로는 표현할 수 없다. | 2층 퍼셉트론을 이용하면 XOR 게이트를 표현할 수 있다. | 단층 퍼셉트론은 직선형 영역만 표현할 수 있고, 다층 퍼셉트론은 비선형 영역도 표현할 수 있다. | 다층 퍼셉트론은 (이론상) 컴퓨터도 표현할 수 있다. | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/02/04/dl.html",
            "relUrl": "/2022/02/04/dl.html",
            "date": " • Feb 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2022/01/29/SAT",
            "content": "Softmax Classfication Logistic Regression의 연장선에 있다고 볼 수 있다. | . | . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # 재현성을 위하여 torch.manual_seed(1) . &lt;torch._C.Generator at 0x208d7a60b70&gt; . Discrete Probability Distribution 이산적인 확률 분포를 말한다. 확률 분포에는 연속적인 확률 분포와 이산적인 확률 분포가 있다. | 가령 우리가 주사위를 던져서 주사위가 6개중 하나의 숫자가 나오게 되는데 이러한 경우 혹은 가위바위보 이런 경우를 이산적인 확률 분포라고 한다. | 주사위의 PMF(Probability Mass Function) 확률질량함수는 아래와 같다. Uniform Distribution | | . | 가위 바위 보도 동일하게 정의할 수 있을 것이다. Uniform Distribution | | . | 일반적으로 알고 있는 확률분포함수pdf에서는 | | 위와 같이 생길 수 있는데 면적이 확률을 의미한다. 어떤 지점 자체는 확률을 의미하지 않는다. 알 수도 없다. | discrete에서는 지점의 확률을 구할 수 있다. | 그래서 이러한 이산적인 확률 분포를 바탕으로 머신러닝을 수행한다고 생각해보자 | 가위바위보 상황에서 이전이 상대방이 낸 것을 바탕으로 해서 다음에 상대방이 낼 것을 예측할 수 있을 것이다. 완전히 랜덤은 아닐 것이고 사람마다 일정한 어떤 패턴이 있을 것이다. 즉 확률 분포함수가 있다는 것이다. 예전에 철수가 가위를 냈을 떄 다음에 주먹을 낼 확률은 얼마? 이런 것이다. 이렇게 정의할 수 있을 것이다. | P(주먹 | 가위) = ? 혹은 P(가위 | 가위) 혹은 P(보 | 가위), 이 세가지를 알 수 있다면 상대방이 가위를 냈을 떄 다음에 무엇을 낼 지 예측할 수 잇을 것이다. | 분명히 이런 패턴의 확률 분포가 있을 것이고 이 확률 분포를 근사해야한다. | . | softmax라는 함수가 있는데 말 그대로 max값을 뽑아주는데 soft하게 뽑아준다. | softmax | | 기존에 max를 뽑는다면.. 아래를 살펴보자 | . | . z = torch.FloatTensor([1,2,3]) . 위와 같이 주어졌을 떄, argmax 값은 max=(0,0,1)이 됐을 것이다. | 그런데 softmax는 가볍게 뽑아준다. 부드럽게 뽑아준다. 즉, 위 처럼 (0,0,1) 대신에 합쳐서 1이 되는,, 비율에 따라서 나타내 줄 수 있겠다. | 확률 값으로 볼 수도 있겠다. 아래를 살펴보자 | . hypothesis = F.softmax(z, dim=0) print(hypothesis) . tensor([0.0900, 0.2447, 0.6652]) . 각각의 이 세 값들은 위 사진 수식에 따라서 활용이 될 것이다. | 예를 들어 첫 번쨰 값인 0.09같은 경우에는 | | 이와 같이 구해졌을 것이다. | . hypothesis.sum() . tensor(1.) . softmax값은 1이 된다. | 우리는 이를 이용해 철수가 가위를 냈을 때 다음에 어떤 것을 낼지 확률 분포를 근사할 수 있겠다, | . . Cross Entropy 이러한 두개의 확률 분포가 주어졌을 때 두 확률 분포가 얼마나 비슷한지를 나타내는 수치라고 볼 수 있다 | 수식을 살펴보자 | | 좀 더 직관적으로 나타내보자 | | 예를 들어서 맨 왼쪽에 있는 분포가 P 가운데가 Q_1 맨 오른쪽에 있는 분포가 Q_2라고 했을 때, P에서 샘플링한다는 (사진상에서 E밑에있는 P가 P에서 샘플링한다는 것을 의미함) 것은 P에 해당되는 density대로 샘플링이 되겠다. | 예를 들어 사진상에서 빨간동그라미에서 점이 뽑혔다면, 그 위에서 Q_1과 Q_2에 해당될 것이다. | 즉, 우리는 이 log를 취하고 -를 붙여줬기 때문에, Q_1의 값이 Q_2의 값보다 훨씬 크게 될 것이다. | | 따라서 만약에 철수가 가위를 냈을 때 다음에 무엇을 낼지에 대한 확률 분포함수가 있을 것인데 그것을 P라고 했을 떄, 우리는 이 cross entropy를 구해서 cross entropy 이것을 최소화하도록 하면 Q_2에서 Q_1으로 그리고 P로 다가갈 수 있을 것이다. | 그래서 우리가 가지고 있는 모델의 확률 분포함수는 점점 P에 근사하게 될 것이다. | 따라서 cross entropy를 최소화하는 것이 중요하다. | . | . corss entropy loss 손실함수를 계산해보자 cross entropy는 아래와 같이 수식으로 계산할 수 있겠다. | | . | . z = torch.rand(3 ,5 ,requires_grad = True) hypothesis = F.softmax(z,dim=1) # 두 번째 행에 대해서 softmax를 수행하라. print(hypothesis) . tensor([[0.2441, 0.1429, 0.2298, 0.2344, 0.1487], [0.1665, 0.2504, 0.2309, 0.1707, 0.1815], [0.2733, 0.1576, 0.2292, 0.2147, 0.1252]], grad_fn=&lt;SoftmaxBackward0&gt;) . 이것이 사실은 예측값 즉 yhat이 될 것이다. | 정답이 뭔지 알아보자(지금은 따로 정답이 없기 때문에 랜덤으로 정답을 생성해보자) | . y = torch.randint(5, (3,)).long() print(y) . tensor([1, 0, 0]) . 우리는 각각의 샘플에 대해서 정답 인덱스를 구했다고 볼 수 있다. | 첫 번째 행에서 1인덱스 두 번째 행에서 0인덱스 세 번째 행에서 0인덱스를 의미한다. | . one hot vector로 나타내보자 | . y_one_hot = torch.zeros_like(hypothesis) y_one_hot.scatter_(1, y.unsqueeze(1),1) # tensor([1,0,0]) . tensor([[0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.]]) . cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean() print(cost) . tensor(1.6784, grad_fn=&lt;MeanBackward0&gt;) . sim(dim=1)은 3 x 1이 남을 것이다. | . . Cross-entropy Loss with torch.nn.functional | . torch.log(F.softmax(z, dim=1)) # low lovel . 을 생략하고 아래와 같이 이용할 수 있겠다 . | . F.log_softmax(z, dim=1) . tensor([[-1.4101, -1.9457, -1.4703, -1.4507, -1.9056], [-1.7925, -1.3848, -1.4657, -1.7679, -1.7066], [-1.2971, -1.8475, -1.4731, -1.5385, -2.0782]], grad_fn=&lt;LogSoftmaxBackward0&gt;) . 한 번 더 간편하게 이용할 수 있다. | NLL (Negative Log Likelihood) | . F.nll_loss(F.log_softmax(z,dim=1),y) . tensor(1.6784, grad_fn=&lt;NllLossBackward0&gt;) . 위 값과 동일함을 알 수 있다. | 더 단순하게 수행할 수 잇다. | . F.cross_entropy(z,y) . tensor(1.6784, grad_fn=&lt;NllLossBackward0&gt;) . 값이 동일함을 알 수 있다. | 필요에 따라서 원하는 값을 사용하면 된다. | . . Training with Low-level Cross Entripy Loss | 손실함수를 통해서 직접 최적화, 학습을 해보자 | . x_train = [[1,2,1,1], [2,1,3,2], [3,1,3,4], [4,1,5,5], [1,7,5,5], [1,2,5,6], [1,6,6,6], [1,7,7,7]] y_train = [2,2,2,1,1,1,0,0] x_train = torch.FloatTensor(x_train) y_train = torch.FloatTensor(y_train) . 이와 같이 학스부data가 주어져있다고 해보자. | x_train이 m x 4 라고 해보고 그렇다면 y_train도 m개가 있을 것이다. 즉 4차원의 벡터를 받아서 어떤 클래스인지 예측하도록 하고싶다. | 여기서 y_train은 one-hot벡터로 나타냈을 때, 1이 있는 위치에서의 인덱스 값일 것이다. | 코드를 작성해보자 | . # 모델 초기화 W = torch.zeros((4,3), requires_grad = True) b = torch. zeros(1, requires_grad = True) # optimizer 설정 optimizer = optim.SGD([W,b], lr=0.1) nb_epochs = 1000 for epoch in range(nb_epochs +1): # cost 계산(1) hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) y_one_hot = torch.zeros_like(hypothesis) y_one_hot.scatter_(1, y_train.unsqueeze(1),1) cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번 마다 로그 출력 if epoch7 % 100 == 0 : print(&#39;Epoch {:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) . 좀 더 쉽게 구현을 해보자 | Training with F.cross_entropy | . # 모델 초기화 W = torch.zeros((4,3), requires_grad = True) b = torch. zeros(1, requires_grad = True) # optimizer 설정 optimizer = optim.SGD([W,b], lr=0.1) nb_epochs = 1000 for epoch in range(nb_epochs +1): # cost 계산(2) z = x_train.matmul(W)+b cost = F.cross_entropy(z, y_train) # scatter를 사용할 필요가 없어짐 # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번 마다 로그 출력 if epoch7 % 100 == 0 : print(&#39;Epoch {:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) . 좀 더 실전에 가깝게 쉽게 구현해보자 | High level Implementation with nn.Module | . class SoftmaxClassifierModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(4,3) def forward(self, x) : return self.linear(x) model = SoftmaxClassifierModel() . # optimizer 설정 optimizer = optim.SGD(model.parameters(), lr=0.1) nb_epochs = 1000 for epoch in range(nb_epochs +1): # H(x) 계산 prediction = model(x_train) # cost 계산 cost = F.cross_entropy(prediction, y_train) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번 마다 로그 출력 if epoch7 % 100 == 0 : print(&#39;Epoch {:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/29/intro.html",
            "relUrl": "/2022/01/29/intro.html",
            "date": " • Jan 29, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "2022/01/28/FRI",
            "content": "회귀 분석 회귀 분석은 데이터의 특성에 따라 선형 회귀와 비선형 회귀로 나눌 수 있다. | 데이터 분포에 따라 선형적 특성을 보이면 선형 회귀를 사용하고 데이터가 비선형적 특성을 보이면 비선형 회귀를 사용한다. | 이러한 방법을 사용하는 회귀분석의 목적은 컴퓨터가 보유한 데이터로부터 데이터의 특성을 스스로 학습하고 앞으로 데이터를 추정 및 예측하는 것이다. Linear Regression vs Logistic Regression | 두 가지 모두 회귀 분석의 종류이다. 즉, output의 형태만 다를 뿐이다. | 먼저 선형,비선형 회귀는 연속형 data를 input으로 하여 연속형 output을 추정하는 알고리즘이다. | 반면 Logistic Regression은 연속형 data를 input으로하여 이산형 output을 추정하는 알고리즘이다. | 즉 넣어주는 값의 형태는 동일하지만 도출되는 output의 형태가 다르다는 것이 두 알고리즘의 차이점이다.ref | | 왼쪽 그림은 초록색 또는 검은색의 연속적인 data를 쉽게 표현하는 선형,비선형 그래프를 추정하는 것이 목적이다. | 반면 오른쪽 그림은 0또는 1이라는 값으로 고양이 여부, 즉 binary한 값을 도출하는 알고리즘이다. | 여기서 input이 이미지로 이루어진 것처럼 보이지만 나중에 이미지는 컴퓨터가 계산할 수 있는 실수값으로 이루어진 어떤 tensor(다차원의 행렬로 보면 될 것 같다)값으로 변환되기 때문에 숫자로 이루어진 연속형 data로 보면 된다. | 이렇게 Logistic Regression (Binary Classification에 이용)입력된 data의 특징을 통해 0또는 1이라는 output의 값을 추정하는 것을 목표로 하는 알고리즘. 이를 위해 sigmoid라는 활성화 함수를 사용하게 된다. | | 간단히, 특정 범주에 속할 것인지에 대한 확률을 예측하는 것이기 때문에 Output이 반드기 0~1사이에 있어야하므로 sigmoid라는 활성화함수(스칼라값을 입력하면 스칼라값을 출력하는 함수)를 사용한다고 알아두자 | 예를 들어 고양이라고 생각하는 1에 해당하는 확률 값이 sigmoid를 통해 0.78이 나왔다면 컴퓨터는 0.78의 확률로 고양이라고 생각한다는 것이다. 마지막으로 Logistic Regression 알고리즘을 통해 추정된 yhat의 값이 정말 옳은 값인지 잘못된 예측인지 알아야 할 것이다. | 이러한 오차를 계산하기 위해 Cross Entropy가 사용됨 | | 이 방법을 통해 추정된 yaht의 값과 참값이 얼마나 다른지를 수학적으로 계산하여 하나의 숫자값으로 오차를 표현해준다. | 따라서 이 오차를 0으로 만드는 것이 Logistic Regression의 최종 목표이다. | . | . | 정리하자면 다음과 같다. | 오차를 줄이는 방향으로 알고리즘이 학습되어야 할 것이다. | . | . | . import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim # 재현성을 위하여 torch.manual_seed(1) # 사용될 data x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]] y_data = [[0],[0],[0],[1],[1],[1]] x_train = torch.FloatTensor(x_data) y_train = torch.FloatTensor(y_data) print(x_train.shape) print(y_train.shape) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy _distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs: C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll warnings.warn(&#34;loaded more than 1 DLL from .libs:&#34; . torch.Size([6, 2]) torch.Size([6, 1]) . Computing the Hypothesis | . print(&#39;e^1 equals: &#39;,torch.exp(torch.FloatTensor([1]))) . e^1 equals: tensor([2.7183]) . | . W = torch.zeros((2,1), requires_grad = True) # gradient를 배우겠다. b = torch.zeros(1, requires_grad = True) # gradient를 배우겠다. # 지금은 일단 0으로 줬음 hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b))) # x_train.matmul(W) = torch.matmul(x,W) print(hypothesis) print(hypothesis.shape) . tensor([[0.5000], [0.5000], [0.5000], [0.5000], [0.5000], [0.5000]], grad_fn=&lt;MulBackward0&gt;) torch.Size([6, 1]) . print(&#39;1/(1+e^{-1}) = &#39; , torch.sigmoid(torch.FloatTensor([1]))) . 1/(1+e^{-1}) = tensor([0.7311]) . 사실은 위 셀처럼 제공되어지고 있으니, 아래와 같이 간단히 써볼 수 있겠다. | . hypothesis = torch.sigmoid(x_train.matmul(W) + b) print(hypothesis) print(hypothesis.shape) . tensor([[0.5000], [0.5000], [0.5000], [0.5000], [0.5000], [0.5000]], grad_fn=&lt;SigmoidBackward0&gt;) torch.Size([6, 1]) . 값이 동일하게 나옴을 알 수 있다. | . . Computing the Cost Function | 위 사진에서 H(x)는 P(x = 1; W)로 표현할 수 있겠다. | . | 우리는 hypothesis와 y_train의 오차를 알고싶다. | . print(hypothesis) print(y_train) . tensor([[0.5000], [0.5000], [0.5000], [0.5000], [0.5000], [0.5000]], grad_fn=&lt;SigmoidBackward0&gt;) tensor([[0.], [0.], [0.], [1.], [1.], [1.]]) . 일단 한 개의 요소에 대해서만 계산해보자 | . -(y_train[0] * torch.log(hypothesis[0]) + (1 - y_train[0]) * torch.log(1 - hypothesis[0])) . tensor([0.6931], grad_fn=&lt;NegBackward0&gt;) . 아래 사진을 참고해보자 | | . 전체 sample에 대해 표현해보자 | . losses = -(y_train*torch.log(hypothesis) + (1-y_train)*torch.log(1-hypothesis)) print(losses) . tensor([[0.6931], [0.6931], [0.6931], [0.6931], [0.6931], [0.6931]], grad_fn=&lt;NegBackward0&gt;) . cost = losses.mean() print(cost) . tensor(0.6931, grad_fn=&lt;MeanBackward0&gt;) . 이 모든 것을 한 번에 해결해보자 | . F.binary_cross_entropy(hypothesis, y_train) . tensor(0.6931, grad_fn=&lt;BinaryCrossEntropyBackward0&gt;) . 동일한 값이 나옴을 알 수 있다. | . . Whole Training Procedure | . W = torch.zeros((2,1), requires_grad=True) b = torch.zeros(1, requires_grad=True) # optimizer 설정 optimizer = optim.SGD([W,b], lr=1) # SGD를 이용하여 learning rate=1인 상태로 W,b를 학습 nb_epochs = 1000 for epoch in range(nb_epochs + 1) : # cost 계산 hypothesis = torch.sigmoid(x_train.matmul(W) + b) cost = F.binary_cross_entropy(hypothesis, y_train) # cost로 H(x) 개선 optimizer.zero_grad() cost.backward() optimizer.step() # 100번마다 로그 출력 if epoch % 100 == 0 : print(&#39;Epoch{:4d}/{} Cost : {:.6f}&#39;.format(epoch, nb_epochs, cost.item())) . Epoch 0/1000 Cost : 0.693147 Epoch 100/1000 Cost : 0.134722 Epoch 200/1000 Cost : 0.080643 Epoch 300/1000 Cost : 0.057900 Epoch 400/1000 Cost : 0.045300 Epoch 500/1000 Cost : 0.037261 Epoch 600/1000 Cost : 0.031672 Epoch 700/1000 Cost : 0.027556 Epoch 800/1000 Cost : 0.024394 Epoch 900/1000 Cost : 0.021888 Epoch1000/1000 Cost : 0.019852 . cost값이 점점 줄어듦을 알 수 있다. | . . Evlauation | . hypothesis = torch.sigmoid(x_train.matmul(W) +b) print(hypothesis[:5]) . tensor([[2.7648e-04], [3.1608e-02], [3.8977e-02], [9.5622e-01], [9.9823e-01]], grad_fn=&lt;SliceBackward0&gt;) . 사실은 x_train이 아니라 x_test인 게 더 정확하다. | 간단히 해석해보면 순서대로 x=1일 확률을 의미한다. | . prediction = hypothesis &gt;= torch.FloatTensor([0.5]) print(prediction[:5]) # 예측 print(y_train[:5]) # 실제 정답 . tensor([[False], [False], [False], [ True], [ True]]) tensor([[0.], [0.], [0.], [1.], [1.]]) . 잘 예측했음을 알 수 있다. | . correct_prediction = prediction.float() == y_train print(correct_prediction) . tensor([[True], [True], [True], [True], [True], [True]]) . 5개에 대해선 동일함을 알 수 있다. | . . Higher Implementation with Class | . class BinaryClassifier(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(8,1) self.sigmoid = nn.Sigmoid() def forward(self, x): return self.sigmoid(self.linear(x)) . model = BinaryClassifier() . | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/28/intro.html",
            "relUrl": "/2022/01/28/intro.html",
            "date": " • Jan 28, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "2022/01/27/THU",
            "content": "Simple Linear Regression 하나의 정보로부터 하나의 결론을 짓는 모델이었다. 다음과 같다. | $H(x) = W(x) + b$ | 하지만 대부분의 경우 좀 더 복잡한 예측을 하기 위해선 다양한 정보가 필요하다. | 이를 위해 Multivariate Linear Regression을 알아보자 | . | Multivariate Linear Regression 복수의 정보가 존재할 때 어떻게 하나의 추측값을 계산할 수 있는지? | 예를 들어, 만약 쪽지시험들의 성적이 73,80,75점일때, 이 학생의 기말고사 점수가 몇점일지 예측해보자 | . | . import torch x_train = torch.FloatTensor([[73,80,75], [93,88,93], [89,91,90], [96,98,100], [73,66,70]]) y_train = torch.FloatTensor([[125],[185],[180],[196],[142]]) . Hypothesis Function은 인공신경망의 구조를 나타내는데 이번에도 동일하게 $H(x) = W(x) + b$를 이용하여 표현한다. 하지만 simple linear regression에서는 x에 하나의 정보밖에 없어서 x를 1x1 vector로 표현했다면 이번에는 x에 3개의 정보가 있으므로 아래와 같이 나타낼 수 있다. | | 즉 x를 3x1 vector로 나타낸다. | 그렇다면 이 Hypothesis function을 PyTorch에서 어떻게 계산할 수 있을까 | 단순한 hypothesis 정의를 이용하여 아래와 같이 작성할 수 있을 것이다. | | 하지만 x에 3개의 정보가 있을 때 이렇게 나열하는 게 가능할 수 있겠지만 더 많은 양의 정보를 x가 가지고 있다면 hypothesis를 계산하는 이 한줄은 점점 길어질 것이다. | 따라서 우리는 PyTorch에서 제공해주는 matmul이라는 함수를 사용하면 된다. | 코드는 아래와 같다.hypothesis = x_train.matmul(W) + b . | Multivariate Linear Regression의 Cost function은 Simple Linear Regression과 마찬가지로 MSE를 사용하며 계산 방식역시 동일하다. | 또한 Multivariate Linear Regression의 학습방식 또한 Gradient Descent with torch.optim로서 동일하다. | | 이제 완성된 코드를 한 번 작성해보자 | . import torch x_train = torch.FloatTensor([[73,80,75], [93,88,93], [89,91,90], [96,98,100], [73,66,70]]) y_train = torch.FloatTensor([[125],[185],[180],[196],[142]]) # 모델 초기화 W = torch.zeros((3,1), requires_grad=True) b = torch.zeros(1, requires_grad=True) # optimizer 설정 import torch.optim as optim optimizer = optim.SGD([W,b], lr = 1e-5) . nb_epochs = 20 for epoch in range(nb_epochs+1): # H(x) 계산 hypothesis = x_train.matmul(W) + b # cost 계산 cost = torch.mean((hypothesis - y_train)**2) # cost로 H(x)계산 optimizer.zero_grad() cost.backward() optimizer.step() print(&#39;Epoch {:4d}/{} hypothesis: {} Cost {:.6f}&#39;.format(epoch, nb_epochs, hypothesis.squeeze().detach(),cost.item())) . Epoch 0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost 28166.000000 Epoch 1/20 hypothesis: tensor([65.3835, 78.5927, 77.4353, 84.3257, 59.9476]) Cost 8919.982422 Epoch 2/20 hypothesis: tensor([101.9877, 122.5950, 120.7879, 131.5363, 93.5114]) Cost 2887.348145 Epoch 3/20 hypothesis: tensor([122.4795, 147.2313, 145.0590, 157.9675, 112.3039]) Cost 996.406433 Epoch 4/20 hypothesis: tensor([133.9505, 161.0253, 158.6470, 172.7651, 122.8264]) Cost 403.667480 Epoch 5/20 hypothesis: tensor([140.3712, 168.7492, 166.2539, 181.0495, 128.7189]) Cost 217.844818 Epoch 6/20 hypothesis: tensor([143.9643, 173.0745, 170.5123, 185.6873, 132.0192]) Cost 159.569305 Epoch 7/20 hypothesis: tensor([145.9744, 175.4972, 172.8959, 188.2836, 133.8682]) Cost 141.273163 Epoch 8/20 hypothesis: tensor([147.0982, 176.8546, 174.2299, 189.7369, 134.9047]) Cost 135.508194 Epoch 9/20 hypothesis: tensor([147.7258, 177.6156, 174.9762, 190.5503, 135.4863]) Cost 133.671143 Epoch 10/20 hypothesis: tensor([148.0756, 178.0428, 175.3936, 191.0054, 135.8133]) Cost 133.065277 Epoch 11/20 hypothesis: tensor([148.2699, 178.2830, 175.6268, 191.2600, 135.9977]) Cost 132.845520 Epoch 12/20 hypothesis: tensor([148.3771, 178.4185, 175.7569, 191.4022, 136.1022]) Cost 132.746689 Epoch 13/20 hypothesis: tensor([148.4356, 178.4955, 175.8292, 191.4816, 136.1620]) Cost 132.685730 Epoch 14/20 hypothesis: tensor([148.4667, 178.5396, 175.8692, 191.5258, 136.1968]) Cost 132.636658 Epoch 15/20 hypothesis: tensor([148.4826, 178.5654, 175.8911, 191.5503, 136.2177]) Cost 132.591431 Epoch 16/20 hypothesis: tensor([148.4900, 178.5809, 175.9028, 191.5637, 136.2306]) Cost 132.547455 Epoch 17/20 hypothesis: tensor([148.4925, 178.5906, 175.9090, 191.5710, 136.2392]) Cost 132.503815 Epoch 18/20 hypothesis: tensor([148.4924, 178.5971, 175.9119, 191.5748, 136.2453]) Cost 132.460098 Epoch 19/20 hypothesis: tensor([148.4908, 178.6018, 175.9130, 191.5766, 136.2500]) Cost 132.416672 Epoch 20/20 hypothesis: tensor([148.4883, 178.6055, 175.9132, 191.5774, 136.2540]) Cost 132.373291 . cost가 점점 작아짐을 알 수 있다. | H(x)도 점점 y에 가까워짐을 알 수 있다. | Learning rate에 따라 발산할 수도 있다. | . . W와 b를 일일히 작성해주는 것은 번거로운 일이 될 수 있다. | 따라서 PyTorch에서 제공해주는 nn.Module을 상속해서 모델을 생성하자 | . # 아래는 Full code이다. x_train = torch.FloatTensor([[73,80,75], [93,88,93], [89,91,90], [96,98,100], [73,66,70]]) y_train = torch.FloatTensor([[125],[185],[180],[196],[142]]) import torch.nn as nn class MultivariateLinearRegressionModel(nn.Module): def __init__(self): super().__init__() self.linear = nn.Linear(3,1) def forward(self,x): return self.linear(x) # 모델 초기화 model = MultivariateLinearRegressionModel() # optimizer 설정 optimizer = optim.SGD([W,b],lr=1e-5) import torch.nn.functional as F # cost 계산 cost = F.mse_loss(prediction, y_train) nb_epochs = 20 for epoch in range(nb_epochs+1): # H(x) 계산 hypothesis = model(x_train) # cost 계산 cost = F.mse_loss(prediction, y_train) # cost로 H(x)계산 optimizer.zero_grad() cost.backward() optimizer.step() print(&#39;Epoch {:4d}/{} hypothesis: {} Cost {:.6f}&#39;.format(epoch, nb_epochs, hypothesis.squeeze().detach(),cost.item())) . nn.linear (3,1) 입력 차원 = 3 | 출력 차원 = 1 | . | Hypothesis 계산은 forward에서 진행된다. | Gradient 계산은 Pytorch가 알아서 진행해준다. backward() | . . 또한 Pytorch에서는 다양한 cost function을 제공한다. | . import torch.nn.functional as F # cost 계산 cost = F.mse_loss(prediction, y_train) . 이렇게 사용하면 쉽게 다른 loss와 교체 가능하다. (l1_loss, smooth_l1_loss 등 ...) | 그런데 여기서 prediction이 정의되지 않았다고 함... | . 지금까지 적은 양의 데이터를 가지고 학습했다. | 하지만 딥러닝은 많은 양의 데이터와 함께할 때 빛을 발한다. | PyTorch에서는 많은 양의 데이터를 어떻게 다룰까? | . . 복잡한 머신러닝 모델을 학습하려면 엄청난 양의 데이터가 필요하다. | 대부분 dataset은 적어도 수십만 개의 데이터를 제공한다. | 엄청난 양의 데이터를 한 번에 학습시킬 수 없다 너무 느리다 | 하드웨어적으로 불가능하다. | . | 그렇다면 일부분의 데이터로만 학습하면 어떨까? 이렇게 해서 나온 아이디어가 Minibatch Gradient Descent이다 | 전체 데이터를 균일하게 나눠서 학습하자 | 아래 사진을 참고해보자 | | 보다 작은 단위인 Minibatch로 나누어서 Minibatch하나하나 학습하는 것이다. | 각 Minibatch에 있는 cost만 계산한 후에 Gradient descent 할 수 있기 때문에 컴퓨터에 무리가 덜 가게 된다. | 한 번에 Gradient descent를 하지 않기 때문에 업데이트를 좀 더 빠르게 할 수 있다 | 그렇지만 모델의 cost를 계산할 때 전체 데이터를 사용하지 않기 때문에 잘못된 방향으로 업데이트를 할 수도 있다. | 기존 Gradient descent처럼 매끄럽게 cost가 줄어들지 않고 좀 더 거칠게 줄어들게 된다. (각각 좌우의 그래프이다.) | | 이제 실제 dataset을 minibatch로 쪼개는 데에 사용되는 PyTorch Dataset과 module에 대해 알아보자 | . | . | . torch.utils.data.Dataset 상속 | __len__() 이 dataset의 총 data 수 | . | __getitem__() 어떠한 인덱스 idx르르 받았을 때, 그에 상응하는 입출력 데이터 반환 | . | . 이렇게 dataset을 만들었다면, PyTorch에서 module로 제공해주는 DataLoader를 사용할 수 있다. | | instance를 만드려면 두 개의 parameter를 지정해주어야 한다. | batch_size = 2 각 minibatch의 크기 | 통상적으로 2의 제곱수로 설정한다. | . | shuffle = True Epoch마다 dataset을 섞어서 데이터가 학습되는 순서를 바꾼다. | 이 옵션을 설정함으로써 우리의 모델이 dataset의 순서를 외우지 못하게 방지할 수 있으므로 권장하는 옵션이다. | . | . Full Code with Dataset and DataLoader | | enumerate(dataloader) minibatch 인덱스와 데이터를 받음 | . | len(dataloader) 한 epoch당 minibatch 개수 | . | . 지금까지 하나 또는 여러 개의 입력으로부터 어떤 숫자 하나를 예측하는 모델을 만들었다. | 다음시간엔 이렇게 숫자 하나를 예측하는 모델이 아니라 어떤 입력을 받았을 때 그것을 분류하는 모델에 대해서 알아보자 | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/27/intro.html",
            "relUrl": "/2022/01/27/intro.html",
            "date": " • Jan 27, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "2022/01/26/WED",
            "content": "PyTorch를 이용해서 Linear Regression을 작성해보자 . Data definition 공부한 시간과 점수와의 상관관계에 대해 알아보자 | | 학습 시간이 1,2,3일때를 Training dataset이라 한다. | 학습이 끝난 후 이 모델이 얼마나 잘 작동하는지 판별하기 위해 사용하는 data를 Test dataset이라 한다. | 이때 4시간 공부했을 때 점수는 몇점일까? | 본격적으로 코딩해보자 | . | . 모델을 학습시키기 위한 data는 torch.tensor를 이용한다. | 이때 입력과 출력은 각기 다른 dataset에 입력해준다. | . import torch x_train = torch.FloatTensor([[1],[2],[3]]) # 모델을 학습시키기 위한 입력 dataset y_train = torch.FloatTensor([[2],[4],[6]]) # 모델을 학습시키기 위한 출력 dataset . 입출력은 x,y로 구분한다. | 우리의 모델인 Hypothesis를 구현해보자 | Linear Regression은 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일이다. | 이러한 직선 $y$는 $Wx + b$로 나타낼 수 있다. | 이때 $W$는 Weight $b$는 Bias이다. | 그렇다면 이러한 Hypothesis를 정의하려면 W와 b를 먼저 정의해야 한다. | 일단 Weight와 Bias를 0으로 초기화해보자, 즉 어떤 입력을 받더라도 0을 예측할 것이다. | 이 W와 b를 학습시키는 게 우리의 목적이므로 requires_grad=True라고 입력해줌으로써 학습할 것이라고 명시해주는 것이다. | . W = torch.zeros(1, requires_grad = True) # Hypothesis 초기화 b = torch.zeros(1, requires_grad = True) # Hypothesis 초기화 hypothesis = x_train * W + b . 이렇게 W와 b를 정의하면 hypothesis는 $Wx + b$ 이렇게 간단한 곱과 합으로 나타낼 수 있다. | 이제 문제와 모델 정의가 모두 끝났으니 학습을 시작해보자 | 학습을 하려면 일단 우리의 모델이 얼마나 정답과 근사한지 알아야 한다. | . | 우리의 예측값과 실제 training dataset의 y값의 차이를 제곱해서 평균을 내는 것이다. | 위 수식을 PyTorch로 구현해보자 | . cost = torch.mean((hypothesis - y_train)**2) . MSE를 구했다. | 이 계산된 loss를 이용해서 모델을 개선시킬 차례이다. | . import torch.optim as optim optimizer = optim.SGD([W,b], lr = 0.01) # Optimizer를 정의해준다. # 우리의 모델에서 학습시킬 변수는 Weight와 Bias이기 때문에 이 2개를 list로 # 만들어서 입력해준뒤 적당한 learning rate도 입력해준다. optimizer.zero_grad() # gradient를 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선 . 이제 학습을 원하는만큼 for문을 반복해준다. | . nb_epochs = 1000 for epoch in range(1, nb_epochs + 1): hypothesis = x_train * W + b cost = torch.mean((hypothesis - y_train)**2) optimizer.zero_grad() # gradient를 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선 . 이렇게 반복적으로 학습하면 W와 b가 각각 하나의 최적의 수로 수렴하게 된다. | . W . tensor([1.9709], requires_grad=True) . b . tensor([0.0663], requires_grad=True) . W*4+b . tensor([7.9497], grad_fn=&lt;AddBackward0&gt;) . 8에 근접함을 알 수 있다. | . . 그렇다면 이렇게 cost를 줄이는 Gradient Descent에 대해 알아보자 | | Hypothesis 함수는 인공신경망의 구조로 나타내는데 주어진 input x에 대해 어떤 output y를 예측할지 알려주며 위와 같이 H(x)로 나타낸다. | b가 삭제된 더 간단한 모델에 대해 알아보자. 즉, W만 학습하면 된다. | . 위 train dataset은 입력과 출력이 동일하므로 H(x)=x가 정확한 모델일 것이다. 즉 W=1이 가장 좋은 숫자이다. | 반대로 W가 어떤 1이 아닌 값에서 시작할 때 학습의 목표는 W를 1로 수렴시키는 것이다. | W가 1에 가까울수록 더 정확한 모델이 되는 것이다. | 그렇다면 어떤 모델이 주어졌을 때 그것이 좋고 나쁨을 어떻게 평가할 수 있을까? 여기서 우리는 cost function을 정의할 수 있겠다. | cost function은 모델의 예측값이 실제 데이터와 얼마나 다른지를 나타내는 값으로 잘 학습된 모델일수록 낮은 cost를 가질 것이다. | 우리의 예제에서는 w가 1일 때 모든 데이터가 정확히 들어맞기 때문에 이때 cost가 0이다 | 그리고 w가 1에서 멀어질수록 예측값과 실제 데이터가 달라지므로 cost가 높아질 것이다. | | . | linear regression에서 사용되는 cost function은 MSE(예측값과 실제값의 차이를 제곱한 것의 평균)이다. | 우리가 원하는 cost function을 최소화하는 것이다. | 그러기 위해선 일단 위의 w-cost 그래프를 보자 | 기울기가 음수일 때는 W가 더 커져야 하고 기울기가 양수일 땐 W가 더 작아져야 한다. | 또한 기울기가 가파를수록 cost가 큰 것이니 w를 많이 바꿔야 하고 기울기가 완만할 땐 기울기가 가파를 때에 비해 w를 조금만 바꾸면 된다. | 우리는 이 기울기를 gradient라고 한다. | 이 gradient를 계산하기 위해선 미분을 해야하는데 cost function은 결국 w에 대한 2차 함수이기 때문에 간단한 미분 방정식을 통해 계산할 수 있다. | | 아까 설명했듯 gradient 즉 기울기가 양수일 때는 w를 줄이고 기울기가 음수일 때는 w를 늘려야 한다. | | 이러한 방법이 Gradient Descent이다. | . | . x_train = torch.FloatTensor([[1],[2],[3]]) y_train = torch.FloatTensor([[1],[2],[3]]) # 모델 초기화 W = torch.zeros(1) # learning rate 설정 lr = 0.1 nb_epochs = 10 for epoch in range(nb_epochs + 1): # H(x) 계산 hypothesis = x_train * W # cost gradient 계산 cost = torch.mean((hypothesis - y_train)**2) gradient = torch.sum((W * x_train - y_train)*x_train) print(&#39;Epoch {:4d}/{} W: {:.3f}, Cost: {:.6f}&#39;.format(epoch, nb_epochs,W.item(), cost.item())) # cost gradient로 H(x) 개선 W-=lr*gradient . Epoch 0/10 W: 0.000, Cost: 4.666667 Epoch 1/10 W: 1.400, Cost: 0.746666 Epoch 2/10 W: 0.840, Cost: 0.119467 Epoch 3/10 W: 1.064, Cost: 0.019115 Epoch 4/10 W: 0.974, Cost: 0.003058 Epoch 5/10 W: 1.010, Cost: 0.000489 Epoch 6/10 W: 0.996, Cost: 0.000078 Epoch 7/10 W: 1.002, Cost: 0.000013 Epoch 8/10 W: 0.999, Cost: 0.000002 Epoch 9/10 W: 1.000, Cost: 0.000000 Epoch 10/10 W: 1.000, Cost: 0.000000 . Epoch이 증가할수록 W가 1에 수렴하고 Cost역시 줄어듦을 알 수 있다. | gradient descesnt를 더 편리하게 할 수 있도록 torch.optim을 이용할 수 있다. | . import torch.optim as optim optimizer = optim.SGD([W,b], lr = 0.01) # Optimizer를 정의해준다. # 우리의 모델에서 학습시킬 변수는 Weight와 Bias이기 때문에 이 2개를 list로 # 만들어서 입력해준뒤 적당한 learning rate도 입력해준다. optimizer.zero_grad() # gradient를 0으로 초기화 cost.backward() # gradient를 계산 optimizer.step() # step을 이용하여 계산된 gradient의 방향대로 gradient를 개선, 즉 실행한다. . | 이와 같이 이용할 수 있겠다. | . 지금까지 우리는 하나의 정보로부터 하나의 정보를 추측하는 모델을 만들었다. 예를 들어 한 학생읭 수업 참여도를 알 때 점수를 추측할 수 있는 모델 같은 게 있을 수 있다. 하지만 좋은 추측을 하기 위해선 정보 양이 많이 필요하다. | 예를 들어 한 학생의 점수를 추측할 때, 쪽지시험 하나의 성적보다 그 수업 때 진행된 모든 쪽지시험의 점수를 알면 좋듯 많은 정보는 추측의 질을 높여준다. | . | . | . 다음시간엔 여러개의 정보로부터 모델을 만들어보자 | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/26/intro.html",
            "relUrl": "/2022/01/26/intro.html",
            "date": " • Jan 26, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "2022/01/13/THU",
            "content": "View(Reshape) . import numpy as np import torch t=np.array([[[0, 1, 2], [3, 4, 5]], [[6, 7, 8], [9, 10, 11]]]) ft=torch.FloatTensor(t) print(ft.shape) . torch.Size([2, 2, 3]) . t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]]) . print(ft.view([-1,3])) # view사용해서 모양 바꾸자 print(ft.view([-1,3]).shape) . tensor([[ 0., 1., 2.], [ 3., 4., 5.], [ 6., 7., 8.], [ 9., 10., 11.]]) torch.Size([4, 3]) . 두개의 차원으로 변경할 건데 앞 차원은 모르겠고 뒷 차원은 세개의 element를 가질래 | . print(ft.view([-1,1,3])) print(ft.view([-1,1,3]).shape) . tensor([[[ 0., 1., 2.]], [[ 3., 4., 5.]], [[ 6., 7., 8.]], [[ 9., 10., 11.]]]) torch.Size([4, 1, 3]) . 세개의 차원으로 변경할 건데 첫번째 차원은 아직 모르겠고 두번째 차원과 세번째 차원은 각각 1과3으로 변경해줘 | . Squeeze . ft=torch.FloatTensor([[0],[1],[2]]) print(ft) print(ft.shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . print(ft.squeeze()) print(ft.squeeze().shape) . tensor([0., 1., 2.]) torch.Size([3]) . 차원이 줄어들었음 | 1만 압축해주는 것 같음 | . print(ft.squeeze(dim=0)) print(ft.squeeze(dim=1)) # 해당 차원이 1일 때만! . tensor([[0.], [1.], [2.]]) tensor([0., 1., 2.]) . Unsqueeze . ft = torch.Tensor([0,1,2]) print(ft.shape) . torch.Size([3]) . 첫번째 차원에 1을 넣자 | . print(ft.unsqueeze(0)) print(ft.unsqueeze(0).shape) . tensor([[0., 1., 2.]]) torch.Size([1, 3]) . print(ft.view(1,-1)) print(ft.view(1,-1).shape) . tensor([[0., 1., 2.]]) torch.Size([1, 3]) . 두번째 차원에 1을 넣자 | . print(ft.unsqueeze(1)) print(ft.unsqueeze(1).shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . -1=(dim=1) | . print(ft.unsqueeze(-1)) print(ft.unsqueeze(-1).shape) . tensor([[0.], [1.], [2.]]) torch.Size([3, 1]) . Type Casting . lt = torch.LongTensor([1,2,3,4]) print(lt) . tensor([1, 2, 3, 4]) . print(lt.float()) . tensor([1., 2., 3., 4.]) . bool형일 때 | . bt=torch.ByteTensor([True,True,True]) print(bt) . tensor([1, 1, 1], dtype=torch.uint8) . print(bt.long()) print(bt.float()) . tensor([1, 1, 1]) tensor([1., 1., 1.]) . Concatenate . x=torch.FloatTensor([[1,2],[3,4]]) y=torch.FloatTensor([[5,6],[7,8]]) . print(torch.cat([x,y], dim=0)) print(torch.cat([x,y], dim=1)) . tensor([[1., 2.], [3., 4.], [5., 6.], [7., 8.]]) tensor([[1., 2., 5., 6.], [3., 4., 7., 8.]]) . Stacking . x=torch.FloatTensor([1,4]) y=torch.FloatTensor([2,5]) z=torch.FloatTensor([3,6]) . print(torch.stack([x,y,z])) print(torch.stack([x,y,z],dim=1)) . tensor([[1., 4.], [2., 5.], [3., 6.]]) tensor([[1., 2., 3.], [4., 5., 6.]]) . print(torch.cat([x.unsqueeze(0),y.unsqueeze(0),z.unsqueeze(0)],dim=0)) . tensor([[1., 4.], [2., 5.], [3., 6.]]) . Ones and Zeros . x=torch.FloatTensor([[0,1,2],[2,1,0]]) print(torch.ones_like(x)) print(torch.zeros_like(x)) . tensor([[1., 1., 1.], [1., 1., 1.]]) tensor([[0., 0., 0.], [0., 0., 0.]]) . In-place Operation . x=torch.FloatTensor([[1,2],[3,4]]) . print(x.mul(2)) print(x) print(x.mul_(2)) print(x) . tensor([[2., 4.], [6., 8.]]) tensor([[1., 2.], [3., 4.]]) tensor([[2., 4.], [6., 8.]]) tensor([[2., 4.], [6., 8.]]) . 즉 언더바를 통해 inplace=True 효과! | .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/13/intro.html",
            "relUrl": "/2022/01/13/intro.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "2022/01/12/WED",
            "content": "import torch . t=torch.FloatTensor([0.,1.,2.,3.,4.,5.,6.]) print(t) . tensor([0., 1., 2., 3., 4., 5., 6.]) . print(t.dim()) print(t.shape) print(t.size()) print(t[0],t[1],t[-1]) . 1 torch.Size([7]) torch.Size([7]) tensor(0.) tensor(1.) tensor(6.) . 그 외에도 slicing도 ndarray처럼 사용 가능 | 2차원 3차원도 ndarray처럼 사용 가능 | . Broadcasting . m1 = torch.FloatTensor([[3,3]]) m2 = torch.FloatTensor([[2,2]]) print(m1+m2) . tensor([[5., 5.]]) . 이렇게 같은 크기뿐만 아니라 Broadcasting을 통해 | . m1 = torch.FloatTensor([[1,2]]) m2 = torch.FloatTensor([[3]]) print(m1+m2) . tensor([[4., 5.]]) . 이렇게 크기가 동일하지 않아도 pytorch가 자동으로 동일한 size로 변환하여 연산 수행 가능 | . m1 = torch.FloatTensor([[1,2]]) m2 = torch.FloatTensor([[3],[4]]) print(m1+m2) . tensor([[4., 5.], [5., 6.]]) . print(m1.shape) . torch.Size([1, 2]) . m1 . tensor([[1., 2.]]) . print(m2.shape) . torch.Size([2, 1]) . m2 . tensor([[3.], [4.]]) . 이렇게 행렬 모양이 다르더라도 m1과 m2를 각각 2x2행렬로 변경하여 덧셈 수행하는 기능도 가능하다 | 이런 Broadcasting 기능은 자동으로 수행되기 때문에 잘못 사용하지 않게 유의하자 | . m1 = torch.FloatTensor([[1,2],[3,4]]) m2 = torch.FloatTensor([[3],[4]]) print(m1*m2) print(m1.mul(m2)) . tensor([[ 3., 6.], [12., 16.]]) tensor([[ 3., 6.], [12., 16.]]) . 이때 m2가 Broadcasting되면서 우리가 일반적으로 알고 있는 행렬 연산 수행과는 상이하게 진행됨 | 우리가 알고 있는 행렬 연산을 하기 위해서는 | . m1 = torch.FloatTensor([[1,2],[3,4]]) m2 = torch.FloatTensor([[3],[4]]) print(m1.matmul(m2)) . tensor([[11.], [25.]]) . 이렇게 해야한다. | . t = torch.FloatTensor([1,2]) print(t.mean()) . tensor(1.5000) . t = torch.FloatTensor([[1,2],[3,4]]) print(t.mean(dim=0)) # dim=0을 없애겠다. (2x2) -&gt; (1x2) print(t.mean(dim=1)) # dim=1을 없애겠다. (2x2) -&gt; (2x1) print(t.mean(dim=-1)) . tensor([2., 3.]) tensor([1.5000, 3.5000]) tensor([1.5000, 3.5000]) . t = torch.FloatTensor([[1,2],[3,4]]) print(t) . tensor([[1., 2.], [3., 4.]]) . print(t.sum()) print(t.sum(dim=0)) print(t.sum(dim=1)) print(t.sum(dim=-1)) . tensor(10.) tensor([4., 6.]) tensor([3., 7.]) tensor([3., 7.]) . t = torch.FloatTensor([[3,2],[1,4]]) print(t) . tensor([[3., 2.], [1., 4.]]) . print(t.max()) . tensor(4.) . print(t.max(dim=0)) # dim=0 제외하고 각 열에서의 max와 그 때의 index값을 알려줌 print(t.max(dim=0)[0]) # max값 print(t.max(dim=0)[1]) # max값의 index 즉, argmax . torch.return_types.max( values=tensor([3., 4.]), indices=tensor([0, 1])) tensor([3., 4.]) tensor([0, 1]) . print(t.max(dim=1)) print(t.max(dim=1)[0]) print(t.max(dim=1)[1]) . torch.return_types.max( values=tensor([3., 4.]), indices=tensor([0, 1])) tensor([3., 4.]) tensor([0, 1]) .",
            "url": "https://rhkrehtjd.github.io/INTROdl/2022/01/12/intro.html",
            "relUrl": "/2022/01/12/intro.html",
            "date": " • Jan 12, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rhkrehtjd.github.io/INTROdl/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rhkrehtjd.github.io/INTROdl/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}